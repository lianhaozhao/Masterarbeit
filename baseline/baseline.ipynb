{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-06T22:41:27.926807Z",
     "start_time": "2025-08-06T22:41:21.906686Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from models.Flexible_CNN import Flexible_CNN\n",
    "from PKLDataset import PKLDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import random\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "seed = 44\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "with open(\"../configs/default.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)['baseline']\n",
    "# 提取参数\n",
    "batch_size = config['batch_size']\n",
    "learning_rate = config['learning_rate']\n",
    "weight_decay = config['weight_decay']\n",
    "num_layers = config['num_layers']\n",
    "kernel_size = config['kernel_size']\n",
    "start_channels = config['start_channels']\n",
    "num_epochs = config['num_epochs']\n",
    "early_stopping_patience = config['early_stopping_patience']"
   ],
   "id": "47482a562ebe7eca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "                num_epochs=20, early_stopping_patience=3, scheduler=None, out_path=None):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size_actual = inputs.size(0)\n",
    "            train_loss += loss.item() * batch_size_actual\n",
    "            total_train_samples += batch_size_actual\n",
    "\n",
    "            # Accuracy calculation (training)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train / total_train_samples\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                batch_size_actual = inputs.size(0)\n",
    "                val_loss += loss.item() * batch_size_actual\n",
    "                total_val_samples += batch_size_actual\n",
    "\n",
    "                # Accuracy calculation (validation)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "\n",
    "        val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val / total_val_samples\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "                  f\"- Train Loss: {train_loss:.6f}, Acc: {train_accuracy:.4f} \"\n",
    "                  f\"- Val Loss: {val_loss:.6f}, Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience Counter: {patience_counter}/{early_stopping_patience}\")\n",
    "            if patience_counter >= early_stopping_patience and epoch > num_epochs * 0.4:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, os.path.join(out_path, 'best_model.pth'))\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return best_val_loss"
   ],
   "id": "24e3dc46ccb8dd3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = PKLDataset('../datasets/source/train/DC_T197_RP.txt')\n",
    "val_dataset = PKLDataset('../datasets/source/validation/DC_T197_RP.txt')\n",
    "out_path = \"model\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "model = Flexible_CNN(num_layers=num_layers,\n",
    "                         start_channels=start_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         cnn_act='leakrelu',\n",
    "                         num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.7, patience=3\n",
    ")\n",
    "best_val_loss= train_model(\n",
    "        model, train_loader, val_loader, optimizer, criterion, device,\n",
    "        num_epochs=num_epochs, early_stopping_patience=early_stopping_patience,scheduler=scheduler,out_path=out_path\n",
    "    )"
   ],
   "id": "7b4be82c73974508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8fdd54a41db4ef11"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
