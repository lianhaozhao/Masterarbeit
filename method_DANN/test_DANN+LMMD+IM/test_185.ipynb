{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-20T17:51:43.146706Z",
     "start_time": "2025-08-20T17:51:43.122037Z"
    }
   },
   "source": [
    "import copy, math, random, os\n",
    "import yaml\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from models.Flexible_DANN_pseudo_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from utils.general_train_and_test import general_test_model\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    src_ds = PKLDataset(txt_path=source_path)\n",
    "    tgt_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    src_loader = DataLoader(src_ds, batch_size=batch_size, shuffle=True)\n",
    "    return src_loader, tgt_loader\n",
    "\n",
    "# DANN 的 λ 调度（只进 GRL）\n",
    "def dann_lambda(epoch, num_epochs, max_lambda=0.5):\n",
    "    p = epoch / max(1, num_epochs-1)\n",
    "    return (2.0 / (1.0 + np.exp(-10 * p)) - 1.0) * max_lambda\n",
    "\n",
    "# LMMD 的基线权重（再乘质量门控得到最终权重）\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1, start_epoch=5):\n",
    "    if epoch < start_epoch: return 0.0\n",
    "    p = (epoch-start_epoch) / max(1, (num_epochs-1-start_epoch))\n",
    "    s = 1/(1+math.exp(-10*(p-0.5)))\n",
    "    return float(max_lambda*s)\n",
    "\n",
    "# ------------------ InfoMax（目标域） ------------------\n",
    "@torch.no_grad()\n",
    "def _safe_mean_prob(p, eps=1e-8):\n",
    "    p = p.clamp_min(eps)\n",
    "    return p / p.sum(dim=1, keepdim=True)\n",
    "\n",
    "def entropy_mean(p, eps=1e-8):\n",
    "    # E_x[ H(p_x) ]\n",
    "    p = p.clamp_min(eps)\n",
    "    return (-p * p.log()).sum(dim=1).mean()\n",
    "\n",
    "def entropy_marginal(p, eps=1e-8):\n",
    "    # H( E_x[p_x] )\n",
    "    p_bar = p.mean(dim=0)\n",
    "    p_bar = p_bar.clamp_min(eps)\n",
    "    return -(p_bar * p_bar.log()).sum()\n",
    "\n",
    "def infomax_loss_from_logits(logits, T=1.0, marg_weight=1.0):\n",
    "    \"\"\"\n",
    "        依据 “信息最大化” (InfoMax) 思想，从分类 logits 计算无监督正则项：\n",
    "        I(z; ŷ) = H(ŷ) - H(ŷ|z)。\n",
    "        训练中最小化的目标为：\n",
    "            L = H(ŷ|z) - w * H(ŷ)\n",
    "        其中 H(ŷ|z) 是条件熵（鼓励单样本预测更自信），H(ŷ) 是边际熵（鼓励整体类别使用均衡，防止塌缩）。\n",
    "\n",
    "        参数\n",
    "        ----\n",
    "        logits : Tensor\n",
    "            分类头输出的未归一化得分，形状 [B, C]。\n",
    "        T : float, 默认 1.0\n",
    "            Softmax 温度。T > 1 使分布变“软”（置信度下降），T < 1 使分布变“尖”（置信度上升）。\n",
    "            会同时影响条件熵与边际熵的数值。\n",
    "        marg_weight : float, 默认 1.0\n",
    "            边际熵权重 w。数值越大，越强烈地抑制“塌缩到单一类别”的解。\n",
    "            常见范围 0.5 ~ 2.0，可据验证集曲线调参。\n",
    "\n",
    "        返回\n",
    "        ----\n",
    "        loss : Tensor (标量，requires_grad=True)\n",
    "            最小化目标：H(ŷ|z) - w * H(ŷ)。用于反向传播。\n",
    "        h_cond_detached : Tensor (标量，no grad)\n",
    "            条件熵 H(ŷ|z) 的经验估计（E_x[-∑_c p(c|x) log p(c|x)]）。仅用于日志监控。\n",
    "        h_marg_detached : Tensor (标量，no grad)\n",
    "            边际熵 H(ŷ) 的经验估计，其中 ŷ 的边际分布为 p̄ = E_x[p(·|x)]。\n",
    "            数学形式：H(ŷ) = -∑_c p̄_c log p̄_c。仅用于日志监控。\n",
    "\n",
    "        \"\"\"\n",
    "    # I(z;ŷ) = H(ŷ) - H(ŷ|z); minimiere  H(ŷ|z) - w * H(ŷ)\n",
    "    p = F.softmax(logits / T, dim=1)  # [B, C], Klassenwahrscheinlichkeiten pro Beispiel\n",
    "    h_cond = entropy_mean(p)  # Skalar, Schätzung der bedingten Entropie\n",
    "    h_marg = entropy_marginal(p)  # Schätzung der marginalen Entropie\n",
    "    return h_cond - marg_weight * h_marg, h_cond.detach(), h_marg.detach()\n",
    "\n",
    "# ------------------ 伪标签 + 统计 ------------------\n",
    "@torch.no_grad()\n",
    "def generate_pseudo_with_stats(model, target_loader, device, threshold=0.95, T=1.0):\n",
    "    \"\"\"\n",
    "        在目标域无标签数据上，使用当前模型进行一次“离线伪标注”，并统计覆盖率与平均质量。\n",
    "\n",
    "        该函数会对目标域的每个样本计算分类概率（可选温度 T），\n",
    "        仅保留 Top-1 概率 conf >= threshold 的样本作为伪标签样本；\n",
    "        同时用 (Top-1 - Top-2) 的概率差值作为“样本质量权重”（margin），\n",
    "        以便后续在类条件对齐（如 LMMD）或蒸馏中对不确定样本降权。\n",
    "\n",
    "        函数在 `torch.no_grad()` + `model.eval()` 下执行，不会产生梯度与参数更新。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torch.nn.Module\n",
    "            已训练（或正在训练中的）分类模型。其 `forward(x, grl=False)` 应返回\n",
    "            `(logits, domain_logits, features)`，其中本函数只使用 `logits`。\n",
    "            注意：这里通过 `grl=False` 关闭 GRL 的反向影响（仅为语义明确；在 no_grad 下无反向）。\n",
    "        target_loader : torch.utils.data.DataLoader\n",
    "            目标域无标签数据的 DataLoader。`__getitem__` 应返回 `x` 或 `(x, ...)`，\n",
    "            其中本函数仅使用第一个元素作为输入。\n",
    "        device : torch.device\n",
    "            推理设备（如 `torch.device(\"cuda\")` 或 `torch.device(\"cpu\")`）。\n",
    "        threshold : float, default=0.95\n",
    "            伪标签保留阈值：当样本的 Top-1 概率 `conf` 满足 `conf >= threshold` 时保留该样本。\n",
    "            阈值越高，覆盖率（保留比例）通常越低但质量更高。\n",
    "        T : float, default=1.0\n",
    "            Softmax 温度。logits 会先除以 T 再做 softmax：\n",
    "            - T > 1：概率更“平”（置信度下降，覆盖率通常降低）；\n",
    "            - T < 1：概率更“尖”（置信度升高，覆盖率通常上升）。\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_cat : torch.Tensor\n",
    "            通过阈值的目标样本拼接后的张量，位于 **CPU**。\n",
    "        y_cat : torch.Tensor\n",
    "            对应的“硬伪标签”（`argmax`），dtype 为 `torch.long`，位于 **CPU**。\n",
    "        w_cat : torch.Tensor\n",
    "            样本级权重（质量），使用 `margin = p_top1 - p_top2`，位于 **CPU**。\n",
    "        stats : Dict[str, float]\n",
    "            统计信息字典，包含：\n",
    "            - \"kept\" : int，本轮保留的样本数 `N_keep`；\n",
    "            - \"total\": int，目标域样本总数；\n",
    "            - \"coverage\": float，覆盖率 = `N_keep / total`；\n",
    "            - \"margin_mean\": float，保留样本的平均 margin，衡量伪标签整体质量。\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    xs, ys, ws = [], [], []\n",
    "    margins = []\n",
    "    total = 0\n",
    "    for batch in target_loader:\n",
    "        x = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
    "        total += x.size(0)\n",
    "        x_dev = x.to(device)\n",
    "        logits, _, _ = model(x_dev, grl=False)\n",
    "        prob = F.softmax(logits/T, dim=1)\n",
    "        top2 = torch.topk(prob, k=2, dim=1).values #每个样本的 Top-1 概率 和 Top-2 概率 [B,2]\n",
    "        conf, _ = torch.max(prob, dim=1)  #每个样本的Top-1 概率[B,1]\n",
    "        margin = top2[:,0] - top2[:,1] #每个样本的 Top-1 概率 和 Top-2 概率差值\n",
    "        keep = conf >= threshold\n",
    "        if keep.any():\n",
    "            xs.append(x[keep].cpu())\n",
    "            ys.append(prob[keep].argmax(dim=1).cpu().long())\n",
    "            # 用 margin 作为样本权重（质量）\n",
    "            ws.append(margin[keep].cpu())\n",
    "            margins.append(margin[keep].cpu())\n",
    "    if len(xs) == 0:\n",
    "        x_cat = torch.empty(0); y_cat = torch.empty(0, dtype=torch.long); w_cat = torch.empty(0)\n",
    "        cov = 0.0; margin_mean = 0.0\n",
    "    else:\n",
    "        x_cat = torch.cat(xs, dim=0)\n",
    "        y_cat = torch.cat(ys, dim=0)\n",
    "        w_cat = torch.cat(ws, dim=0)\n",
    "        cov = float(x_cat.size(0)) / max(1, total)\n",
    "        margin_mean = float(torch.cat(margins).mean())\n",
    "    return x_cat, y_cat, w_cat, {\"kept\": int(x_cat.size(0)), \"total\": int(total),\n",
    "                                 \"coverage\": cov, \"margin_mean\": margin_mean}\n",
    "\n",
    "#  加权类条件 MMD（多核 RBF）\n",
    "def _pairwise_sq_dists(a, b):\n",
    "    # a: [m,d], b: [n,d]\n",
    "    a2 = (a*a).sum(dim=1, keepdim=True)       # [m,1]\n",
    "    b2 = (b*b).sum(dim=1, keepdim=True).t()   # [1,n]\n",
    "    return a2 + b2 - 2 * (a @ b.t())\n",
    "\n",
    "def _mk_kernel(a, b, gammas):\n",
    "    d2 = _pairwise_sq_dists(a, b).clamp_min(0)\n",
    "    k = 0.0\n",
    "    for g in gammas:\n",
    "        k = k + torch.exp(-g * d2)\n",
    "    return k\n",
    "\n",
    "def _weighted_mean_kernel(K, w_row, w_col):\n",
    "    # E_w[k] = (w_row^T K w_col) / (sum(w_row)*sum(w_col))\n",
    "    num = (w_row.view(1,-1) @ K @ w_col.view(-1,1)).squeeze()\n",
    "    den = (w_row.sum() * w_col.sum()).clamp_min(1e-8)\n",
    "    return num / den\n",
    "\n",
    "def mmd2_weighted(a, b, w_a=None, w_b=None, gammas=(0.5,1,2,4,8)):\n",
    "    # MMD^2 = E_aa k + E_bb k - 2 E_ab k  （带权）\n",
    "    if w_a is None: w_a = torch.ones(a.size(0), device=a.device)\n",
    "    if w_b is None: w_b = torch.ones(b.size(0), device=b.device)\n",
    "    Kaa = _mk_kernel(a, a, gammas)\n",
    "    Kbb = _mk_kernel(b, b, gammas)\n",
    "    Kab = _mk_kernel(a, b, gammas)\n",
    "    e_aa = _weighted_mean_kernel(Kaa, w_a, w_a)\n",
    "    e_bb = _weighted_mean_kernel(Kbb, w_b, w_b)\n",
    "    e_ab = _weighted_mean_kernel(Kab, w_a, w_b)\n",
    "    return (e_aa + e_bb - 2*e_ab).clamp_min(0.0)\n",
    "\n",
    "def classwise_mmd_biased_weighted(feat_src, y_src, feat_tgt, y_tgt, w_tgt,\n",
    "                                  num_classes, gammas=(0.5,1,2,4,8),\n",
    "                                  min_count_per_class=2):\n",
    "    total = feat_src.new_tensor(0.0)\n",
    "    wsum = 0.0\n",
    "    for c in range(num_classes):\n",
    "        ms = (y_src == c)\n",
    "        mt = (y_tgt == c)\n",
    "        ns, nt = int(ms.sum()), int(mt.sum())\n",
    "        if ns >= min_count_per_class and nt >= min_count_per_class:\n",
    "            w_c = w_tgt[mt]\n",
    "            mmd_c = mmd2_weighted(feat_src[ms], feat_tgt[mt], None, w_c, gammas)\n",
    "            w = float(min(ns, nt))\n",
    "            total = total + mmd_c * w\n",
    "            wsum += w\n",
    "    return total / wsum if wsum > 0 else total\n",
    "\n",
    "# 训练主循环\n",
    "def train_dann_infomax_lmmd(model,\n",
    "                            source_loader, target_loader,\n",
    "                            optimizer, criterion_cls, criterion_domain,\n",
    "                            device, num_epochs=20, num_classes=10,\n",
    "                            pseudo_thresh=0.95,\n",
    "                            mmd_gammas=(0.5,1,2,4,8),\n",
    "                            scheduler=None, batch_size=16,\n",
    "                            # InfoMax\n",
    "                            im_T=1.0, im_weight=0.5, im_marg_w=1.0,\n",
    "                            # 门控\n",
    "                            lmmd_start_epoch=5,\n",
    "                            ):\n",
    "    # 早停相关的超参\n",
    "    W = 4\n",
    "    GAP_TH = 0.05  # DomAcc 距 0.5 的门槛（越小越对齐）\n",
    "    PATIENCE = 3  # 连续几轮不提升后停止\n",
    "\n",
    "    # 轨迹缓存 & 最优记录\n",
    "    gap_hist = deque(maxlen=W)\n",
    "    best_score = -float('inf')\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 1) 伪标签\n",
    "        pl_loader = None\n",
    "        pseudo_x = torch.empty(0)\n",
    "        pseudo_y = torch.empty(0, dtype=torch.long)\n",
    "        pseudo_w = torch.empty(0)\n",
    "        cov = margin_mean = 0.0\n",
    "        if epoch >= lmmd_start_epoch:\n",
    "            pseudo_x, pseudo_y, pseudo_w, stats = generate_pseudo_with_stats(\n",
    "                model, target_loader, device, threshold=pseudo_thresh, T=1.0\n",
    "            )\n",
    "            kept, total = stats[\"kept\"], stats[\"total\"]\n",
    "            cov, margin_mean = stats[\"coverage\"], stats[\"margin_mean\"]\n",
    "            if kept > 0:\n",
    "                pl_ds = TensorDataset(pseudo_x, pseudo_y, pseudo_w)\n",
    "                pl_loader = DataLoader(pl_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # 2) 门控后的 LMMD 权重\n",
    "        lambda_mmd_base = mmd_lambda(epoch, num_epochs, max_lambda=1e-1, start_epoch=lmmd_start_epoch)\n",
    "        # 质量 q：对 margin 做线性归一（0.05~0.5），再与覆盖率耦合（concave，避免覆盖率高但质量差）\n",
    "        def _lin(x, lo, hi):\n",
    "            return float(min(max((x - lo) / max(1e-6, hi - lo), 0.0), 1.0))\n",
    "        q_margin = _lin(margin_mean, 0.05, 0.50)\n",
    "        q_cov = math.sqrt(max(0.0, cov))  # concave\n",
    "        q = q_margin * q_cov\n",
    "        lambda_mmd_eff = lambda_mmd_base * q\n",
    "\n",
    "        # 3) epoch 训练\n",
    "        model.train()\n",
    "        it_src = iter(source_loader)\n",
    "        it_tgt = iter(target_loader)\n",
    "        it_pl  = iter(pl_loader) if pl_loader is not None else None\n",
    "        len_src, len_tgt = len(source_loader), len(target_loader)\n",
    "        len_pl = len(pl_loader) if pl_loader is not None else 0\n",
    "        num_iters = max(len_src, len_tgt, len_pl) if len_pl > 0 else max(len_src, len_tgt)\n",
    "\n",
    "        cls_loss_sum = dom_loss_sum = mmd_loss_sum = im_loss_sum = 0.0\n",
    "        tot_loss_sum = 0.0\n",
    "        tot_target_samples=tot_cls_samples = tot_dom_samples = 0\n",
    "        dom_correct_src = dom_correct_tgt = 0\n",
    "        dom_total_src = dom_total_tgt = 0\n",
    "\n",
    "        for _ in range(num_iters):\n",
    "            try: src_x, src_y = next(it_src)\n",
    "            except StopIteration:\n",
    "                it_src = iter(source_loader); src_x, src_y = next(it_src)\n",
    "            try: tgt_x = next(it_tgt)\n",
    "            except StopIteration:\n",
    "                it_tgt = iter(target_loader); tgt_x = next(it_tgt)\n",
    "            if isinstance(tgt_x, (tuple, list)): tgt_x = tgt_x[0]\n",
    "            if it_pl is not None:\n",
    "                try: tgt_pl_x, tgt_pl_y, tgt_pl_w = next(it_pl)\n",
    "                except StopIteration:\n",
    "                    it_pl = iter(pl_loader); tgt_pl_x, tgt_pl_y, tgt_pl_w = next(it_pl)\n",
    "            else:\n",
    "                tgt_pl_x = tgt_pl_y = tgt_pl_w = None\n",
    "\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "            if tgt_pl_x is not None:\n",
    "                tgt_pl_x = tgt_pl_x.to(device)\n",
    "                tgt_pl_y = tgt_pl_y.to(device)\n",
    "                tgt_pl_w = tgt_pl_w.to(device)\n",
    "\n",
    "            # forword\n",
    "            # 把 λ 只放进 GRL\n",
    "            model.lambda_ = float(dann_lambda(epoch, num_epochs))\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x, grl=True)\n",
    "            cls_out_tgt, dom_out_tgt, feat_tgt = model(tgt_x, grl=True)\n",
    "\n",
    "            # 1) 源分类\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域对抗\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = (\n",
    "                criterion_domain(dom_out_src, dom_label_src) * src_x.size(0)\n",
    "                + criterion_domain(dom_out_tgt, dom_label_tgt) * tgt_x.size(0)\n",
    "            ) / (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 3) InfoMax\n",
    "            loss_im, h_cond, h_marg = infomax_loss_from_logits(cls_out_tgt, T=im_T, marg_weight=im_marg_w)\n",
    "            loss_im = im_weight * loss_im\n",
    "\n",
    "            # 4) 类条件 LMMD（加权、质量门控）\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            if tgt_pl_x is not None and lambda_mmd_eff > 0:\n",
    "                _, _, feat_tgt_pl = model(tgt_pl_x, grl=False)\n",
    "                feat_tgt_pl_n = F.normalize(feat_tgt_pl, dim=1)\n",
    "                loss_lmmd = classwise_mmd_biased_weighted(\n",
    "                    feat_src_n, src_y, feat_tgt_pl_n, tgt_pl_y, tgt_pl_w,\n",
    "                    num_classes=num_classes, gammas=mmd_gammas, min_count_per_class=2\n",
    "                )\n",
    "                loss_lmmd = lambda_mmd_eff * loss_lmmd\n",
    "            else:\n",
    "                loss_lmmd = feat_src_n.new_tensor(0.0)\n",
    "\n",
    "            loss = loss_cls + loss_dom + loss_im + loss_lmmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ------ 统计 ------\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            im_loss_sum   += loss_im.item()  * (tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_lmmd.item() * src_x.size(0)\n",
    "            tot_loss_sum  += loss.item()     * (src_x.size(0) + tgt_x.size(0))\n",
    "            tot_cls_samples += src_x.size(0)\n",
    "            tot_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "            tot_target_samples += tgt_x.size(0)\n",
    "\n",
    "            dom_correct_src += (dom_out_src.argmax(1) == dom_label_src).sum().item()\n",
    "            dom_total_src   += dom_label_src.size(0)\n",
    "            dom_correct_tgt += (dom_out_tgt.argmax(1) == dom_label_tgt).sum().item()\n",
    "            dom_total_tgt   += dom_label_tgt.size(0)\n",
    "\n",
    "        # ---- epoch 日志 ----\n",
    "        avg_cls = cls_loss_sum / max(1, tot_cls_samples)\n",
    "        avg_dom = dom_loss_sum / max(1, tot_dom_samples)\n",
    "        avg_im  = im_loss_sum  / max(1, tot_target_samples)\n",
    "        avg_mmd = mmd_loss_sum / max(1, tot_cls_samples)\n",
    "        avg_tot = tot_loss_sum / max(1, tot_dom_samples)\n",
    "        acc_src = dom_correct_src / max(1, dom_total_src)\n",
    "        acc_tgt = dom_correct_tgt / max(1, dom_total_tgt)\n",
    "        dom_acc = 0.5 * (acc_src + acc_tgt)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "        if scheduler is not None: scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total loss:{avg_tot:.4f} | Avg cls loss:{avg_cls:.4f} | avg Dom loss:{avg_dom:.4f} \"\n",
    "              f\"| avg IM loss:{avg_im:.4f} | avg LMMD loss:{avg_mmd:.4f} | DomAcc:{dom_acc:.4f} | \"\n",
    "              f\"cov:{cov:.2%} margin:{margin_mean:.3f} | \"\n",
    "              f\"λ_GRL:{model.lambda_:.4f} | λ_mmd_eff:{lambda_mmd_eff:.4f}\")\n",
    "\n",
    "        gap_hist.append(gap)\n",
    "        # 只用无监督指标：鼓励小 gap、同时惩罚大的 LMMD\n",
    "        score = gap * 0.5 + avg_im\n",
    "\n",
    "        # 记录最优模型\n",
    "        if epoch > 30:\n",
    "            improved = score < best_score + 1e-6\n",
    "            if improved:\n",
    "                best_score = score\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # --- 判断是否“对齐达标 进入平台期”---\n",
    "            gap_ok = (len(gap_hist) == W) and (sum(gap_hist) / W < GAP_TH)\n",
    "\n",
    "            # 主早停逻辑：若已“对齐 + 平台”，则看分数是否持续不提升\n",
    "            if gap_ok :\n",
    "                patience = 0 if improved else (patience + 1)\n",
    "                print(\n",
    "                    f\"[EARLY-STOP] patience {patience}/{PATIENCE} | gap_ok={gap_ok} | score={score:.4f}\")\n",
    "                if patience >= PATIENCE:\n",
    "                    print(\"[EARLY-STOP] Stopping training due to stable alignment/MMD and no score improvement.\")\n",
    "                    if best_state is not None:\n",
    "                        model.load_state_dict(best_state)\n",
    "\n",
    "                    break\n",
    "            else:\n",
    "                patience = 0\n",
    "\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:10:48.682851Z",
     "start_time": "2025-08-20T15:57:01.924787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed(11)\n",
    "    with open(\"../../configs/default.yaml\", 'r') as f:\n",
    "        cfg = yaml.safe_load(f)['baseline']\n",
    "    bs = cfg['batch_size']; lr = cfg['learning_rate']; wd = cfg['weight_decay']\n",
    "    num_layers = cfg['num_layers']; ksz = cfg['kernel_size']; sc = cfg['start_channels']\n",
    "    num_epochs = cfg['num_epochs']\n",
    "\n",
    "    src_path = '../../datasets/source/train/DC_T197_RP.txt'\n",
    "    tgt_path = '../../datasets/target/train/HC_T185_RP.txt'\n",
    "    tgt_test = '../datasets/target/test/HC_T185_RP.txt'\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    NUM_CLASSES = 10\n",
    "\n",
    "    model = Flexible_DANN(num_layers=num_layers, start_channels=sc, kernel_size=ksz,\n",
    "                          cnn_act='leakrelu', num_classes=NUM_CLASSES, lambda_=1.0).to(device)\n",
    "\n",
    "    src_loader, tgt_loader = get_dataloaders(src_path, tgt_path, bs)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=lr*0.1)\n",
    "    c_cls = nn.CrossEntropyLoss(); c_dom = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting DANN + InfoMax + (quality-gated) LMMD ...\")\n",
    "    model = train_dann_infomax_lmmd(\n",
    "        model, src_loader, tgt_loader,\n",
    "        optimizer, c_cls, c_dom, device,\n",
    "        num_epochs=num_epochs, num_classes=NUM_CLASSES,\n",
    "        pseudo_thresh=0.95,\n",
    "        scheduler=scheduler, batch_size=bs,\n",
    "        # InfoMax 超参\n",
    "        im_T=1.0, im_weight=0.5, im_marg_w=1.0,\n",
    "        lmmd_start_epoch=5,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_ds = PKLDataset(tgt_test)\n",
    "    test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False)\n",
    "    general_test_model(model, c_cls, test_loader, device)"
   ],
   "id": "9881c4207aae374f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting DANN + InfoMax + (quality-gated) LMMD ...\n",
      "[Epoch 1] Total loss:-0.3016 | Avg cls loss:0.3956 | avg Dom loss:0.0528 | avg IM loss:-0.7510 | avg LMMD loss:0.0000 | DomAcc:0.9797 | cov:0.00% margin:0.000 | λ_GRL:0.0000 | λ_mmd_eff:0.0000\n",
      "[Epoch 2] Total loss:-0.8536 | Avg cls loss:0.0628 | avg Dom loss:0.0032 | avg IM loss:-0.9206 | avg LMMD loss:0.0000 | DomAcc:0.9996 | cov:0.00% margin:0.000 | λ_GRL:0.0638 | λ_mmd_eff:0.0000\n",
      "[Epoch 3] Total loss:-0.9002 | Avg cls loss:0.0292 | avg Dom loss:0.0005 | avg IM loss:-0.9309 | avg LMMD loss:0.0000 | DomAcc:1.0000 | cov:0.00% margin:0.000 | λ_GRL:0.1255 | λ_mmd_eff:0.0000\n",
      "[Epoch 4] Total loss:-0.8994 | Avg cls loss:0.0330 | avg Dom loss:0.0017 | avg IM loss:-0.9355 | avg LMMD loss:0.0000 | DomAcc:0.9997 | cov:0.00% margin:0.000 | λ_GRL:0.1834 | λ_mmd_eff:0.0000\n",
      "[Epoch 5] Total loss:-0.7659 | Avg cls loss:0.0355 | avg Dom loss:0.1201 | avg IM loss:-0.9222 | avg LMMD loss:0.0000 | DomAcc:0.9525 | cov:0.00% margin:0.000 | λ_GRL:0.2361 | λ_mmd_eff:0.0000\n",
      "[Epoch 6] Total loss:-0.4494 | Avg cls loss:0.0430 | avg Dom loss:0.4381 | avg IM loss:-0.9321 | avg LMMD loss:0.0007 | DomAcc:0.7963 | cov:92.43% margin:0.995 | λ_GRL:0.2828 | λ_mmd_eff:0.0006\n",
      "[Epoch 7] Total loss:-0.2918 | Avg cls loss:0.0572 | avg Dom loss:0.5874 | avg IM loss:-0.9380 | avg LMMD loss:0.0006 | DomAcc:0.6653 | cov:90.50% margin:0.995 | λ_GRL:0.3232 | λ_mmd_eff:0.0009\n",
      "[Epoch 8] Total loss:-0.2502 | Avg cls loss:0.0559 | avg Dom loss:0.6427 | avg IM loss:-0.9514 | avg LMMD loss:0.0007 | DomAcc:0.6315 | cov:91.95% margin:0.996 | λ_GRL:0.3575 | λ_mmd_eff:0.0011\n",
      "[Epoch 9] Total loss:-0.2611 | Avg cls loss:0.0322 | avg Dom loss:0.6637 | avg IM loss:-0.9593 | avg LMMD loss:0.0012 | DomAcc:0.5991 | cov:90.22% margin:0.996 | λ_GRL:0.3861 | λ_mmd_eff:0.0015\n",
      "[Epoch 10] Total loss:-0.2295 | Avg cls loss:0.0361 | avg Dom loss:0.6855 | avg IM loss:-0.9537 | avg LMMD loss:0.0015 | DomAcc:0.5717 | cov:92.05% margin:0.996 | λ_GRL:0.4095 | λ_mmd_eff:0.0021\n",
      "[Epoch 11] Total loss:-0.2443 | Avg cls loss:0.0265 | avg Dom loss:0.6883 | avg IM loss:-0.9625 | avg LMMD loss:0.0022 | DomAcc:0.5375 | cov:92.23% margin:0.997 | λ_GRL:0.4285 | λ_mmd_eff:0.0027\n",
      "[Epoch 12] Total loss:-0.2758 | Avg cls loss:0.0235 | avg Dom loss:0.6685 | avg IM loss:-0.9715 | avg LMMD loss:0.0026 | DomAcc:0.5996 | cov:95.20% margin:0.995 | λ_GRL:0.4438 | λ_mmd_eff:0.0037\n",
      "[Epoch 13] Total loss:-0.2476 | Avg cls loss:0.0223 | avg Dom loss:0.6942 | avg IM loss:-0.9685 | avg LMMD loss:0.0032 | DomAcc:0.5556 | cov:97.21% margin:0.998 | λ_GRL:0.4559 | λ_mmd_eff:0.0049\n",
      "[Epoch 14] Total loss:-0.2527 | Avg cls loss:0.0265 | avg Dom loss:0.6933 | avg IM loss:-0.9783 | avg LMMD loss:0.0047 | DomAcc:0.5203 | cov:96.01% margin:0.998 | λ_GRL:0.4656 | λ_mmd_eff:0.0065\n",
      "[Epoch 15] Total loss:-0.2369 | Avg cls loss:0.0231 | avg Dom loss:0.7010 | avg IM loss:-0.9672 | avg LMMD loss:0.0052 | DomAcc:0.5107 | cov:95.56% margin:0.998 | λ_GRL:0.4731 | λ_mmd_eff:0.0085\n",
      "[EARLY-STOP] patience 1/3 | gap_ok=True | score=-0.9619\n",
      "[Epoch 16] Total loss:-0.2508 | Avg cls loss:0.0136 | avg Dom loss:0.6962 | avg IM loss:-0.9694 | avg LMMD loss:0.0076 | DomAcc:0.5251 | cov:95.00% margin:0.997 | λ_GRL:0.4791 | λ_mmd_eff:0.0110\n",
      "[EARLY-STOP] patience 2/3 | gap_ok=True | score=-0.9568\n",
      "[Epoch 17] Total loss:-0.2723 | Avg cls loss:0.0191 | avg Dom loss:0.6792 | avg IM loss:-0.9804 | avg LMMD loss:0.0087 | DomAcc:0.5770 | cov:97.31% margin:0.998 | λ_GRL:0.4837 | λ_mmd_eff:0.0144\n",
      "[EARLY-STOP] patience 3/3 | gap_ok=True | score=-0.9419\n",
      "[EARLY-STOP] Stopping training due to stable alignment/MMD and no score improvement.\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 0.702574, test Acc: 0.9204\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:23:41.777954Z",
     "start_time": "2025-08-20T16:11:43.917862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed(12)\n",
    "    with open(\"../../configs/default.yaml\", 'r') as f:\n",
    "        cfg = yaml.safe_load(f)['baseline']\n",
    "    bs = cfg['batch_size']; lr = cfg['learning_rate']; wd = cfg['weight_decay']\n",
    "    num_layers = cfg['num_layers']; ksz = cfg['kernel_size']; sc = cfg['start_channels']\n",
    "    num_epochs = cfg['num_epochs']\n",
    "\n",
    "    src_path = '../../datasets/source/train/DC_T197_RP.txt'\n",
    "    tgt_path = '../../datasets/target/train/HC_T185_RP.txt'\n",
    "    tgt_test = '../datasets/target/test/HC_T185_RP.txt'\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    NUM_CLASSES = 10\n",
    "\n",
    "    model = Flexible_DANN(num_layers=num_layers, start_channels=sc, kernel_size=ksz,\n",
    "                          cnn_act='leakrelu', num_classes=NUM_CLASSES, lambda_=1.0).to(device)\n",
    "\n",
    "    src_loader, tgt_loader = get_dataloaders(src_path, tgt_path, bs)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=lr*0.1)\n",
    "    c_cls = nn.CrossEntropyLoss(); c_dom = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting DANN + InfoMax + (quality-gated) LMMD ...\")\n",
    "    model = train_dann_infomax_lmmd(\n",
    "        model, src_loader, tgt_loader,\n",
    "        optimizer, c_cls, c_dom, device,\n",
    "        num_epochs=num_epochs, num_classes=NUM_CLASSES,\n",
    "        pseudo_thresh=0.95,\n",
    "        scheduler=scheduler, batch_size=bs,\n",
    "        # InfoMax 超参\n",
    "        im_T=1.0, im_weight=0.5, im_marg_w=1.0,\n",
    "        lmmd_start_epoch=5,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_ds = PKLDataset(tgt_test)\n",
    "    test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False)\n",
    "    general_test_model(model, c_cls, test_loader, device)"
   ],
   "id": "56e9bdacb2fe3ca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting DANN + InfoMax + (quality-gated) LMMD ...\n",
      "[Epoch 1] Total loss:-0.4012 | Avg cls loss:0.3213 | avg Dom loss:0.0605 | avg IM loss:-0.7843 | avg LMMD loss:0.0000 | DomAcc:0.9749 | cov:0.00% margin:0.000 | λ_GRL:0.0000 | λ_mmd_eff:0.0000\n",
      "[Epoch 2] Total loss:-0.8601 | Avg cls loss:0.0598 | avg Dom loss:0.0017 | avg IM loss:-0.9228 | avg LMMD loss:0.0000 | DomAcc:0.9999 | cov:0.00% margin:0.000 | λ_GRL:0.0638 | λ_mmd_eff:0.0000\n",
      "[Epoch 3] Total loss:-0.9057 | Avg cls loss:0.0282 | avg Dom loss:0.0004 | avg IM loss:-0.9352 | avg LMMD loss:0.0000 | DomAcc:1.0000 | cov:0.00% margin:0.000 | λ_GRL:0.1255 | λ_mmd_eff:0.0000\n",
      "[Epoch 4] Total loss:-0.9339 | Avg cls loss:0.0299 | avg Dom loss:0.0004 | avg IM loss:-0.9649 | avg LMMD loss:0.0000 | DomAcc:1.0000 | cov:0.00% margin:0.000 | λ_GRL:0.1834 | λ_mmd_eff:0.0000\n",
      "[Epoch 5] Total loss:-0.9397 | Avg cls loss:0.0278 | avg Dom loss:0.0006 | avg IM loss:-0.9693 | avg LMMD loss:0.0000 | DomAcc:1.0000 | cov:0.00% margin:0.000 | λ_GRL:0.2361 | λ_mmd_eff:0.0000\n",
      "[Epoch 6] Total loss:-0.6476 | Avg cls loss:0.0293 | avg Dom loss:0.2894 | avg IM loss:-0.9679 | avg LMMD loss:0.0009 | DomAcc:0.8650 | cov:97.77% margin:0.999 | λ_GRL:0.2828 | λ_mmd_eff:0.0007\n",
      "[Epoch 7] Total loss:-0.4687 | Avg cls loss:0.0394 | avg Dom loss:0.4463 | avg IM loss:-0.9560 | avg LMMD loss:0.0006 | DomAcc:0.7900 | cov:95.78% margin:0.998 | λ_GRL:0.3232 | λ_mmd_eff:0.0009\n",
      "[Epoch 8] Total loss:-0.3876 | Avg cls loss:0.0330 | avg Dom loss:0.5435 | avg IM loss:-0.9667 | avg LMMD loss:0.0008 | DomAcc:0.7073 | cov:94.31% margin:0.996 | λ_GRL:0.3575 | λ_mmd_eff:0.0012\n",
      "[Epoch 9] Total loss:-0.3309 | Avg cls loss:0.0450 | avg Dom loss:0.5861 | avg IM loss:-0.9640 | avg LMMD loss:0.0011 | DomAcc:0.6591 | cov:93.68% margin:0.996 | λ_GRL:0.3861 | λ_mmd_eff:0.0016\n",
      "[Epoch 10] Total loss:-0.2721 | Avg cls loss:0.0423 | avg Dom loss:0.6478 | avg IM loss:-0.9647 | avg LMMD loss:0.0014 | DomAcc:0.6205 | cov:96.27% margin:0.996 | λ_GRL:0.4095 | λ_mmd_eff:0.0021\n",
      "[Epoch 11] Total loss:-0.2437 | Avg cls loss:0.0365 | avg Dom loss:0.6713 | avg IM loss:-0.9546 | avg LMMD loss:0.0019 | DomAcc:0.5772 | cov:97.05% margin:0.997 | λ_GRL:0.4285 | λ_mmd_eff:0.0028\n",
      "[Epoch 12] Total loss:-0.2777 | Avg cls loss:0.0255 | avg Dom loss:0.6622 | avg IM loss:-0.9690 | avg LMMD loss:0.0024 | DomAcc:0.5994 | cov:91.98% margin:0.997 | λ_GRL:0.4438 | λ_mmd_eff:0.0036\n",
      "[Epoch 13] Total loss:-0.2637 | Avg cls loss:0.0153 | avg Dom loss:0.6959 | avg IM loss:-0.9794 | avg LMMD loss:0.0033 | DomAcc:0.5398 | cov:94.85% margin:0.996 | λ_GRL:0.4559 | λ_mmd_eff:0.0049\n",
      "[Epoch 14] Total loss:-0.2335 | Avg cls loss:0.0174 | avg Dom loss:0.7139 | avg IM loss:-0.9702 | avg LMMD loss:0.0041 | DomAcc:0.5037 | cov:92.53% margin:0.996 | λ_GRL:0.4656 | λ_mmd_eff:0.0064\n",
      "[Epoch 15] Total loss:-0.2609 | Avg cls loss:0.0216 | avg Dom loss:0.6889 | avg IM loss:-0.9790 | avg LMMD loss:0.0058 | DomAcc:0.5478 | cov:98.53% margin:0.999 | λ_GRL:0.4731 | λ_mmd_eff:0.0086\n",
      "[EARLY-STOP] patience 1/3 | gap_ok=True | score=-0.9551\n",
      "[Epoch 16] Total loss:-0.2574 | Avg cls loss:0.0234 | avg Dom loss:0.6894 | avg IM loss:-0.9791 | avg LMMD loss:0.0077 | DomAcc:0.5413 | cov:97.38% margin:0.999 | λ_GRL:0.4791 | λ_mmd_eff:0.0112\n",
      "[EARLY-STOP] patience 2/3 | gap_ok=True | score=-0.9584\n",
      "[Epoch 17] Total loss:-0.2615 | Avg cls loss:0.0128 | avg Dom loss:0.6951 | avg IM loss:-0.9805 | avg LMMD loss:0.0099 | DomAcc:0.4888 | cov:97.51% margin:0.998 | λ_GRL:0.4837 | λ_mmd_eff:0.0144\n",
      "[EARLY-STOP] patience 3/3 | gap_ok=True | score=-0.9749\n",
      "[EARLY-STOP] Stopping training due to stable alignment/MMD and no score improvement.\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 0.260575, test Acc: 0.9718\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
