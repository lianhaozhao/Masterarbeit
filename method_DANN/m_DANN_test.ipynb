{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-18T17:22:25.333334Z",
     "start_time": "2025-08-18T17:22:22.393118Z"
    }
   },
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T18:24:04.908771Z",
     "start_time": "2025-08-18T18:24:04.898445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    \"\"\"\n",
    "        构建源域 (source) 和目标域 (target) 的 DataLoader。\n",
    "\n",
    "        参数：\n",
    "            source_path (str): 源域数据的 txt 文件路径。\n",
    "                               每一行通常包含样本文件路径及其标签。\n",
    "            target_path (str): 目标域数据的 txt 文件路径。\n",
    "                               通常目标域没有标签\n",
    "            batch_size (int): 每个 batch 的样本数量。\n",
    "\n",
    "        返回：\n",
    "            tuple:\n",
    "                - source_loader : 源域的 DataLoader，\n",
    "                  会返回 (x, y) 格式的批次数据。\n",
    "                - target_loader : 目标域的 DataLoader，\n",
    "                  会返回 x（无标签）。\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 0.6\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    if epoch < 10:\n",
    "        return 0.8\n",
    "    elif epoch < 20:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.3\n",
    "\n",
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=dann_lambda,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, total_loss_sum = 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long, device=device)\n",
    "            bs_src, bs_tgt = src_x.size(0), tgt_x.size(0)\n",
    "            loss_dom_src = criterion_domain(dom_out_src, dom_label_src)\n",
    "            loss_dom_tgt = criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 样本数加权的“单个域损失均值”\n",
    "            loss_dom = (loss_dom_src * bs_src + loss_dom_tgt * bs_tgt) / (bs_src + bs_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cls_loss_sum += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "        avg_cls_loss = cls_loss_sum / total_cls_samples\n",
    "        avg_dom_loss = dom_loss_sum / total_dom_samples\n",
    "        avg_total_loss = total_loss_sum / total_dom_samples\n",
    "\n",
    "        # 域分类准确率（整轮）\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total Loss: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls loss: {avg_cls_loss:.4f} | Dom loss: {avg_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f} | lambda: {lambda_:.4f} \")\n",
    "\n",
    "\n",
    "        if gap < 0.02 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "\n",
    "\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model"
   ],
   "id": "8b735842a766e318",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T18:42:46.966813Z",
     "start_time": "2025-08-18T18:24:17.025804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    set_seed(seed=11)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)"
   ],
   "id": "653a3a8a9247770b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 0.8579 | Cls loss: 0.5585 | Dom loss: 0.5972 | DomAcc: 0.6460 | lambda: 0.5000 \n",
      "[Epoch 2] Total Loss: 0.4123 | Cls loss: 0.0911 | Dom loss: 0.6423 | DomAcc: 0.6280 | lambda: 0.5000 \n",
      "[Epoch 3] Total Loss: 0.4197 | Cls loss: 0.0620 | Dom loss: 0.7152 | DomAcc: 0.4921 | lambda: 0.5000 \n",
      "[Epoch 4] Total Loss: 0.3849 | Cls loss: 0.0459 | Dom loss: 0.6780 | DomAcc: 0.5750 | lambda: 0.5000 \n",
      "[Epoch 5] Total Loss: 0.3670 | Cls loss: 0.0475 | Dom loss: 0.6389 | DomAcc: 0.6440 | lambda: 0.5000 \n",
      "[Epoch 6] Total Loss: 0.3921 | Cls loss: 0.0378 | Dom loss: 0.7086 | DomAcc: 0.4914 | lambda: 0.5000 \n",
      "[Epoch 7] Total Loss: 0.3832 | Cls loss: 0.0342 | Dom loss: 0.6979 | DomAcc: 0.5006 | lambda: 0.5000 \n",
      "[Epoch 8] Total Loss: 0.3699 | Cls loss: 0.0292 | Dom loss: 0.6815 | DomAcc: 0.5693 | lambda: 0.5000 \n",
      "[Epoch 9] Total Loss: 0.3684 | Cls loss: 0.0217 | Dom loss: 0.6933 | DomAcc: 0.5622 | lambda: 0.5000 \n",
      "[Epoch 10] Total Loss: 0.3715 | Cls loss: 0.0281 | Dom loss: 0.6869 | DomAcc: 0.5407 | lambda: 0.5000 \n",
      "[Epoch 11] Total Loss: 0.3722 | Cls loss: 0.0260 | Dom loss: 0.6923 | DomAcc: 0.5295 | lambda: 0.5000 \n",
      "[Epoch 12] Total Loss: 0.3674 | Cls loss: 0.0219 | Dom loss: 0.6910 | DomAcc: 0.5579 | lambda: 0.5000 \n",
      "[Epoch 13] Total Loss: 0.3792 | Cls loss: 0.0305 | Dom loss: 0.6972 | DomAcc: 0.4986 | lambda: 0.5000 \n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 14] Total Loss: 0.3558 | Cls loss: 0.0164 | Dom loss: 0.6789 | DomAcc: 0.5849 | lambda: 0.5000 \n",
      "[Epoch 15] Total Loss: 0.3541 | Cls loss: 0.0158 | Dom loss: 0.6766 | DomAcc: 0.5625 | lambda: 0.5000 \n",
      "[Epoch 16] Total Loss: 0.3542 | Cls loss: 0.0132 | Dom loss: 0.6820 | DomAcc: 0.5696 | lambda: 0.5000 \n",
      "[Epoch 17] Total Loss: 0.3425 | Cls loss: 0.0185 | Dom loss: 0.6479 | DomAcc: 0.6336 | lambda: 0.5000 \n",
      "[Epoch 18] Total Loss: 0.3541 | Cls loss: 0.0249 | Dom loss: 0.6584 | DomAcc: 0.6118 | lambda: 0.5000 \n",
      "[Epoch 19] Total Loss: 0.3449 | Cls loss: 0.0132 | Dom loss: 0.6635 | DomAcc: 0.6061 | lambda: 0.5000 \n",
      "[Epoch 20] Total Loss: 0.3419 | Cls loss: 0.0189 | Dom loss: 0.6459 | DomAcc: 0.6489 | lambda: 0.5000 \n",
      "[Epoch 21] Total Loss: 0.3493 | Cls loss: 0.0140 | Dom loss: 0.6706 | DomAcc: 0.5951 | lambda: 0.5000 \n",
      "[Epoch 22] Total Loss: 0.3414 | Cls loss: 0.0182 | Dom loss: 0.6463 | DomAcc: 0.6202 | lambda: 0.5000 \n",
      "[Epoch 23] Total Loss: 0.3517 | Cls loss: 0.0199 | Dom loss: 0.6637 | DomAcc: 0.5994 | lambda: 0.5000 \n",
      "[Epoch 24] Total Loss: 0.3456 | Cls loss: 0.0175 | Dom loss: 0.6561 | DomAcc: 0.6399 | lambda: 0.5000 \n",
      "[Epoch 25] Total Loss: 0.3323 | Cls loss: 0.0146 | Dom loss: 0.6355 | DomAcc: 0.6489 | lambda: 0.5000 \n",
      "[Epoch 26] Total Loss: 0.3561 | Cls loss: 0.0139 | Dom loss: 0.6844 | DomAcc: 0.5594 | lambda: 0.5000 \n",
      "[Epoch 27] Total Loss: 0.3422 | Cls loss: 0.0078 | Dom loss: 0.6689 | DomAcc: 0.6041 | lambda: 0.5000 \n",
      "[Epoch 28] Total Loss: 0.3466 | Cls loss: 0.0103 | Dom loss: 0.6727 | DomAcc: 0.5736 | lambda: 0.5000 \n",
      "[Epoch 29] Total Loss: 0.3538 | Cls loss: 0.0053 | Dom loss: 0.6970 | DomAcc: 0.4744 | lambda: 0.5000 \n",
      "[Epoch 30] Total Loss: 0.3453 | Cls loss: 0.0041 | Dom loss: 0.6823 | DomAcc: 0.5795 | lambda: 0.5000 \n",
      "[Epoch 31] Total Loss: 0.3438 | Cls loss: 0.0031 | Dom loss: 0.6814 | DomAcc: 0.6091 | lambda: 0.5000 \n",
      "[Epoch 32] Total Loss: 0.3424 | Cls loss: 0.0024 | Dom loss: 0.6801 | DomAcc: 0.5864 | lambda: 0.5000 \n",
      "[Epoch 33] Total Loss: 0.3473 | Cls loss: 0.0050 | Dom loss: 0.6846 | DomAcc: 0.5598 | lambda: 0.5000 \n",
      "[Epoch 34] Total Loss: 0.3468 | Cls loss: 0.0033 | Dom loss: 0.6871 | DomAcc: 0.5399 | lambda: 0.5000 \n",
      "[Epoch 35] Total Loss: 0.3469 | Cls loss: 0.0062 | Dom loss: 0.6813 | DomAcc: 0.5658 | lambda: 0.5000 \n",
      "[Epoch 36] Total Loss: 0.3474 | Cls loss: 0.0033 | Dom loss: 0.6882 | DomAcc: 0.5403 | lambda: 0.5000 \n",
      "[Epoch 37] Total Loss: 0.3456 | Cls loss: 0.0019 | Dom loss: 0.6874 | DomAcc: 0.5399 | lambda: 0.5000 \n",
      "[Epoch 38] Total Loss: 0.3477 | Cls loss: 0.0027 | Dom loss: 0.6900 | DomAcc: 0.5411 | lambda: 0.5000 \n",
      "[Epoch 39] Total Loss: 0.3475 | Cls loss: 0.0028 | Dom loss: 0.6894 | DomAcc: 0.5322 | lambda: 0.5000 \n",
      "[Epoch 40] Total Loss: 0.3525 | Cls loss: 0.0051 | Dom loss: 0.6949 | DomAcc: 0.5108 | lambda: 0.5000 \n",
      "[INFO] patience 1 / 3\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.453512, test Acc: 0.6583\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T19:05:03.423172Z",
     "start_time": "2025-08-18T18:43:42.249493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    set_seed(seed=22)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)"
   ],
   "id": "32c5c124ed6a0e50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 0.8262 | Cls loss: 0.5662 | Dom loss: 0.5189 | DomAcc: 0.7436 | lambda: 0.5000 \n",
      "[Epoch 2] Total Loss: 0.4059 | Cls loss: 0.1190 | Dom loss: 0.5735 | DomAcc: 0.7017 | lambda: 0.5000 \n",
      "[Epoch 3] Total Loss: 0.4176 | Cls loss: 0.0839 | Dom loss: 0.6672 | DomAcc: 0.5828 | lambda: 0.5000 \n",
      "[Epoch 4] Total Loss: 0.3758 | Cls loss: 0.0759 | Dom loss: 0.5997 | DomAcc: 0.6962 | lambda: 0.5000 \n",
      "[Epoch 5] Total Loss: 0.3868 | Cls loss: 0.0628 | Dom loss: 0.6479 | DomAcc: 0.6069 | lambda: 0.5000 \n",
      "[Epoch 6] Total Loss: 0.3733 | Cls loss: 0.0599 | Dom loss: 0.6266 | DomAcc: 0.6647 | lambda: 0.5000 \n",
      "[Epoch 7] Total Loss: 0.3824 | Cls loss: 0.0580 | Dom loss: 0.6489 | DomAcc: 0.6142 | lambda: 0.5000 \n",
      "[Epoch 8] Total Loss: 0.3537 | Cls loss: 0.0334 | Dom loss: 0.6406 | DomAcc: 0.6555 | lambda: 0.5000 \n",
      "[Epoch 9] Total Loss: 0.3694 | Cls loss: 0.0384 | Dom loss: 0.6620 | DomAcc: 0.6072 | lambda: 0.5000 \n",
      "[Epoch 10] Total Loss: 0.3681 | Cls loss: 0.0392 | Dom loss: 0.6580 | DomAcc: 0.6117 | lambda: 0.5000 \n",
      "[Epoch 11] Total Loss: 0.3710 | Cls loss: 0.0376 | Dom loss: 0.6668 | DomAcc: 0.5858 | lambda: 0.5000 \n",
      "[Epoch 12] Total Loss: 0.3728 | Cls loss: 0.0279 | Dom loss: 0.6897 | DomAcc: 0.5470 | lambda: 0.5000 \n",
      "[Epoch 13] Total Loss: 0.3657 | Cls loss: 0.0230 | Dom loss: 0.6853 | DomAcc: 0.5451 | lambda: 0.5000 \n",
      "[Epoch 14] Total Loss: 0.3721 | Cls loss: 0.0218 | Dom loss: 0.7006 | DomAcc: 0.4904 | lambda: 0.5000 \n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 15] Total Loss: 0.3506 | Cls loss: 0.0128 | Dom loss: 0.6755 | DomAcc: 0.6028 | lambda: 0.5000 \n",
      "[Epoch 16] Total Loss: 0.3500 | Cls loss: 0.0174 | Dom loss: 0.6653 | DomAcc: 0.5924 | lambda: 0.5000 \n",
      "[Epoch 17] Total Loss: 0.3460 | Cls loss: 0.0189 | Dom loss: 0.6540 | DomAcc: 0.6181 | lambda: 0.5000 \n",
      "[Epoch 18] Total Loss: 0.3639 | Cls loss: 0.0275 | Dom loss: 0.6727 | DomAcc: 0.5781 | lambda: 0.5000 \n",
      "[Epoch 19] Total Loss: 0.3533 | Cls loss: 0.0181 | Dom loss: 0.6702 | DomAcc: 0.5885 | lambda: 0.5000 \n",
      "[Epoch 20] Total Loss: 0.3476 | Cls loss: 0.0151 | Dom loss: 0.6650 | DomAcc: 0.6032 | lambda: 0.5000 \n",
      "[Epoch 21] Total Loss: 0.3466 | Cls loss: 0.0225 | Dom loss: 0.6481 | DomAcc: 0.6212 | lambda: 0.5000 \n",
      "[Epoch 22] Total Loss: 0.3523 | Cls loss: 0.0160 | Dom loss: 0.6726 | DomAcc: 0.5839 | lambda: 0.5000 \n",
      "[Epoch 23] Total Loss: 0.3507 | Cls loss: 0.0160 | Dom loss: 0.6694 | DomAcc: 0.5849 | lambda: 0.5000 \n",
      "[Epoch 24] Total Loss: 0.3571 | Cls loss: 0.0198 | Dom loss: 0.6745 | DomAcc: 0.5741 | lambda: 0.5000 \n",
      "[Epoch 25] Total Loss: 0.3534 | Cls loss: 0.0081 | Dom loss: 0.6907 | DomAcc: 0.5452 | lambda: 0.5000 \n",
      "[Epoch 26] Total Loss: 0.3543 | Cls loss: 0.0117 | Dom loss: 0.6851 | DomAcc: 0.5606 | lambda: 0.5000 \n",
      "[Epoch 27] Total Loss: 0.3432 | Cls loss: 0.0059 | Dom loss: 0.6744 | DomAcc: 0.5992 | lambda: 0.5000 \n",
      "[Epoch 28] Total Loss: 0.3531 | Cls loss: 0.0149 | Dom loss: 0.6764 | DomAcc: 0.5761 | lambda: 0.5000 \n",
      "[Epoch 29] Total Loss: 0.3451 | Cls loss: 0.0037 | Dom loss: 0.6827 | DomAcc: 0.5545 | lambda: 0.5000 \n",
      "[Epoch 30] Total Loss: 0.3534 | Cls loss: 0.0090 | Dom loss: 0.6887 | DomAcc: 0.5426 | lambda: 0.5000 \n",
      "[Epoch 31] Total Loss: 0.3475 | Cls loss: 0.0073 | Dom loss: 0.6805 | DomAcc: 0.5714 | lambda: 0.5000 \n",
      "[Epoch 32] Total Loss: 0.3511 | Cls loss: 0.0053 | Dom loss: 0.6915 | DomAcc: 0.5198 | lambda: 0.5000 \n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 33] Total Loss: 0.3483 | Cls loss: 0.0051 | Dom loss: 0.6864 | DomAcc: 0.5472 | lambda: 0.5000 \n",
      "[Epoch 34] Total Loss: 0.3455 | Cls loss: 0.0039 | Dom loss: 0.6830 | DomAcc: 0.5610 | lambda: 0.5000 \n",
      "[Epoch 35] Total Loss: 0.3444 | Cls loss: 0.0020 | Dom loss: 0.6848 | DomAcc: 0.5550 | lambda: 0.5000 \n",
      "[Epoch 36] Total Loss: 0.3503 | Cls loss: 0.0034 | Dom loss: 0.6938 | DomAcc: 0.5110 | lambda: 0.5000 \n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 37] Total Loss: 0.3490 | Cls loss: 0.0029 | Dom loss: 0.6922 | DomAcc: 0.5215 | lambda: 0.5000 \n",
      "[Epoch 38] Total Loss: 0.3486 | Cls loss: 0.0013 | Dom loss: 0.6945 | DomAcc: 0.5016 | lambda: 0.5000 \n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 39] Total Loss: 0.3498 | Cls loss: 0.0027 | Dom loss: 0.6942 | DomAcc: 0.5149 | lambda: 0.5000 \n",
      "[INFO] patience 2 / 3\n",
      "[Epoch 40] Total Loss: 0.3489 | Cls loss: 0.0016 | Dom loss: 0.6946 | DomAcc: 0.5164 | lambda: 0.5000 \n",
      "[INFO] patience 3 / 3\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.934641, test Acc: 0.4211\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T21:17:07.878762Z",
     "start_time": "2025-08-18T21:04:07.483578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    set_seed(seed=133)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T191_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T191_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=30, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)"
   ],
   "id": "4940f9d3aa22d8fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 0.7411 | Cls loss: 0.4746 | Dom loss: 0.5333 | DomAcc: 0.7114 | lambda: 0.5000 \n",
      "[Epoch 2] Total Loss: 0.4195 | Cls loss: 0.1065 | Dom loss: 0.6259 | DomAcc: 0.6285 | lambda: 0.5000 \n",
      "[Epoch 3] Total Loss: 0.3800 | Cls loss: 0.0757 | Dom loss: 0.6086 | DomAcc: 0.6611 | lambda: 0.5000 \n",
      "[Epoch 4] Total Loss: 0.3714 | Cls loss: 0.0760 | Dom loss: 0.5908 | DomAcc: 0.6776 | lambda: 0.5000 \n",
      "[Epoch 5] Total Loss: 0.3920 | Cls loss: 0.0665 | Dom loss: 0.6509 | DomAcc: 0.6052 | lambda: 0.5000 \n",
      "[Epoch 6] Total Loss: 0.3752 | Cls loss: 0.0616 | Dom loss: 0.6273 | DomAcc: 0.6384 | lambda: 0.5000 \n",
      "[Epoch 7] Total Loss: 0.3786 | Cls loss: 0.0605 | Dom loss: 0.6362 | DomAcc: 0.6330 | lambda: 0.5000 \n",
      "[Epoch 8] Total Loss: 0.3672 | Cls loss: 0.0486 | Dom loss: 0.6372 | DomAcc: 0.6372 | lambda: 0.5000 \n",
      "[Epoch 9] Total Loss: 0.3695 | Cls loss: 0.0416 | Dom loss: 0.6559 | DomAcc: 0.6132 | lambda: 0.5000 \n",
      "[Epoch 10] Total Loss: 0.3556 | Cls loss: 0.0358 | Dom loss: 0.6398 | DomAcc: 0.6449 | lambda: 0.5000 \n",
      "[Epoch 11] Total Loss: 0.3614 | Cls loss: 0.0311 | Dom loss: 0.6605 | DomAcc: 0.6026 | lambda: 0.5000 \n",
      "[Epoch 12] Total Loss: 0.3594 | Cls loss: 0.0356 | Dom loss: 0.6476 | DomAcc: 0.6217 | lambda: 0.5000 \n",
      "[Epoch 13] Total Loss: 0.3639 | Cls loss: 0.0316 | Dom loss: 0.6645 | DomAcc: 0.6012 | lambda: 0.5000 \n",
      "[Epoch 14] Total Loss: 0.3526 | Cls loss: 0.0284 | Dom loss: 0.6485 | DomAcc: 0.6241 | lambda: 0.5000 \n",
      "[Epoch 15] Total Loss: 0.3588 | Cls loss: 0.0269 | Dom loss: 0.6638 | DomAcc: 0.5941 | lambda: 0.5000 \n",
      "[Epoch 16] Total Loss: 0.3580 | Cls loss: 0.0211 | Dom loss: 0.6739 | DomAcc: 0.5761 | lambda: 0.5000 \n",
      "[Epoch 17] Total Loss: 0.3727 | Cls loss: 0.0353 | Dom loss: 0.6748 | DomAcc: 0.5618 | lambda: 0.5000 \n",
      "[Epoch 18] Total Loss: 0.3575 | Cls loss: 0.0237 | Dom loss: 0.6675 | DomAcc: 0.5978 | lambda: 0.5000 \n",
      "[Epoch 19] Total Loss: 0.3614 | Cls loss: 0.0153 | Dom loss: 0.6922 | DomAcc: 0.5324 | lambda: 0.5000 \n",
      "[Epoch 20] Total Loss: 0.3574 | Cls loss: 0.0132 | Dom loss: 0.6883 | DomAcc: 0.5505 | lambda: 0.5000 \n",
      "[Epoch 21] Total Loss: 0.3549 | Cls loss: 0.0108 | Dom loss: 0.6883 | DomAcc: 0.5428 | lambda: 0.5000 \n",
      "[Epoch 22] Total Loss: 0.3540 | Cls loss: 0.0110 | Dom loss: 0.6860 | DomAcc: 0.5492 | lambda: 0.5000 \n",
      "[Epoch 23] Total Loss: 0.3559 | Cls loss: 0.0117 | Dom loss: 0.6883 | DomAcc: 0.5536 | lambda: 0.5000 \n",
      "[Epoch 24] Total Loss: 0.3577 | Cls loss: 0.0115 | Dom loss: 0.6926 | DomAcc: 0.5206 | lambda: 0.5000 \n",
      "[Epoch 25] Total Loss: 0.3543 | Cls loss: 0.0084 | Dom loss: 0.6918 | DomAcc: 0.5227 | lambda: 0.5000 \n",
      "[Epoch 26] Total Loss: 0.3547 | Cls loss: 0.0098 | Dom loss: 0.6897 | DomAcc: 0.5347 | lambda: 0.5000 \n",
      "[Epoch 27] Total Loss: 0.3556 | Cls loss: 0.0105 | Dom loss: 0.6904 | DomAcc: 0.5326 | lambda: 0.5000 \n",
      "[Epoch 28] Total Loss: 0.3502 | Cls loss: 0.0041 | Dom loss: 0.6922 | DomAcc: 0.5206 | lambda: 0.5000 \n",
      "[Epoch 29] Total Loss: 0.3509 | Cls loss: 0.0053 | Dom loss: 0.6912 | DomAcc: 0.5252 | lambda: 0.5000 \n",
      "[Epoch 30] Total Loss: 0.3502 | Cls loss: 0.0049 | Dom loss: 0.6905 | DomAcc: 0.5335 | lambda: 0.5000 \n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.530301, test Acc: 0.3832\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T21:46:24.637558Z",
     "start_time": "2025-08-18T21:38:15.818604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    set_seed(seed=133)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)"
   ],
   "id": "f7e68f5d948f2b7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 0.7650 | Cls loss: 0.4737 | Dom loss: 0.5823 | DomAcc: 0.6758 | lambda: 0.5000 \n",
      "[Epoch 2] Total Loss: 0.4321 | Cls loss: 0.1016 | Dom loss: 0.6618 | DomAcc: 0.5734 | lambda: 0.5000 \n",
      "[Epoch 3] Total Loss: 0.4005 | Cls loss: 0.0762 | Dom loss: 0.6486 | DomAcc: 0.6308 | lambda: 0.5000 \n",
      "[Epoch 4] Total Loss: 0.4001 | Cls loss: 0.0567 | Dom loss: 0.6866 | DomAcc: 0.5371 | lambda: 0.5000 \n",
      "[Epoch 5] Total Loss: 0.3956 | Cls loss: 0.0606 | Dom loss: 0.6697 | DomAcc: 0.5776 | lambda: 0.5000 \n",
      "[Epoch 6] Total Loss: 0.4018 | Cls loss: 0.0556 | Dom loss: 0.6923 | DomAcc: 0.5733 | lambda: 0.5000 \n",
      "[Epoch 7] Total Loss: 0.3834 | Cls loss: 0.0497 | Dom loss: 0.6674 | DomAcc: 0.6029 | lambda: 0.5000 \n",
      "[Epoch 8] Total Loss: 0.3761 | Cls loss: 0.0389 | Dom loss: 0.6744 | DomAcc: 0.5786 | lambda: 0.5000 \n",
      "[Epoch 9] Total Loss: 0.3569 | Cls loss: 0.0320 | Dom loss: 0.6497 | DomAcc: 0.6028 | lambda: 0.5000 \n",
      "[Epoch 10] Total Loss: 0.3492 | Cls loss: 0.0270 | Dom loss: 0.6445 | DomAcc: 0.6158 | lambda: 0.5000 \n",
      "[Epoch 11] Total Loss: 0.3650 | Cls loss: 0.0363 | Dom loss: 0.6575 | DomAcc: 0.6056 | lambda: 0.5000 \n",
      "[Epoch 12] Total Loss: 0.3658 | Cls loss: 0.0315 | Dom loss: 0.6687 | DomAcc: 0.5804 | lambda: 0.5000 \n",
      "[Epoch 13] Total Loss: 0.3679 | Cls loss: 0.0366 | Dom loss: 0.6624 | DomAcc: 0.6143 | lambda: 0.5000 \n",
      "[Epoch 14] Total Loss: 0.3756 | Cls loss: 0.0302 | Dom loss: 0.6908 | DomAcc: 0.5413 | lambda: 0.5000 \n",
      "[Epoch 15] Total Loss: 0.3744 | Cls loss: 0.0301 | Dom loss: 0.6885 | DomAcc: 0.5454 | lambda: 0.5000 \n",
      "[Epoch 16] Total Loss: 0.3564 | Cls loss: 0.0203 | Dom loss: 0.6721 | DomAcc: 0.5947 | lambda: 0.5000 \n",
      "[Epoch 17] Total Loss: 0.3632 | Cls loss: 0.0186 | Dom loss: 0.6891 | DomAcc: 0.5792 | lambda: 0.5000 \n",
      "[Epoch 18] Total Loss: 0.3830 | Cls loss: 0.0355 | Dom loss: 0.6948 | DomAcc: 0.5091 | lambda: 0.5000 \n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 19] Total Loss: 0.3616 | Cls loss: 0.0254 | Dom loss: 0.6722 | DomAcc: 0.5896 | lambda: 0.5000 \n",
      "[Epoch 20] Total Loss: 0.3616 | Cls loss: 0.0152 | Dom loss: 0.6929 | DomAcc: 0.5185 | lambda: 0.5000 \n",
      "[INFO] patience 1 / 3\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.790776, test Acc: 0.5504\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
