{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-14T10:11:17.067396Z",
     "start_time": "2025-08-14T10:11:13.865312Z"
    }
   },
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T10:11:18.773890Z",
     "start_time": "2025-08-14T10:11:18.766792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    # p = epoch / float(num_epochs)\n",
    "    # return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "    if epoch < 10:\n",
    "        return 0.8\n",
    "    elif epoch < 20:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.3"
   ],
   "id": "50946c2804937d95",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:43:56.039498Z",
     "start_time": "2025-08-11T23:39:44.220299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, total_loss_sum = 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cls_loss_sum += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "        avg_cls_loss = cls_loss_sum / total_cls_samples\n",
    "        avg_dom_loss = dom_loss_sum / total_dom_samples\n",
    "        avg_total_loss = total_loss_sum / total_dom_samples\n",
    "\n",
    "        # 域分类准确率（整轮）\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total Loss: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.03 and avg_cls_loss < 0.5 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=1,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "77ac832e8182a6d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 2.7007 | Cls: 1.3054 | Dom: 1.3953 | DomAcc: 0.5358\n",
      "[Epoch 2] Total Loss: 1.6789 | Cls: 0.4881 | Dom: 1.1908 | DomAcc: 0.6557\n",
      "[Epoch 3] Total Loss: 1.5989 | Cls: 0.3699 | Dom: 1.2290 | DomAcc: 0.6562\n",
      "[Epoch 4] Total Loss: 1.4997 | Cls: 0.2143 | Dom: 1.2854 | DomAcc: 0.6522\n",
      "[Epoch 5] Total Loss: 1.4682 | Cls: 0.1732 | Dom: 1.2950 | DomAcc: 0.6295\n",
      "[Epoch 6] Total Loss: 1.4464 | Cls: 0.1505 | Dom: 1.2959 | DomAcc: 0.6195\n",
      "[Epoch 7] Total Loss: 1.4164 | Cls: 0.1157 | Dom: 1.3007 | DomAcc: 0.6018\n",
      "[Epoch 8] Total Loss: 1.5035 | Cls: 0.1407 | Dom: 1.3628 | DomAcc: 0.5696\n",
      "[Epoch 9] Total Loss: 1.3922 | Cls: 0.0984 | Dom: 1.2938 | DomAcc: 0.6205\n",
      "[Epoch 10] Total Loss: 1.3202 | Cls: 0.1204 | Dom: 1.1998 | DomAcc: 0.7026\n",
      "[Epoch 11] Total Loss: 1.4701 | Cls: 0.0960 | Dom: 1.3740 | DomAcc: 0.5418\n",
      "[Epoch 12] Total Loss: 1.5798 | Cls: 0.0841 | Dom: 1.4956 | DomAcc: 0.4350\n",
      "[Epoch 13] Total Loss: 1.5396 | Cls: 0.0581 | Dom: 1.4815 | DomAcc: 0.4123\n",
      "[Epoch 14] Total Loss: 1.4558 | Cls: 0.0525 | Dom: 1.4033 | DomAcc: 0.4748\n",
      "[Epoch 15] Total Loss: 1.4673 | Cls: 0.0837 | Dom: 1.3836 | DomAcc: 0.5312\n",
      "[Epoch 16] Total Loss: 1.4097 | Cls: 0.0405 | Dom: 1.3692 | DomAcc: 0.5756\n",
      "[Epoch 17] Total Loss: 1.4201 | Cls: 0.0484 | Dom: 1.3717 | DomAcc: 0.5544\n",
      "[Epoch 18] Total Loss: 1.4403 | Cls: 0.0534 | Dom: 1.3869 | DomAcc: 0.5131\n",
      "[Epoch 19] Total Loss: 1.4400 | Cls: 0.0427 | Dom: 1.3973 | DomAcc: 0.4728\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 20] Total Loss: 1.3866 | Cls: 0.0345 | Dom: 1.3521 | DomAcc: 0.6376\n",
      "[Epoch 21] Total Loss: 1.4083 | Cls: 0.0321 | Dom: 1.3762 | DomAcc: 0.5413\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 22] Total Loss: 1.3903 | Cls: 0.0242 | Dom: 1.3660 | DomAcc: 0.5847\n",
      "[Epoch 23] Total Loss: 1.4067 | Cls: 0.0309 | Dom: 1.3758 | DomAcc: 0.5549\n",
      "[Epoch 24] Total Loss: 1.4078 | Cls: 0.0262 | Dom: 1.3817 | DomAcc: 0.5413\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 25] Total Loss: 1.3958 | Cls: 0.0234 | Dom: 1.3724 | DomAcc: 0.5625\n",
      "[Epoch 26] Total Loss: 1.4008 | Cls: 0.0243 | Dom: 1.3765 | DomAcc: 0.5514\n",
      "[Epoch 27] Total Loss: 1.3917 | Cls: 0.0157 | Dom: 1.3761 | DomAcc: 0.5459\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 28] Total Loss: 1.3879 | Cls: 0.0136 | Dom: 1.3742 | DomAcc: 0.5655\n",
      "[Epoch 29] Total Loss: 1.3941 | Cls: 0.0154 | Dom: 1.3787 | DomAcc: 0.5459\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 30] Total Loss: 1.3845 | Cls: 0.0134 | Dom: 1.3711 | DomAcc: 0.5685\n",
      "[Epoch 31] Total Loss: 1.3890 | Cls: 0.0153 | Dom: 1.3737 | DomAcc: 0.5766\n",
      "[Epoch 32] Total Loss: 1.3949 | Cls: 0.0213 | Dom: 1.3736 | DomAcc: 0.5519\n",
      "[Epoch 33] Total Loss: 1.3918 | Cls: 0.0193 | Dom: 1.3725 | DomAcc: 0.5680\n",
      "[Epoch 34] Total Loss: 1.3794 | Cls: 0.0089 | Dom: 1.3705 | DomAcc: 0.5620\n",
      "[Epoch 35] Total Loss: 1.3896 | Cls: 0.0105 | Dom: 1.3791 | DomAcc: 0.5428\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 36] Total Loss: 1.3876 | Cls: 0.0173 | Dom: 1.3703 | DomAcc: 0.5625\n",
      "[Epoch 37] Total Loss: 1.3873 | Cls: 0.0170 | Dom: 1.3703 | DomAcc: 0.5761\n",
      "[Epoch 38] Total Loss: 1.3992 | Cls: 0.0287 | Dom: 1.3705 | DomAcc: 0.5696\n",
      "[Epoch 39] Total Loss: 1.3887 | Cls: 0.0187 | Dom: 1.3700 | DomAcc: 0.5660\n",
      "[Epoch 40] Total Loss: 1.3833 | Cls: 0.0134 | Dom: 1.3699 | DomAcc: 0.5741\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.005660, test Acc: 0.5660\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T23:49:22.471364Z",
     "start_time": "2025-08-10T23:28:26.773207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_cls_loss, total_dom_loss = 0.0, 0.0, 0.0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "        num_batches = 0\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            num_batches += 1\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            total_dom_loss += loss_dom.item()\n",
    "\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        avg_cls_loss = total_cls_loss / num_batches\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {total_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "        # print(\"[INFO] Evaluating on target test set...\")\n",
    "        # target_test_path = '../datasets/HC_T185_RP.txt'\n",
    "        # test_dataset = PKLDataset(target_test_path)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        # pseudo_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "\n",
    "\n",
    "        if gap < 0.005 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "    if best_model_state is not None:\n",
    "\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=44)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "427d3adb6a872972",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 312.2282 | Cls: 0.7016 | Dom: 273.6700 | DomAcc: 0.6900\n",
      "[Epoch 2] Total Loss: 173.5931 | Cls: 0.1267 | Dom: 283.8536 | DomAcc: 0.6893\n",
      "[Epoch 3] Total Loss: 192.4783 | Cls: 0.0823 | Dom: 343.7952 | DomAcc: 0.5965\n",
      "[Epoch 4] Total Loss: 191.1798 | Cls: 0.0665 | Dom: 349.0959 | DomAcc: 0.5167\n",
      "[Epoch 5] Total Loss: 183.9482 | Cls: 0.0480 | Dom: 343.8882 | DomAcc: 0.5465\n",
      "[Epoch 6] Total Loss: 177.7356 | Cls: 0.0400 | Dom: 335.4661 | DomAcc: 0.5980\n",
      "[Epoch 7] Total Loss: 171.7944 | Cls: 0.0369 | Dom: 325.1553 | DomAcc: 0.6084\n",
      "[Epoch 8] Total Loss: 172.1987 | Cls: 0.0373 | Dom: 325.7460 | DomAcc: 0.6006\n",
      "[Epoch 9] Total Loss: 176.5720 | Cls: 0.0323 | Dom: 336.9959 | DomAcc: 0.5458\n",
      "[Epoch 10] Total Loss: 171.5147 | Cls: 0.0395 | Dom: 323.2586 | DomAcc: 0.6296\n",
      "[Epoch 11] Total Loss: 173.0376 | Cls: 0.0349 | Dom: 328.6305 | DomAcc: 0.6055\n",
      "[Epoch 12] Total Loss: 175.7335 | Cls: 0.0274 | Dom: 337.7629 | DomAcc: 0.5706\n",
      "[Epoch 13] Total Loss: 158.0050 | Cls: 0.0303 | Dom: 300.8803 | DomAcc: 0.6860\n",
      "[Epoch 14] Total Loss: 163.8128 | Cls: 0.0235 | Dom: 315.8598 | DomAcc: 0.6439\n",
      "[Epoch 15] Total Loss: 166.8970 | Cls: 0.0320 | Dom: 317.7830 | DomAcc: 0.6410\n",
      "[Epoch 16] Total Loss: 165.2495 | Cls: 0.0215 | Dom: 319.7723 | DomAcc: 0.6411\n",
      "[Epoch 17] Total Loss: 167.0120 | Cls: 0.0123 | Dom: 327.8880 | DomAcc: 0.6292\n",
      "[Epoch 18] Total Loss: 168.7767 | Cls: 0.0146 | Dom: 330.2374 | DomAcc: 0.6082\n",
      "[Epoch 19] Total Loss: 166.5901 | Cls: 0.0157 | Dom: 325.3186 | DomAcc: 0.6364\n",
      "[Epoch 20] Total Loss: 171.9311 | Cls: 0.0144 | Dom: 336.6430 | DomAcc: 0.5807\n",
      "[Epoch 21] Total Loss: 171.4858 | Cls: 0.0110 | Dom: 337.4656 | DomAcc: 0.5818\n",
      "[Epoch 22] Total Loss: 171.9399 | Cls: 0.0066 | Dom: 340.5788 | DomAcc: 0.5672\n",
      "[Epoch 23] Total Loss: 173.7555 | Cls: 0.0057 | Dom: 344.6491 | DomAcc: 0.5621\n",
      "[Epoch 24] Total Loss: 175.3119 | Cls: 0.0062 | Dom: 347.5354 | DomAcc: 0.5177\n",
      "[Epoch 25] Total Loss: 173.5228 | Cls: 0.0030 | Dom: 345.5297 | DomAcc: 0.5297\n",
      "[Epoch 26] Total Loss: 173.4333 | Cls: 0.0051 | Dom: 344.3332 | DomAcc: 0.5545\n",
      "[Epoch 27] Total Loss: 173.8225 | Cls: 0.0016 | Dom: 346.8429 | DomAcc: 0.5358\n",
      "[Epoch 28] Total Loss: 173.6588 | Cls: 0.0025 | Dom: 346.0501 | DomAcc: 0.5343\n",
      "[Epoch 29] Total Loss: 173.9371 | Cls: 0.0017 | Dom: 347.0200 | DomAcc: 0.5198\n",
      "[Epoch 30] Total Loss: 174.2248 | Cls: 0.0035 | Dom: 346.7077 | DomAcc: 0.5182\n",
      "[Epoch 31] Total Loss: 174.0182 | Cls: 0.0007 | Dom: 347.7031 | DomAcc: 0.4978\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 32] Total Loss: 174.4568 | Cls: 0.0007 | Dom: 348.5603 | DomAcc: 0.4757\n",
      "[Epoch 33] Total Loss: 174.9165 | Cls: 0.0084 | Dom: 345.6556 | DomAcc: 0.5233\n",
      "[Epoch 34] Total Loss: 174.6864 | Cls: 0.0029 | Dom: 347.9100 | DomAcc: 0.4839\n",
      "[Epoch 35] Total Loss: 175.0960 | Cls: 0.0072 | Dom: 346.6143 | DomAcc: 0.5134\n",
      "[Epoch 36] Total Loss: 174.1200 | Cls: 0.0031 | Dom: 346.7082 | DomAcc: 0.5057\n",
      "[Epoch 37] Total Loss: 174.7319 | Cls: 0.0065 | Dom: 346.2367 | DomAcc: 0.5144\n",
      "[Epoch 38] Total Loss: 174.2948 | Cls: 0.0077 | Dom: 344.7448 | DomAcc: 0.5242\n",
      "[Epoch 39] Total Loss: 173.9282 | Cls: 0.0052 | Dom: 345.2785 | DomAcc: 0.5308\n",
      "[Epoch 40] Total Loss: 176.3113 | Cls: 0.0159 | Dom: 344.6571 | DomAcc: 0.5262\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.679201, test Acc: 0.6826\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T11:09:33.864356Z",
     "start_time": "2025-08-11T10:55:30.315479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_cls_loss, total_dom_loss = 0.0, 0.0, 0.0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "        num_batches = 0\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            num_batches += 1\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            total_dom_loss += loss_dom.item()\n",
    "\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        avg_cls_loss = total_cls_loss / num_batches\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {total_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.03 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "    if best_model_state is not None:\n",
    "\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=188)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "eae45690c147fc83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 262.9256 | Cls: 0.5452 | Dom: 253.2420 | DomAcc: 0.7581\n",
      "[Epoch 2] Total Loss: 183.4174 | Cls: 0.1369 | Dom: 298.4002 | DomAcc: 0.6896\n",
      "[Epoch 3] Total Loss: 168.9419 | Cls: 0.0898 | Dom: 293.0012 | DomAcc: 0.6950\n",
      "[Epoch 4] Total Loss: 175.1027 | Cls: 0.0783 | Dom: 311.0342 | DomAcc: 0.6667\n",
      "[Epoch 5] Total Loss: 174.7883 | Cls: 0.0521 | Dom: 323.5196 | DomAcc: 0.6151\n",
      "[Epoch 6] Total Loss: 179.4945 | Cls: 0.0465 | Dom: 335.7604 | DomAcc: 0.5862\n",
      "[Epoch 7] Total Loss: 169.3155 | Cls: 0.0487 | Dom: 314.2756 | DomAcc: 0.6539\n",
      "[Epoch 8] Total Loss: 163.3561 | Cls: 0.0370 | Dom: 308.2311 | DomAcc: 0.6749\n",
      "[Epoch 9] Total Loss: 173.5950 | Cls: 0.0430 | Dom: 325.7055 | DomAcc: 0.6066\n",
      "[Epoch 10] Total Loss: 169.5951 | Cls: 0.0224 | Dom: 328.0061 | DomAcc: 0.6178\n",
      "[Epoch 11] Total Loss: 174.5310 | Cls: 0.0343 | Dom: 331.8966 | DomAcc: 0.5922\n",
      "[Epoch 12] Total Loss: 174.0767 | Cls: 0.0216 | Dom: 337.3530 | DomAcc: 0.5893\n",
      "[Epoch 13] Total Loss: 169.0750 | Cls: 0.0122 | Dom: 332.0552 | DomAcc: 0.6106\n",
      "[Epoch 14] Total Loss: 174.1357 | Cls: 0.0201 | Dom: 338.2427 | DomAcc: 0.5656\n",
      "[Epoch 15] Total Loss: 174.8362 | Cls: 0.0195 | Dom: 339.9078 | DomAcc: 0.5660\n",
      "[Epoch 16] Total Loss: 173.0815 | Cls: 0.0111 | Dom: 340.6077 | DomAcc: 0.5632\n",
      "[Epoch 17] Total Loss: 170.6753 | Cls: 0.0095 | Dom: 336.5766 | DomAcc: 0.5885\n",
      "[Epoch 18] Total Loss: 172.2456 | Cls: 0.0107 | Dom: 339.1321 | DomAcc: 0.5635\n",
      "[Epoch 19] Total Loss: 174.4030 | Cls: 0.0155 | Dom: 341.0477 | DomAcc: 0.5572\n",
      "[Epoch 20] Total Loss: 172.0058 | Cls: 0.0031 | Dom: 342.4438 | DomAcc: 0.5626\n",
      "[Epoch 21] Total Loss: 172.3752 | Cls: 0.0100 | Dom: 339.7651 | DomAcc: 0.5799\n",
      "[Epoch 22] Total Loss: 172.9220 | Cls: 0.0037 | Dom: 343.9965 | DomAcc: 0.5358\n",
      "[Epoch 23] Total Loss: 173.4074 | Cls: 0.0015 | Dom: 346.0431 | DomAcc: 0.5104\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 24] Total Loss: 172.7256 | Cls: 0.0028 | Dom: 344.0617 | DomAcc: 0.5435\n",
      "[Epoch 25] Total Loss: 172.9357 | Cls: 0.0005 | Dom: 345.6023 | DomAcc: 0.5227\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 26] Total Loss: 174.6764 | Cls: 0.0018 | Dom: 348.4607 | DomAcc: 0.4837\n",
      "[INFO] patience 2 / 3\n",
      "[Epoch 27] Total Loss: 172.8117 | Cls: 0.0004 | Dom: 345.4376 | DomAcc: 0.5307\n",
      "[Epoch 28] Total Loss: 173.6840 | Cls: 0.0004 | Dom: 347.1745 | DomAcc: 0.4976\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 29] Total Loss: 172.7610 | Cls: 0.0003 | Dom: 345.3671 | DomAcc: 0.5148\n",
      "[INFO] patience 2 / 3\n",
      "[Epoch 30] Total Loss: 173.7958 | Cls: 0.0003 | Dom: 347.4434 | DomAcc: 0.4818\n",
      "[INFO] patience 3 / 3\n",
      "[Epoch 31] Total Loss: 173.9773 | Cls: 0.0004 | Dom: 347.7694 | DomAcc: 0.4704\n",
      "[INFO] patience 4 / 3\n",
      "[INFO] Early stopping: domain aligned and classifier converged.\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.394681, test Acc: 0.4310\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:14:00.836280Z",
     "start_time": "2025-08-11T23:10:01.074239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_cls_loss, total_dom_loss = 0.0, 0.0, 0.0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "        num_batches = 0\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            num_batches += 1\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            total_dom_loss += loss_dom.item()\n",
    "\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        avg_cls_loss = total_cls_loss / num_batches\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {total_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.015 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "    if best_model_state is not None:\n",
    "\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=191)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/test/HC_T191_RP.txt'\n",
    "    target_test_path = '../datasets/target/train/HC_T191_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "af67191a76772b7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 107.4720 | Cls: 1.2015 | Dom: 61.1553 | DomAcc: 0.7852\n",
      "[Epoch 2] Total Loss: 42.0315 | Cls: 0.3317 | Dom: 41.6113 | DomAcc: 0.9031\n",
      "[Epoch 3] Total Loss: 35.3511 | Cls: 0.1812 | Dom: 47.5135 | DomAcc: 0.8437\n",
      "[Epoch 4] Total Loss: 35.0832 | Cls: 0.1315 | Dom: 53.3284 | DomAcc: 0.7684\n",
      "[Epoch 5] Total Loss: 35.8496 | Cls: 0.1410 | Dom: 53.6561 | DomAcc: 0.8269\n",
      "[Epoch 6] Total Loss: 32.5553 | Cls: 0.1231 | Dom: 49.3595 | DomAcc: 0.8491\n",
      "[Epoch 7] Total Loss: 33.1143 | Cls: 0.0969 | Dom: 53.8270 | DomAcc: 0.8368\n",
      "[Epoch 8] Total Loss: 38.0941 | Cls: 0.1343 | Dom: 58.9997 | DomAcc: 0.8024\n",
      "[Epoch 9] Total Loss: 38.6681 | Cls: 0.0865 | Dom: 66.2631 | DomAcc: 0.7522\n",
      "[Epoch 10] Total Loss: 35.6480 | Cls: 0.0938 | Dom: 59.2918 | DomAcc: 0.7979\n",
      "[Epoch 11] Total Loss: 38.1515 | Cls: 0.1236 | Dom: 60.4816 | DomAcc: 0.7817\n",
      "[Epoch 12] Total Loss: 37.1170 | Cls: 0.0730 | Dom: 64.8887 | DomAcc: 0.7557\n",
      "[Epoch 13] Total Loss: 37.6678 | Cls: 0.0934 | Dom: 63.3761 | DomAcc: 0.7822\n",
      "[Epoch 14] Total Loss: 39.5822 | Cls: 0.0846 | Dom: 68.3411 | DomAcc: 0.7360\n",
      "[Epoch 15] Total Loss: 39.3257 | Cls: 0.0504 | Dom: 72.2044 | DomAcc: 0.7217\n",
      "[Epoch 16] Total Loss: 39.1553 | Cls: 0.0577 | Dom: 70.9271 | DomAcc: 0.7266\n",
      "[Epoch 17] Total Loss: 40.5227 | Cls: 0.0626 | Dom: 73.0345 | DomAcc: 0.7144\n",
      "[Epoch 18] Total Loss: 37.0208 | Cls: 0.0354 | Dom: 69.5114 | DomAcc: 0.7448\n",
      "[Epoch 19] Total Loss: 39.7879 | Cls: 0.0833 | Dom: 68.9098 | DomAcc: 0.7542\n",
      "[Epoch 20] Total Loss: 38.1460 | Cls: 0.0465 | Dom: 70.3399 | DomAcc: 0.7527\n",
      "[Epoch 21] Total Loss: 35.7352 | Cls: 0.0475 | Dom: 65.3926 | DomAcc: 0.7704\n",
      "[Epoch 22] Total Loss: 35.5293 | Cls: 0.0372 | Dom: 66.2969 | DomAcc: 0.7778\n",
      "[Epoch 23] Total Loss: 38.3839 | Cls: 0.0345 | Dom: 72.3454 | DomAcc: 0.7330\n",
      "[Epoch 24] Total Loss: 35.8476 | Cls: 0.0480 | Dom: 65.5541 | DomAcc: 0.7837\n",
      "[Epoch 25] Total Loss: 37.7359 | Cls: 0.0224 | Dom: 72.6100 | DomAcc: 0.7163\n",
      "[Epoch 26] Total Loss: 37.5506 | Cls: 0.0227 | Dom: 72.1906 | DomAcc: 0.7232\n",
      "[Epoch 27] Total Loss: 36.4334 | Cls: 0.0248 | Dom: 69.6968 | DomAcc: 0.7360\n",
      "[Epoch 28] Total Loss: 37.1165 | Cls: 0.0173 | Dom: 72.0166 | DomAcc: 0.7419\n",
      "[Epoch 29] Total Loss: 40.6819 | Cls: 0.0239 | Dom: 78.3029 | DomAcc: 0.6691\n",
      "[Epoch 30] Total Loss: 39.9923 | Cls: 0.0132 | Dom: 78.2888 | DomAcc: 0.6731\n",
      "[Epoch 31] Total Loss: 38.8775 | Cls: 0.0142 | Dom: 75.9416 | DomAcc: 0.7011\n",
      "[Epoch 32] Total Loss: 38.9706 | Cls: 0.0106 | Dom: 76.5829 | DomAcc: 0.6883\n",
      "[Epoch 33] Total Loss: 40.6979 | Cls: 0.0147 | Dom: 79.5155 | DomAcc: 0.6760\n",
      "[Epoch 34] Total Loss: 42.7805 | Cls: 0.0276 | Dom: 82.0345 | DomAcc: 0.6500\n",
      "[Epoch 35] Total Loss: 40.9183 | Cls: 0.0130 | Dom: 80.1684 | DomAcc: 0.6686\n",
      "[Epoch 36] Total Loss: 39.1487 | Cls: 0.0320 | Dom: 74.2072 | DomAcc: 0.7021\n",
      "[Epoch 37] Total Loss: 40.5550 | Cls: 0.0196 | Dom: 78.6041 | DomAcc: 0.6760\n",
      "[Epoch 38] Total Loss: 41.5825 | Cls: 0.0410 | Dom: 77.9136 | DomAcc: 0.6691\n",
      "[Epoch 39] Total Loss: 42.5882 | Cls: 0.0307 | Dom: 81.2439 | DomAcc: 0.6441\n",
      "[Epoch 40] Total Loss: 43.2135 | Cls: 0.0366 | Dom: 81.7446 | DomAcc: 0.6259\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.703248, test Acc: 0.5260\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T23:43:26.066145Z",
     "start_time": "2025-08-12T23:21:27.999756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, total_loss_sum = 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "            lambda_ = dann_lambda(epoch, num_epochs)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cls_loss_sum += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "        avg_cls_loss = cls_loss_sum / total_cls_samples\n",
    "        avg_dom_loss = dom_loss_sum / total_dom_samples\n",
    "        avg_total_loss = total_loss_sum / total_dom_samples\n",
    "\n",
    "        # 域分类准确率（整轮）\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total Loss: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.03 and avg_cls_loss < 0.5 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=194)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T194_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=1,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "cbfe2fb5e5f1a299",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 1.3830 | Cls: 0.4813 | Dom: 1.1275 | DomAcc: 0.6980\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.379489, test Acc: 0.3473\n",
      "[Epoch 2] Total Loss: 1.1783 | Cls: 0.1138 | Dom: 1.3307 | DomAcc: 0.5889\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.943707, test Acc: 0.3565\n",
      "[Epoch 3] Total Loss: 1.0821 | Cls: 0.0630 | Dom: 1.2737 | DomAcc: 0.6454\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.992497, test Acc: 0.4413\n",
      "[Epoch 4] Total Loss: 1.1115 | Cls: 0.0695 | Dom: 1.3025 | DomAcc: 0.6242\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.874673, test Acc: 0.4382\n",
      "[Epoch 5] Total Loss: 1.0684 | Cls: 0.0725 | Dom: 1.2448 | DomAcc: 0.6642\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.328156, test Acc: 0.4331\n",
      "[Epoch 6] Total Loss: 1.0800 | Cls: 0.0498 | Dom: 1.2878 | DomAcc: 0.6309\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.347125, test Acc: 0.3115\n",
      "[Epoch 7] Total Loss: 1.1123 | Cls: 0.0389 | Dom: 1.3417 | DomAcc: 0.5885\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.166218, test Acc: 0.3647\n",
      "[Epoch 8] Total Loss: 1.0798 | Cls: 0.0345 | Dom: 1.3066 | DomAcc: 0.6282\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.026965, test Acc: 0.3126\n",
      "[Epoch 9] Total Loss: 1.1150 | Cls: 0.0391 | Dom: 1.3448 | DomAcc: 0.5955\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.233624, test Acc: 0.3453\n",
      "[Epoch 10] Total Loss: 1.1029 | Cls: 0.0427 | Dom: 1.3257 | DomAcc: 0.5872\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.128353, test Acc: 0.3606\n",
      "[Epoch 11] Total Loss: 0.6876 | Cls: 0.0261 | Dom: 1.3229 | DomAcc: 0.6101\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.788423, test Acc: 0.3197\n",
      "[Epoch 12] Total Loss: 0.6843 | Cls: 0.0276 | Dom: 1.3134 | DomAcc: 0.6219\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.815337, test Acc: 0.3851\n",
      "[Epoch 13] Total Loss: 0.6835 | Cls: 0.0172 | Dom: 1.3326 | DomAcc: 0.6069\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.795501, test Acc: 0.2993\n",
      "[Epoch 14] Total Loss: 0.6818 | Cls: 0.0170 | Dom: 1.3296 | DomAcc: 0.6000\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.662990, test Acc: 0.3044\n",
      "[Epoch 15] Total Loss: 0.6958 | Cls: 0.0131 | Dom: 1.3652 | DomAcc: 0.5641\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.780650, test Acc: 0.2584\n",
      "[Epoch 16] Total Loss: 0.6923 | Cls: 0.0131 | Dom: 1.3585 | DomAcc: 0.5791\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.521843, test Acc: 0.2584\n",
      "[Epoch 17] Total Loss: 0.6810 | Cls: 0.0117 | Dom: 1.3387 | DomAcc: 0.5982\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.235641, test Acc: 0.2135\n",
      "[Epoch 18] Total Loss: 0.6944 | Cls: 0.0168 | Dom: 1.3551 | DomAcc: 0.5719\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.623472, test Acc: 0.2176\n",
      "[Epoch 19] Total Loss: 0.6879 | Cls: 0.0058 | Dom: 1.3642 | DomAcc: 0.5577\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.317632, test Acc: 0.2421\n",
      "[Epoch 20] Total Loss: 0.6944 | Cls: 0.0105 | Dom: 1.3678 | DomAcc: 0.5563\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.752021, test Acc: 0.2421\n",
      "[Epoch 21] Total Loss: 0.4170 | Cls: 0.0065 | Dom: 1.3686 | DomAcc: 0.5593\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.482497, test Acc: 0.2339\n",
      "[Epoch 22] Total Loss: 0.4196 | Cls: 0.0102 | Dom: 1.3647 | DomAcc: 0.5709\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.816940, test Acc: 0.2319\n",
      "[Epoch 23] Total Loss: 0.4128 | Cls: 0.0038 | Dom: 1.3633 | DomAcc: 0.5751\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 8.078077, test Acc: 0.2308\n",
      "[Epoch 24] Total Loss: 0.4157 | Cls: 0.0026 | Dom: 1.3768 | DomAcc: 0.5436\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 8.691498, test Acc: 0.2247\n",
      "[Epoch 25] Total Loss: 0.4153 | Cls: 0.0040 | Dom: 1.3708 | DomAcc: 0.5563\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 8.670503, test Acc: 0.2278\n",
      "[Epoch 26] Total Loss: 0.4151 | Cls: 0.0011 | Dom: 1.3802 | DomAcc: 0.5332\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 8.814206, test Acc: 0.2288\n",
      "[Epoch 27] Total Loss: 0.4154 | Cls: 0.0005 | Dom: 1.3829 | DomAcc: 0.5307\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.199013, test Acc: 0.2227\n",
      "[Epoch 28] Total Loss: 0.4130 | Cls: 0.0006 | Dom: 1.3749 | DomAcc: 0.5500\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.168770, test Acc: 0.2237\n",
      "[Epoch 29] Total Loss: 0.4164 | Cls: 0.0010 | Dom: 1.3846 | DomAcc: 0.5086\n",
      "[INFO] patience 1 / 3\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 30] Total Loss: 0.4195 | Cls: 0.0004 | Dom: 1.3971 | DomAcc: 0.4806\n",
      "[INFO] patience 2 / 3\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 31] Total Loss: 0.4204 | Cls: 0.0004 | Dom: 1.4000 | DomAcc: 0.4631\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 32] Total Loss: 0.4213 | Cls: 0.0005 | Dom: 1.4025 | DomAcc: 0.4685\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 33] Total Loss: 0.4227 | Cls: 0.0005 | Dom: 1.4073 | DomAcc: 0.4554\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 34] Total Loss: 0.4241 | Cls: 0.0006 | Dom: 1.4115 | DomAcc: 0.4423\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 35] Total Loss: 0.4242 | Cls: 0.0020 | Dom: 1.4073 | DomAcc: 0.4529\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 36] Total Loss: 0.4226 | Cls: 0.0005 | Dom: 1.4071 | DomAcc: 0.4345\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 37] Total Loss: 0.4305 | Cls: 0.0139 | Dom: 1.3889 | DomAcc: 0.5035\n",
      "[INFO] patience 1 / 3\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n",
      "[Epoch 38] Total Loss: 0.4152 | Cls: 0.0088 | Dom: 1.3546 | DomAcc: 0.5744\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n",
      "[Epoch 39] Total Loss: 0.4121 | Cls: 0.0053 | Dom: 1.3560 | DomAcc: 0.5804\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n",
      "[Epoch 40] Total Loss: 0.4155 | Cls: 0.0049 | Dom: 1.3689 | DomAcc: 0.5571\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T10:28:55.873459Z",
     "start_time": "2025-08-14T10:14:05.565444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, total_loss_sum = 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            bs_src, bs_tgt = src_x.size(0), tgt_x.size(0)\n",
    "            loss_dom_src = criterion_domain(dom_out_src, dom_label_src)\n",
    "            loss_dom_tgt = criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 样本数加权的“单个域损失均值”\n",
    "            loss_dom = (loss_dom_src * bs_src + loss_dom_tgt * bs_tgt) / (bs_src + bs_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cls_loss_sum += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "        avg_cls_loss = cls_loss_sum / total_cls_samples\n",
    "        avg_dom_loss = dom_loss_sum / total_dom_samples\n",
    "        avg_total_loss = total_loss_sum / total_dom_samples\n",
    "\n",
    "        # 域分类准确率（整轮）\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total Loss: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.03 and avg_cls_loss < 0.5 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "\n",
    "    if best_model_state is not None:\n",
    "\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=94)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T194_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "321c20d9f0befc96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 0.6434 | Cls: 0.4293 | Dom: 0.4273 | DomAcc: 0.8061\n",
      "[Epoch 2] Total Loss: 0.3447 | Cls: 0.0964 | Dom: 0.4963 | DomAcc: 0.7478\n",
      "[Epoch 3] Total Loss: 0.3646 | Cls: 0.0772 | Dom: 0.5748 | DomAcc: 0.6913\n",
      "[Epoch 4] Total Loss: 0.3461 | Cls: 0.0578 | Dom: 0.5765 | DomAcc: 0.7081\n",
      "[Epoch 5] Total Loss: 0.3559 | Cls: 0.0550 | Dom: 0.6017 | DomAcc: 0.6849\n",
      "[Epoch 6] Total Loss: 0.3525 | Cls: 0.0476 | Dom: 0.6096 | DomAcc: 0.6699\n",
      "[Epoch 7] Total Loss: 0.3397 | Cls: 0.0334 | Dom: 0.6124 | DomAcc: 0.6695\n",
      "[Epoch 8] Total Loss: 0.3448 | Cls: 0.0378 | Dom: 0.6140 | DomAcc: 0.6650\n",
      "[Epoch 9] Total Loss: 0.3503 | Cls: 0.0371 | Dom: 0.6263 | DomAcc: 0.6551\n",
      "[Epoch 10] Total Loss: 0.3501 | Cls: 0.0275 | Dom: 0.6452 | DomAcc: 0.6216\n",
      "[Epoch 11] Total Loss: 0.3472 | Cls: 0.0229 | Dom: 0.6485 | DomAcc: 0.6230\n",
      "[Epoch 12] Total Loss: 0.3577 | Cls: 0.0248 | Dom: 0.6657 | DomAcc: 0.5986\n",
      "[Epoch 13] Total Loss: 0.3550 | Cls: 0.0236 | Dom: 0.6629 | DomAcc: 0.6063\n",
      "[Epoch 14] Total Loss: 0.3411 | Cls: 0.0078 | Dom: 0.6665 | DomAcc: 0.5966\n",
      "[Epoch 15] Total Loss: 0.3530 | Cls: 0.0161 | Dom: 0.6740 | DomAcc: 0.5745\n",
      "[Epoch 16] Total Loss: 0.3499 | Cls: 0.0131 | Dom: 0.6736 | DomAcc: 0.5753\n",
      "[Epoch 17] Total Loss: 0.3440 | Cls: 0.0043 | Dom: 0.6794 | DomAcc: 0.5592\n",
      "[Epoch 18] Total Loss: 0.3458 | Cls: 0.0053 | Dom: 0.6809 | DomAcc: 0.5629\n",
      "[Epoch 19] Total Loss: 0.3523 | Cls: 0.0104 | Dom: 0.6838 | DomAcc: 0.5535\n",
      "[Epoch 20] Total Loss: 0.3480 | Cls: 0.0082 | Dom: 0.6802 | DomAcc: 0.5700\n",
      "[Epoch 21] Total Loss: 0.3466 | Cls: 0.0083 | Dom: 0.6765 | DomAcc: 0.5780\n",
      "[Epoch 22] Total Loss: 0.3462 | Cls: 0.0040 | Dom: 0.6843 | DomAcc: 0.5544\n",
      "[Epoch 23] Total Loss: 0.3471 | Cls: 0.0035 | Dom: 0.6870 | DomAcc: 0.5460\n",
      "[Epoch 24] Total Loss: 0.3480 | Cls: 0.0029 | Dom: 0.6902 | DomAcc: 0.5275\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 25] Total Loss: 0.3477 | Cls: 0.0021 | Dom: 0.6912 | DomAcc: 0.5267\n",
      "[INFO] patience 2 / 3\n",
      "[Epoch 26] Total Loss: 0.3475 | Cls: 0.0022 | Dom: 0.6906 | DomAcc: 0.5357\n",
      "[Epoch 27] Total Loss: 0.3478 | Cls: 0.0012 | Dom: 0.6933 | DomAcc: 0.5151\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 28] Total Loss: 0.3483 | Cls: 0.0009 | Dom: 0.6950 | DomAcc: 0.4962\n",
      "[INFO] patience 2 / 3\n",
      "[Epoch 29] Total Loss: 0.3487 | Cls: 0.0012 | Dom: 0.6950 | DomAcc: 0.4891\n",
      "[INFO] patience 3 / 3\n",
      "[Epoch 30] Total Loss: 0.3489 | Cls: 0.0006 | Dom: 0.6966 | DomAcc: 0.4747\n",
      "[INFO] patience 4 / 3\n",
      "[INFO] Early stopping: domain aligned and classifier converged.\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.544845, test Acc: 0.6629\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T10:53:30.805637Z",
     "start_time": "2025-08-14T10:31:31.548257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, total_loss_sum = 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            bs_src, bs_tgt = src_x.size(0), tgt_x.size(0)\n",
    "            loss_dom_src = criterion_domain(dom_out_src, dom_label_src)\n",
    "            loss_dom_tgt = criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 样本数加权的“单个域损失均值”\n",
    "            loss_dom = (loss_dom_src * bs_src + loss_dom_tgt * bs_tgt) / (bs_src + bs_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cls_loss_sum += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "        avg_cls_loss = cls_loss_sum / total_cls_samples\n",
    "        avg_dom_loss = dom_loss_sum / total_dom_samples\n",
    "        avg_total_loss = total_loss_sum / total_dom_samples\n",
    "\n",
    "        # 域分类准确率（整轮）\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total Loss: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.03 and avg_cls_loss < 0.5 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=97)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T197_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T197_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "8e36c72ed4365506",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 0.9912 | Cls: 0.7145 | Dom: 0.5539 | DomAcc: 0.6969\n",
      "[Epoch 2] Total Loss: 0.4013 | Cls: 0.1274 | Dom: 0.5478 | DomAcc: 0.6991\n",
      "[Epoch 3] Total Loss: 0.3422 | Cls: 0.0700 | Dom: 0.5444 | DomAcc: 0.7088\n",
      "[Epoch 4] Total Loss: 0.3666 | Cls: 0.0772 | Dom: 0.5787 | DomAcc: 0.6749\n",
      "[Epoch 5] Total Loss: 0.3608 | Cls: 0.0582 | Dom: 0.6052 | DomAcc: 0.6564\n",
      "[Epoch 6] Total Loss: 0.3631 | Cls: 0.0549 | Dom: 0.6164 | DomAcc: 0.6455\n",
      "[Epoch 7] Total Loss: 0.3532 | Cls: 0.0450 | Dom: 0.6165 | DomAcc: 0.6389\n",
      "[Epoch 8] Total Loss: 0.3536 | Cls: 0.0415 | Dom: 0.6242 | DomAcc: 0.6320\n",
      "[Epoch 9] Total Loss: 0.3472 | Cls: 0.0373 | Dom: 0.6198 | DomAcc: 0.6329\n",
      "[Epoch 10] Total Loss: 0.3488 | Cls: 0.0395 | Dom: 0.6187 | DomAcc: 0.6391\n",
      "[Epoch 11] Total Loss: 0.3452 | Cls: 0.0315 | Dom: 0.6273 | DomAcc: 0.6315\n",
      "[Epoch 12] Total Loss: 0.3381 | Cls: 0.0230 | Dom: 0.6301 | DomAcc: 0.6310\n",
      "[Epoch 13] Total Loss: 0.3513 | Cls: 0.0270 | Dom: 0.6486 | DomAcc: 0.6156\n",
      "[Epoch 14] Total Loss: 0.3528 | Cls: 0.0324 | Dom: 0.6407 | DomAcc: 0.6244\n",
      "[Epoch 15] Total Loss: 0.3425 | Cls: 0.0190 | Dom: 0.6471 | DomAcc: 0.6209\n",
      "[Epoch 16] Total Loss: 0.3453 | Cls: 0.0204 | Dom: 0.6498 | DomAcc: 0.6185\n",
      "[Epoch 17] Total Loss: 0.3515 | Cls: 0.0238 | Dom: 0.6556 | DomAcc: 0.6020\n",
      "[Epoch 18] Total Loss: 0.3454 | Cls: 0.0175 | Dom: 0.6560 | DomAcc: 0.5973\n",
      "[Epoch 19] Total Loss: 0.3417 | Cls: 0.0168 | Dom: 0.6498 | DomAcc: 0.6139\n",
      "[Epoch 20] Total Loss: 0.3465 | Cls: 0.0159 | Dom: 0.6609 | DomAcc: 0.5948\n",
      "[Epoch 21] Total Loss: 0.3406 | Cls: 0.0094 | Dom: 0.6623 | DomAcc: 0.5955\n",
      "[Epoch 22] Total Loss: 0.3385 | Cls: 0.0047 | Dom: 0.6676 | DomAcc: 0.5923\n",
      "[Epoch 23] Total Loss: 0.3405 | Cls: 0.0078 | Dom: 0.6653 | DomAcc: 0.5887\n",
      "[Epoch 24] Total Loss: 0.3385 | Cls: 0.0045 | Dom: 0.6680 | DomAcc: 0.5907\n",
      "[Epoch 25] Total Loss: 0.3398 | Cls: 0.0035 | Dom: 0.6725 | DomAcc: 0.5718\n",
      "[Epoch 26] Total Loss: 0.3438 | Cls: 0.0089 | Dom: 0.6700 | DomAcc: 0.5848\n",
      "[Epoch 27] Total Loss: 0.3366 | Cls: 0.0027 | Dom: 0.6678 | DomAcc: 0.5888\n",
      "[Epoch 28] Total Loss: 0.3414 | Cls: 0.0035 | Dom: 0.6758 | DomAcc: 0.5653\n",
      "[Epoch 29] Total Loss: 0.3363 | Cls: 0.0009 | Dom: 0.6708 | DomAcc: 0.5800\n",
      "[Epoch 30] Total Loss: 0.3399 | Cls: 0.0020 | Dom: 0.6758 | DomAcc: 0.5652\n",
      "[Epoch 31] Total Loss: 0.3415 | Cls: 0.0031 | Dom: 0.6768 | DomAcc: 0.5695\n",
      "[Epoch 32] Total Loss: 0.3405 | Cls: 0.0056 | Dom: 0.6699 | DomAcc: 0.5831\n",
      "[Epoch 33] Total Loss: 0.3400 | Cls: 0.0025 | Dom: 0.6752 | DomAcc: 0.5701\n",
      "[Epoch 34] Total Loss: 0.3412 | Cls: 0.0079 | Dom: 0.6666 | DomAcc: 0.5902\n",
      "[Epoch 35] Total Loss: 0.3391 | Cls: 0.0032 | Dom: 0.6719 | DomAcc: 0.5685\n",
      "[Epoch 36] Total Loss: 0.3416 | Cls: 0.0042 | Dom: 0.6748 | DomAcc: 0.5801\n",
      "[Epoch 37] Total Loss: 0.3450 | Cls: 0.0066 | Dom: 0.6768 | DomAcc: 0.5730\n",
      "[Epoch 38] Total Loss: 0.3370 | Cls: 0.0037 | Dom: 0.6666 | DomAcc: 0.5951\n",
      "[Epoch 39] Total Loss: 0.3487 | Cls: 0.0145 | Dom: 0.6683 | DomAcc: 0.5943\n",
      "[Epoch 40] Total Loss: 0.3523 | Cls: 0.0153 | Dom: 0.6741 | DomAcc: 0.5780\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.510603, test Acc: 0.5475\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:48:42.247655Z",
     "start_time": "2025-08-14T11:32:16.755704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            bs_src, bs_tgt = src_x.size(0), tgt_x.size(0)\n",
    "            loss_dom_src = criterion_domain(dom_out_src, dom_label_src)\n",
    "            loss_dom_tgt = criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 样本数加权的“单个域损失均值”\n",
    "            loss_dom = (loss_dom_src * bs_src + loss_dom_tgt * bs_tgt) / (bs_src + bs_tgt)\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        test_path = '../datasets/target/test/HC_T191_RP.txt'\n",
    "        dataset = PKLDataset(test_path)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, loader, device)\n",
    "\n",
    "    if best_model_state is not None:\n",
    "       model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=91)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T191_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T191_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=30, lambda_dann=0.5, use_mk=True,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "856eab45d038c430",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 0.6300 | Cls: 0.3753 | Dom: 0.5094 | MMD: 0.1848 | DomAcc: 0.7435 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.631674, test Acc: 0.3990\n",
      "[Epoch 2] Total: 0.3158 | Cls: 0.0853 | Dom: 0.4608 | MMD: 0.1471 | DomAcc: 0.7887 | λ_dann: 0.5000 | λ_mmd: 0.0009\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.982124, test Acc: 0.3139\n",
      "[Epoch 3] Total: 0.3364 | Cls: 0.0615 | Dom: 0.5494 | MMD: 0.1451 | DomAcc: 0.7184 | λ_dann: 0.5000 | λ_mmd: 0.0013\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.924983, test Acc: 0.4050\n",
      "[Epoch 4] Total: 0.3546 | Cls: 0.0592 | Dom: 0.5904 | MMD: 0.1306 | DomAcc: 0.6851 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.874240, test Acc: 0.2208\n",
      "[Epoch 5] Total: 0.3726 | Cls: 0.0582 | Dom: 0.6282 | MMD: 0.1258 | DomAcc: 0.6469 | λ_dann: 0.5000 | λ_mmd: 0.0026\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.757627, test Acc: 0.2119\n",
      "[Epoch 6] Total: 0.3467 | Cls: 0.0271 | Dom: 0.6385 | MMD: 0.1139 | DomAcc: 0.6357 | λ_dann: 0.5000 | λ_mmd: 0.0036\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.262479, test Acc: 0.2218\n",
      "[Epoch 7] Total: 0.3495 | Cls: 0.0321 | Dom: 0.6336 | MMD: 0.1088 | DomAcc: 0.6347 | λ_dann: 0.5000 | λ_mmd: 0.0051\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.581030, test Acc: 0.2030\n",
      "[Epoch 8] Total: 0.3512 | Cls: 0.0333 | Dom: 0.6343 | MMD: 0.1088 | DomAcc: 0.6449 | λ_dann: 0.5000 | λ_mmd: 0.0070\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.253156, test Acc: 0.2010\n",
      "[Epoch 9] Total: 0.3443 | Cls: 0.0192 | Dom: 0.6480 | MMD: 0.1124 | DomAcc: 0.6165 | λ_dann: 0.5000 | λ_mmd: 0.0096\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.650774, test Acc: 0.3020\n",
      "[Epoch 10] Total: 0.3652 | Cls: 0.0241 | Dom: 0.6790 | MMD: 0.1265 | DomAcc: 0.5506 | λ_dann: 0.5000 | λ_mmd: 0.0130\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.478562, test Acc: 0.2079\n",
      "[Epoch 11] Total: 0.3508 | Cls: 0.0122 | Dom: 0.6734 | MMD: 0.1085 | DomAcc: 0.5802 | λ_dann: 0.5000 | λ_mmd: 0.0175\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.041997, test Acc: 0.2297\n",
      "[Epoch 12] Total: 0.3527 | Cls: 0.0173 | Dom: 0.6658 | MMD: 0.1096 | DomAcc: 0.5915 | λ_dann: 0.5000 | λ_mmd: 0.0230\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.605245, test Acc: 0.2990\n",
      "[Epoch 13] Total: 0.3470 | Cls: 0.0085 | Dom: 0.6711 | MMD: 0.0992 | DomAcc: 0.5873 | λ_dann: 0.5000 | λ_mmd: 0.0297\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.142836, test Acc: 0.2960\n",
      "[Epoch 14] Total: 0.3601 | Cls: 0.0185 | Dom: 0.6756 | MMD: 0.1024 | DomAcc: 0.5602 | λ_dann: 0.5000 | λ_mmd: 0.0373\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.991709, test Acc: 0.3059\n",
      "[Epoch 15] Total: 0.3476 | Cls: 0.0096 | Dom: 0.6667 | MMD: 0.1014 | DomAcc: 0.5901 | λ_dann: 0.5000 | λ_mmd: 0.0457\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.987121, test Acc: 0.2941\n",
      "[Epoch 16] Total: 0.3528 | Cls: 0.0077 | Dom: 0.6804 | MMD: 0.0901 | DomAcc: 0.5593 | λ_dann: 0.5000 | λ_mmd: 0.0543\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.100523, test Acc: 0.3693\n",
      "[Epoch 17] Total: 0.3498 | Cls: 0.0097 | Dom: 0.6688 | MMD: 0.0918 | DomAcc: 0.5785 | λ_dann: 0.5000 | λ_mmd: 0.0627\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.046659, test Acc: 0.2891\n",
      "[Epoch 18] Total: 0.3539 | Cls: 0.0128 | Dom: 0.6701 | MMD: 0.0863 | DomAcc: 0.5802 | λ_dann: 0.5000 | λ_mmd: 0.0703\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.559161, test Acc: 0.2495\n",
      "[Epoch 19] Total: 0.3488 | Cls: 0.0032 | Dom: 0.6782 | MMD: 0.0845 | DomAcc: 0.5642 | λ_dann: 0.5000 | λ_mmd: 0.0770\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.540289, test Acc: 0.2861\n",
      "[Epoch 20] Total: 0.3536 | Cls: 0.0037 | Dom: 0.6854 | MMD: 0.0866 | DomAcc: 0.5438 | λ_dann: 0.5000 | λ_mmd: 0.0825\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.495356, test Acc: 0.3614\n",
      "[Epoch 21] Total: 0.3564 | Cls: 0.0036 | Dom: 0.6923 | MMD: 0.0765 | DomAcc: 0.5078 | λ_dann: 0.5000 | λ_mmd: 0.0870\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.956844, test Acc: 0.3564\n",
      "[Epoch 22] Total: 0.3612 | Cls: 0.0103 | Dom: 0.6883 | MMD: 0.0738 | DomAcc: 0.5256 | λ_dann: 0.5000 | λ_mmd: 0.0904\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.124723, test Acc: 0.2980\n",
      "[Epoch 23] Total: 0.3557 | Cls: 0.0041 | Dom: 0.6892 | MMD: 0.0745 | DomAcc: 0.5264 | λ_dann: 0.5000 | λ_mmd: 0.0930\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.181276, test Acc: 0.3455\n",
      "[Epoch 24] Total: 0.3549 | Cls: 0.0014 | Dom: 0.6941 | MMD: 0.0678 | DomAcc: 0.5077 | λ_dann: 0.5000 | λ_mmd: 0.0949\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.895395, test Acc: 0.3881\n",
      "[Epoch 25] Total: 0.3586 | Cls: 0.0048 | Dom: 0.6959 | MMD: 0.0604 | DomAcc: 0.4877 | λ_dann: 0.5000 | λ_mmd: 0.0964\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.424189, test Acc: 0.3812\n",
      "[Epoch 26] Total: 0.3533 | Cls: 0.0006 | Dom: 0.6945 | MMD: 0.0557 | DomAcc: 0.4874 | λ_dann: 0.5000 | λ_mmd: 0.0974\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.037573, test Acc: 0.3901\n",
      "[Epoch 27] Total: 0.3521 | Cls: 0.0005 | Dom: 0.6940 | MMD: 0.0471 | DomAcc: 0.4934 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.149745, test Acc: 0.3891\n",
      "[Epoch 28] Total: 0.3519 | Cls: 0.0006 | Dom: 0.6941 | MMD: 0.0429 | DomAcc: 0.4868 | λ_dann: 0.5000 | λ_mmd: 0.0987\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.930147, test Acc: 0.3842\n",
      "[Epoch 29] Total: 0.3546 | Cls: 0.0027 | Dom: 0.6951 | MMD: 0.0441 | DomAcc: 0.4726 | λ_dann: 0.5000 | λ_mmd: 0.0991\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.971373, test Acc: 0.3861\n",
      "[Epoch 30] Total: 0.3513 | Cls: 0.0003 | Dom: 0.6943 | MMD: 0.0388 | DomAcc: 0.4796 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.211796, test Acc: 0.3851\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.211796, test Acc: 0.3851\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
