{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T22:24:16.079704Z",
     "start_time": "2025-08-10T22:24:13.676892Z"
    }
   },
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.pseudo_train_and_test import pseudo_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T22:24:17.599904Z",
     "start_time": "2025-08-10T22:24:17.594558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1."
   ],
   "id": "50946c2804937d95",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T22:37:29.562586Z",
     "start_time": "2025-08-10T22:24:45.489120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_cls_loss, total_dom_loss = 0.0, 0.0, 0.0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "        num_batches = 0\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            num_batches += 1\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            total_dom_loss += loss_dom.item()\n",
    "\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        avg_cls_loss = total_cls_loss / num_batches\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {total_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "        # print(\"[INFO] Evaluating on target test set...\")\n",
    "        # target_test_path = '../datasets/HC_T185_RP.txt'\n",
    "        # test_dataset = PKLDataset(target_test_path)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        # pseudo_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "\n",
    "\n",
    "        if gap < 0.005 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=30, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    pseudo_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "77ac832e8182a6d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 266.3570 | Cls: 0.5783 | Dom: 243.5677 | DomAcc: 0.7468\n",
      "[Epoch 2] Total Loss: 165.3113 | Cls: 0.1503 | Dom: 255.4878 | DomAcc: 0.7335\n",
      "[Epoch 3] Total Loss: 163.9266 | Cls: 0.1208 | Dom: 267.4597 | DomAcc: 0.7175\n",
      "[Epoch 4] Total Loss: 172.1174 | Cls: 0.0968 | Dom: 295.8527 | DomAcc: 0.6724\n",
      "[Epoch 5] Total Loss: 170.2584 | Cls: 0.0913 | Dom: 294.8573 | DomAcc: 0.6853\n",
      "[Epoch 6] Total Loss: 167.0112 | Cls: 0.0723 | Dom: 297.8749 | DomAcc: 0.6864\n",
      "[Epoch 7] Total Loss: 174.1572 | Cls: 0.0752 | Dom: 310.6978 | DomAcc: 0.6551\n",
      "[Epoch 8] Total Loss: 174.1581 | Cls: 0.0627 | Dom: 316.9883 | DomAcc: 0.6460\n",
      "[Epoch 9] Total Loss: 174.9486 | Cls: 0.0472 | Dom: 326.2841 | DomAcc: 0.6196\n",
      "[Epoch 10] Total Loss: 175.5995 | Cls: 0.0569 | Dom: 322.7432 | DomAcc: 0.6314\n",
      "[Epoch 11] Total Loss: 173.8257 | Cls: 0.0440 | Dom: 325.6300 | DomAcc: 0.6222\n",
      "[Epoch 12] Total Loss: 169.2174 | Cls: 0.0321 | Dom: 322.3864 | DomAcc: 0.6229\n",
      "[Epoch 13] Total Loss: 176.0753 | Cls: 0.0349 | Dom: 334.6986 | DomAcc: 0.5908\n",
      "[Epoch 14] Total Loss: 169.8253 | Cls: 0.0243 | Dom: 327.5195 | DomAcc: 0.6185\n",
      "[Epoch 15] Total Loss: 170.2498 | Cls: 0.0236 | Dom: 328.7139 | DomAcc: 0.6122\n",
      "[Epoch 16] Total Loss: 170.4840 | Cls: 0.0240 | Dom: 328.9634 | DomAcc: 0.6054\n",
      "[Epoch 17] Total Loss: 171.3726 | Cls: 0.0214 | Dom: 332.0222 | DomAcc: 0.5926\n",
      "[Epoch 18] Total Loss: 169.1052 | Cls: 0.0102 | Dom: 333.1313 | DomAcc: 0.5943\n",
      "[Epoch 19] Total Loss: 169.7375 | Cls: 0.0100 | Dom: 334.4958 | DomAcc: 0.5873\n",
      "[Epoch 20] Total Loss: 171.9487 | Cls: 0.0140 | Dom: 336.8867 | DomAcc: 0.5790\n",
      "[Epoch 21] Total Loss: 172.2447 | Cls: 0.0148 | Dom: 337.1143 | DomAcc: 0.5840\n",
      "[Epoch 22] Total Loss: 169.6153 | Cls: 0.0077 | Dom: 335.4009 | DomAcc: 0.5847\n",
      "[Epoch 23] Total Loss: 172.4745 | Cls: 0.0095 | Dom: 340.2110 | DomAcc: 0.5665\n",
      "[Epoch 24] Total Loss: 171.9875 | Cls: 0.0052 | Dom: 341.3814 | DomAcc: 0.5656\n",
      "[Epoch 25] Total Loss: 172.5836 | Cls: 0.0034 | Dom: 343.4852 | DomAcc: 0.5577\n",
      "[Epoch 26] Total Loss: 173.5033 | Cls: 0.0027 | Dom: 345.6678 | DomAcc: 0.5446\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 27] Total Loss: 172.6297 | Cls: 0.0015 | Dom: 344.5312 | DomAcc: 0.5491\n",
      "[INFO] patience 2 / 3\n",
      "[Epoch 28] Total Loss: 173.9998 | Cls: 0.0046 | Dom: 345.7061 | DomAcc: 0.5387\n",
      "[INFO] patience 3 / 3\n",
      "[Epoch 29] Total Loss: 173.6280 | Cls: 0.0049 | Dom: 344.8058 | DomAcc: 0.5455\n",
      "[INFO] patience 4 / 3\n",
      "[INFO] Early stopping: domain aligned and classifier converged.\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.660323, test Acc: 0.6274\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T23:49:22.471364Z",
     "start_time": "2025-08-10T23:28:26.773207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_cls_loss, total_dom_loss = 0.0, 0.0, 0.0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "        num_batches = 0\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            num_batches += 1\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            total_dom_loss += loss_dom.item()\n",
    "\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        avg_cls_loss = total_cls_loss / num_batches\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {total_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "        # print(\"[INFO] Evaluating on target test set...\")\n",
    "        # target_test_path = '../datasets/HC_T185_RP.txt'\n",
    "        # test_dataset = PKLDataset(target_test_path)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        # pseudo_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "\n",
    "\n",
    "        if gap < 0.005 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "        if best_model_state is not None:\n",
    "            # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=44)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    pseudo_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "427d3adb6a872972",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 312.2282 | Cls: 0.7016 | Dom: 273.6700 | DomAcc: 0.6900\n",
      "[Epoch 2] Total Loss: 173.5931 | Cls: 0.1267 | Dom: 283.8536 | DomAcc: 0.6893\n",
      "[Epoch 3] Total Loss: 192.4783 | Cls: 0.0823 | Dom: 343.7952 | DomAcc: 0.5965\n",
      "[Epoch 4] Total Loss: 191.1798 | Cls: 0.0665 | Dom: 349.0959 | DomAcc: 0.5167\n",
      "[Epoch 5] Total Loss: 183.9482 | Cls: 0.0480 | Dom: 343.8882 | DomAcc: 0.5465\n",
      "[Epoch 6] Total Loss: 177.7356 | Cls: 0.0400 | Dom: 335.4661 | DomAcc: 0.5980\n",
      "[Epoch 7] Total Loss: 171.7944 | Cls: 0.0369 | Dom: 325.1553 | DomAcc: 0.6084\n",
      "[Epoch 8] Total Loss: 172.1987 | Cls: 0.0373 | Dom: 325.7460 | DomAcc: 0.6006\n",
      "[Epoch 9] Total Loss: 176.5720 | Cls: 0.0323 | Dom: 336.9959 | DomAcc: 0.5458\n",
      "[Epoch 10] Total Loss: 171.5147 | Cls: 0.0395 | Dom: 323.2586 | DomAcc: 0.6296\n",
      "[Epoch 11] Total Loss: 173.0376 | Cls: 0.0349 | Dom: 328.6305 | DomAcc: 0.6055\n",
      "[Epoch 12] Total Loss: 175.7335 | Cls: 0.0274 | Dom: 337.7629 | DomAcc: 0.5706\n",
      "[Epoch 13] Total Loss: 158.0050 | Cls: 0.0303 | Dom: 300.8803 | DomAcc: 0.6860\n",
      "[Epoch 14] Total Loss: 163.8128 | Cls: 0.0235 | Dom: 315.8598 | DomAcc: 0.6439\n",
      "[Epoch 15] Total Loss: 166.8970 | Cls: 0.0320 | Dom: 317.7830 | DomAcc: 0.6410\n",
      "[Epoch 16] Total Loss: 165.2495 | Cls: 0.0215 | Dom: 319.7723 | DomAcc: 0.6411\n",
      "[Epoch 17] Total Loss: 167.0120 | Cls: 0.0123 | Dom: 327.8880 | DomAcc: 0.6292\n",
      "[Epoch 18] Total Loss: 168.7767 | Cls: 0.0146 | Dom: 330.2374 | DomAcc: 0.6082\n",
      "[Epoch 19] Total Loss: 166.5901 | Cls: 0.0157 | Dom: 325.3186 | DomAcc: 0.6364\n",
      "[Epoch 20] Total Loss: 171.9311 | Cls: 0.0144 | Dom: 336.6430 | DomAcc: 0.5807\n",
      "[Epoch 21] Total Loss: 171.4858 | Cls: 0.0110 | Dom: 337.4656 | DomAcc: 0.5818\n",
      "[Epoch 22] Total Loss: 171.9399 | Cls: 0.0066 | Dom: 340.5788 | DomAcc: 0.5672\n",
      "[Epoch 23] Total Loss: 173.7555 | Cls: 0.0057 | Dom: 344.6491 | DomAcc: 0.5621\n",
      "[Epoch 24] Total Loss: 175.3119 | Cls: 0.0062 | Dom: 347.5354 | DomAcc: 0.5177\n",
      "[Epoch 25] Total Loss: 173.5228 | Cls: 0.0030 | Dom: 345.5297 | DomAcc: 0.5297\n",
      "[Epoch 26] Total Loss: 173.4333 | Cls: 0.0051 | Dom: 344.3332 | DomAcc: 0.5545\n",
      "[Epoch 27] Total Loss: 173.8225 | Cls: 0.0016 | Dom: 346.8429 | DomAcc: 0.5358\n",
      "[Epoch 28] Total Loss: 173.6588 | Cls: 0.0025 | Dom: 346.0501 | DomAcc: 0.5343\n",
      "[Epoch 29] Total Loss: 173.9371 | Cls: 0.0017 | Dom: 347.0200 | DomAcc: 0.5198\n",
      "[Epoch 30] Total Loss: 174.2248 | Cls: 0.0035 | Dom: 346.7077 | DomAcc: 0.5182\n",
      "[Epoch 31] Total Loss: 174.0182 | Cls: 0.0007 | Dom: 347.7031 | DomAcc: 0.4978\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 32] Total Loss: 174.4568 | Cls: 0.0007 | Dom: 348.5603 | DomAcc: 0.4757\n",
      "[Epoch 33] Total Loss: 174.9165 | Cls: 0.0084 | Dom: 345.6556 | DomAcc: 0.5233\n",
      "[Epoch 34] Total Loss: 174.6864 | Cls: 0.0029 | Dom: 347.9100 | DomAcc: 0.4839\n",
      "[Epoch 35] Total Loss: 175.0960 | Cls: 0.0072 | Dom: 346.6143 | DomAcc: 0.5134\n",
      "[Epoch 36] Total Loss: 174.1200 | Cls: 0.0031 | Dom: 346.7082 | DomAcc: 0.5057\n",
      "[Epoch 37] Total Loss: 174.7319 | Cls: 0.0065 | Dom: 346.2367 | DomAcc: 0.5144\n",
      "[Epoch 38] Total Loss: 174.2948 | Cls: 0.0077 | Dom: 344.7448 | DomAcc: 0.5242\n",
      "[Epoch 39] Total Loss: 173.9282 | Cls: 0.0052 | Dom: 345.2785 | DomAcc: 0.5308\n",
      "[Epoch 40] Total Loss: 176.3113 | Cls: 0.0159 | Dom: 344.6571 | DomAcc: 0.5262\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.679201, test Acc: 0.6826\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T00:19:41.605718Z",
     "start_time": "2025-08-11T00:01:10.429953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_cls_loss, total_dom_loss = 0.0, 0.0, 0.0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "        num_batches = 0\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            num_batches += 1\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            total_dom_loss += loss_dom.item()\n",
    "\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        avg_cls_loss = total_cls_loss / num_batches\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {total_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.02 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "        if best_model_state is not None:\n",
    "            # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=44)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    pseudo_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "eae45690c147fc83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 301.3290 | Cls: 0.7057 | Dom: 249.8221 | DomAcc: 0.7611\n",
      "[Epoch 2] Total Loss: 170.2509 | Cls: 0.1536 | Dom: 263.6899 | DomAcc: 0.7250\n",
      "[Epoch 3] Total Loss: 166.8857 | Cls: 0.0880 | Dom: 289.7780 | DomAcc: 0.6844\n",
      "[Epoch 4] Total Loss: 173.9117 | Cls: 0.0730 | Dom: 311.3126 | DomAcc: 0.6421\n",
      "[Epoch 5] Total Loss: 172.0922 | Cls: 0.0737 | Dom: 307.3454 | DomAcc: 0.6677\n",
      "[Epoch 6] Total Loss: 171.1056 | Cls: 0.0514 | Dom: 316.5218 | DomAcc: 0.6331\n",
      "[Epoch 7] Total Loss: 164.3007 | Cls: 0.0284 | Dom: 314.4232 | DomAcc: 0.6486\n",
      "[Epoch 8] Total Loss: 164.9267 | Cls: 0.0443 | Dom: 307.6916 | DomAcc: 0.6545\n",
      "[Epoch 9] Total Loss: 169.1464 | Cls: 0.0382 | Dom: 319.1860 | DomAcc: 0.6306\n",
      "[Epoch 10] Total Loss: 165.3398 | Cls: 0.0345 | Dom: 313.4244 | DomAcc: 0.6603\n",
      "[Epoch 11] Total Loss: 172.6444 | Cls: 0.0324 | Dom: 329.0802 | DomAcc: 0.5953\n",
      "[Epoch 12] Total Loss: 174.2674 | Cls: 0.0225 | Dom: 337.2850 | DomAcc: 0.5726\n",
      "[Epoch 13] Total Loss: 162.1156 | Cls: 0.0213 | Dom: 313.5987 | DomAcc: 0.6895\n",
      "[Epoch 14] Total Loss: 165.5682 | Cls: 0.0207 | Dom: 320.7851 | DomAcc: 0.6297\n",
      "[Epoch 15] Total Loss: 174.6375 | Cls: 0.0197 | Dom: 339.4491 | DomAcc: 0.5512\n",
      "[Epoch 16] Total Loss: 171.4122 | Cls: 0.0191 | Dom: 333.2582 | DomAcc: 0.6052\n",
      "[Epoch 17] Total Loss: 164.2865 | Cls: 0.0190 | Dom: 319.0708 | DomAcc: 0.6556\n",
      "[Epoch 18] Total Loss: 172.3186 | Cls: 0.0174 | Dom: 335.9139 | DomAcc: 0.5889\n",
      "[Epoch 19] Total Loss: 171.0292 | Cls: 0.0131 | Dom: 335.5013 | DomAcc: 0.5874\n",
      "[Epoch 20] Total Loss: 167.7816 | Cls: 0.0122 | Dom: 329.4769 | DomAcc: 0.6392\n",
      "[Epoch 21] Total Loss: 164.8983 | Cls: 0.0102 | Dom: 324.6854 | DomAcc: 0.6275\n",
      "[Epoch 22] Total Loss: 165.8407 | Cls: 0.0056 | Dom: 328.8913 | DomAcc: 0.6092\n",
      "[Epoch 23] Total Loss: 167.4753 | Cls: 0.0086 | Dom: 330.6626 | DomAcc: 0.5925\n",
      "[Epoch 24] Total Loss: 166.5801 | Cls: 0.0074 | Dom: 329.4758 | DomAcc: 0.6248\n",
      "[Epoch 25] Total Loss: 165.3292 | Cls: 0.0063 | Dom: 327.5121 | DomAcc: 0.6103\n",
      "[Epoch 26] Total Loss: 166.2943 | Cls: 0.0046 | Dom: 330.3057 | DomAcc: 0.6037\n",
      "[Epoch 27] Total Loss: 170.2157 | Cls: 0.0076 | Dom: 336.6095 | DomAcc: 0.5714\n",
      "[Epoch 28] Total Loss: 167.9846 | Cls: 0.0034 | Dom: 334.2759 | DomAcc: 0.5864\n",
      "[Epoch 29] Total Loss: 170.5307 | Cls: 0.0029 | Dom: 339.6244 | DomAcc: 0.5692\n",
      "[Epoch 30] Total Loss: 172.6299 | Cls: 0.0037 | Dom: 343.4218 | DomAcc: 0.5314\n",
      "[Epoch 31] Total Loss: 174.4848 | Cls: 0.0047 | Dom: 346.6112 | DomAcc: 0.5210\n",
      "[Epoch 32] Total Loss: 170.8451 | Cls: 0.0019 | Dom: 340.7362 | DomAcc: 0.5649\n",
      "[Epoch 33] Total Loss: 172.3244 | Cls: 0.0054 | Dom: 341.9464 | DomAcc: 0.5526\n",
      "[Epoch 34] Total Loss: 173.1504 | Cls: 0.0035 | Dom: 344.5557 | DomAcc: 0.5277\n",
      "[Epoch 35] Total Loss: 173.9613 | Cls: 0.0050 | Dom: 345.4012 | DomAcc: 0.5377\n",
      "[Epoch 36] Total Loss: 173.4696 | Cls: 0.0037 | Dom: 345.0814 | DomAcc: 0.5333\n",
      "[Epoch 37] Total Loss: 172.4354 | Cls: 0.0045 | Dom: 342.6015 | DomAcc: 0.5538\n",
      "[Epoch 38] Total Loss: 171.3724 | Cls: 0.0054 | Dom: 340.0693 | DomAcc: 0.5664\n",
      "[Epoch 39] Total Loss: 176.1502 | Cls: 0.0181 | Dom: 343.2485 | DomAcc: 0.5507\n",
      "[Epoch 40] Total Loss: 173.0480 | Cls: 0.0172 | Dom: 337.4858 | DomAcc: 0.5875\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.163152, test Acc: 0.4300\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
