{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-12T22:40:56.658338Z",
     "start_time": "2025-08-12T22:40:53.734176Z"
    }
   },
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T22:51:52.533454Z",
     "start_time": "2025-08-12T22:51:52.527837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    # p = epoch / float(num_epochs)\n",
    "    # return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "    if epoch < 10:\n",
    "        return 0.8\n",
    "    elif epoch < 20:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.3"
   ],
   "id": "50946c2804937d95",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:43:56.039498Z",
     "start_time": "2025-08-11T23:39:44.220299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, total_loss_sum = 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cls_loss_sum += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "        avg_cls_loss = cls_loss_sum / total_cls_samples\n",
    "        avg_dom_loss = dom_loss_sum / total_dom_samples\n",
    "        avg_total_loss = total_loss_sum / total_dom_samples\n",
    "\n",
    "        # 域分类准确率（整轮）\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total Loss: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.03 and avg_cls_loss < 0.5 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=1,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "77ac832e8182a6d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 2.7007 | Cls: 1.3054 | Dom: 1.3953 | DomAcc: 0.5358\n",
      "[Epoch 2] Total Loss: 1.6789 | Cls: 0.4881 | Dom: 1.1908 | DomAcc: 0.6557\n",
      "[Epoch 3] Total Loss: 1.5989 | Cls: 0.3699 | Dom: 1.2290 | DomAcc: 0.6562\n",
      "[Epoch 4] Total Loss: 1.4997 | Cls: 0.2143 | Dom: 1.2854 | DomAcc: 0.6522\n",
      "[Epoch 5] Total Loss: 1.4682 | Cls: 0.1732 | Dom: 1.2950 | DomAcc: 0.6295\n",
      "[Epoch 6] Total Loss: 1.4464 | Cls: 0.1505 | Dom: 1.2959 | DomAcc: 0.6195\n",
      "[Epoch 7] Total Loss: 1.4164 | Cls: 0.1157 | Dom: 1.3007 | DomAcc: 0.6018\n",
      "[Epoch 8] Total Loss: 1.5035 | Cls: 0.1407 | Dom: 1.3628 | DomAcc: 0.5696\n",
      "[Epoch 9] Total Loss: 1.3922 | Cls: 0.0984 | Dom: 1.2938 | DomAcc: 0.6205\n",
      "[Epoch 10] Total Loss: 1.3202 | Cls: 0.1204 | Dom: 1.1998 | DomAcc: 0.7026\n",
      "[Epoch 11] Total Loss: 1.4701 | Cls: 0.0960 | Dom: 1.3740 | DomAcc: 0.5418\n",
      "[Epoch 12] Total Loss: 1.5798 | Cls: 0.0841 | Dom: 1.4956 | DomAcc: 0.4350\n",
      "[Epoch 13] Total Loss: 1.5396 | Cls: 0.0581 | Dom: 1.4815 | DomAcc: 0.4123\n",
      "[Epoch 14] Total Loss: 1.4558 | Cls: 0.0525 | Dom: 1.4033 | DomAcc: 0.4748\n",
      "[Epoch 15] Total Loss: 1.4673 | Cls: 0.0837 | Dom: 1.3836 | DomAcc: 0.5312\n",
      "[Epoch 16] Total Loss: 1.4097 | Cls: 0.0405 | Dom: 1.3692 | DomAcc: 0.5756\n",
      "[Epoch 17] Total Loss: 1.4201 | Cls: 0.0484 | Dom: 1.3717 | DomAcc: 0.5544\n",
      "[Epoch 18] Total Loss: 1.4403 | Cls: 0.0534 | Dom: 1.3869 | DomAcc: 0.5131\n",
      "[Epoch 19] Total Loss: 1.4400 | Cls: 0.0427 | Dom: 1.3973 | DomAcc: 0.4728\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 20] Total Loss: 1.3866 | Cls: 0.0345 | Dom: 1.3521 | DomAcc: 0.6376\n",
      "[Epoch 21] Total Loss: 1.4083 | Cls: 0.0321 | Dom: 1.3762 | DomAcc: 0.5413\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 22] Total Loss: 1.3903 | Cls: 0.0242 | Dom: 1.3660 | DomAcc: 0.5847\n",
      "[Epoch 23] Total Loss: 1.4067 | Cls: 0.0309 | Dom: 1.3758 | DomAcc: 0.5549\n",
      "[Epoch 24] Total Loss: 1.4078 | Cls: 0.0262 | Dom: 1.3817 | DomAcc: 0.5413\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 25] Total Loss: 1.3958 | Cls: 0.0234 | Dom: 1.3724 | DomAcc: 0.5625\n",
      "[Epoch 26] Total Loss: 1.4008 | Cls: 0.0243 | Dom: 1.3765 | DomAcc: 0.5514\n",
      "[Epoch 27] Total Loss: 1.3917 | Cls: 0.0157 | Dom: 1.3761 | DomAcc: 0.5459\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 28] Total Loss: 1.3879 | Cls: 0.0136 | Dom: 1.3742 | DomAcc: 0.5655\n",
      "[Epoch 29] Total Loss: 1.3941 | Cls: 0.0154 | Dom: 1.3787 | DomAcc: 0.5459\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 30] Total Loss: 1.3845 | Cls: 0.0134 | Dom: 1.3711 | DomAcc: 0.5685\n",
      "[Epoch 31] Total Loss: 1.3890 | Cls: 0.0153 | Dom: 1.3737 | DomAcc: 0.5766\n",
      "[Epoch 32] Total Loss: 1.3949 | Cls: 0.0213 | Dom: 1.3736 | DomAcc: 0.5519\n",
      "[Epoch 33] Total Loss: 1.3918 | Cls: 0.0193 | Dom: 1.3725 | DomAcc: 0.5680\n",
      "[Epoch 34] Total Loss: 1.3794 | Cls: 0.0089 | Dom: 1.3705 | DomAcc: 0.5620\n",
      "[Epoch 35] Total Loss: 1.3896 | Cls: 0.0105 | Dom: 1.3791 | DomAcc: 0.5428\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 36] Total Loss: 1.3876 | Cls: 0.0173 | Dom: 1.3703 | DomAcc: 0.5625\n",
      "[Epoch 37] Total Loss: 1.3873 | Cls: 0.0170 | Dom: 1.3703 | DomAcc: 0.5761\n",
      "[Epoch 38] Total Loss: 1.3992 | Cls: 0.0287 | Dom: 1.3705 | DomAcc: 0.5696\n",
      "[Epoch 39] Total Loss: 1.3887 | Cls: 0.0187 | Dom: 1.3700 | DomAcc: 0.5660\n",
      "[Epoch 40] Total Loss: 1.3833 | Cls: 0.0134 | Dom: 1.3699 | DomAcc: 0.5741\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.005660, test Acc: 0.5660\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T23:49:22.471364Z",
     "start_time": "2025-08-10T23:28:26.773207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_cls_loss, total_dom_loss = 0.0, 0.0, 0.0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "        num_batches = 0\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            num_batches += 1\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            total_dom_loss += loss_dom.item()\n",
    "\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        avg_cls_loss = total_cls_loss / num_batches\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {total_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "        # print(\"[INFO] Evaluating on target test set...\")\n",
    "        # target_test_path = '../datasets/HC_T185_RP.txt'\n",
    "        # test_dataset = PKLDataset(target_test_path)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        # pseudo_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "\n",
    "\n",
    "        if gap < 0.005 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "        if best_model_state is not None:\n",
    "            # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=44)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "427d3adb6a872972",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 312.2282 | Cls: 0.7016 | Dom: 273.6700 | DomAcc: 0.6900\n",
      "[Epoch 2] Total Loss: 173.5931 | Cls: 0.1267 | Dom: 283.8536 | DomAcc: 0.6893\n",
      "[Epoch 3] Total Loss: 192.4783 | Cls: 0.0823 | Dom: 343.7952 | DomAcc: 0.5965\n",
      "[Epoch 4] Total Loss: 191.1798 | Cls: 0.0665 | Dom: 349.0959 | DomAcc: 0.5167\n",
      "[Epoch 5] Total Loss: 183.9482 | Cls: 0.0480 | Dom: 343.8882 | DomAcc: 0.5465\n",
      "[Epoch 6] Total Loss: 177.7356 | Cls: 0.0400 | Dom: 335.4661 | DomAcc: 0.5980\n",
      "[Epoch 7] Total Loss: 171.7944 | Cls: 0.0369 | Dom: 325.1553 | DomAcc: 0.6084\n",
      "[Epoch 8] Total Loss: 172.1987 | Cls: 0.0373 | Dom: 325.7460 | DomAcc: 0.6006\n",
      "[Epoch 9] Total Loss: 176.5720 | Cls: 0.0323 | Dom: 336.9959 | DomAcc: 0.5458\n",
      "[Epoch 10] Total Loss: 171.5147 | Cls: 0.0395 | Dom: 323.2586 | DomAcc: 0.6296\n",
      "[Epoch 11] Total Loss: 173.0376 | Cls: 0.0349 | Dom: 328.6305 | DomAcc: 0.6055\n",
      "[Epoch 12] Total Loss: 175.7335 | Cls: 0.0274 | Dom: 337.7629 | DomAcc: 0.5706\n",
      "[Epoch 13] Total Loss: 158.0050 | Cls: 0.0303 | Dom: 300.8803 | DomAcc: 0.6860\n",
      "[Epoch 14] Total Loss: 163.8128 | Cls: 0.0235 | Dom: 315.8598 | DomAcc: 0.6439\n",
      "[Epoch 15] Total Loss: 166.8970 | Cls: 0.0320 | Dom: 317.7830 | DomAcc: 0.6410\n",
      "[Epoch 16] Total Loss: 165.2495 | Cls: 0.0215 | Dom: 319.7723 | DomAcc: 0.6411\n",
      "[Epoch 17] Total Loss: 167.0120 | Cls: 0.0123 | Dom: 327.8880 | DomAcc: 0.6292\n",
      "[Epoch 18] Total Loss: 168.7767 | Cls: 0.0146 | Dom: 330.2374 | DomAcc: 0.6082\n",
      "[Epoch 19] Total Loss: 166.5901 | Cls: 0.0157 | Dom: 325.3186 | DomAcc: 0.6364\n",
      "[Epoch 20] Total Loss: 171.9311 | Cls: 0.0144 | Dom: 336.6430 | DomAcc: 0.5807\n",
      "[Epoch 21] Total Loss: 171.4858 | Cls: 0.0110 | Dom: 337.4656 | DomAcc: 0.5818\n",
      "[Epoch 22] Total Loss: 171.9399 | Cls: 0.0066 | Dom: 340.5788 | DomAcc: 0.5672\n",
      "[Epoch 23] Total Loss: 173.7555 | Cls: 0.0057 | Dom: 344.6491 | DomAcc: 0.5621\n",
      "[Epoch 24] Total Loss: 175.3119 | Cls: 0.0062 | Dom: 347.5354 | DomAcc: 0.5177\n",
      "[Epoch 25] Total Loss: 173.5228 | Cls: 0.0030 | Dom: 345.5297 | DomAcc: 0.5297\n",
      "[Epoch 26] Total Loss: 173.4333 | Cls: 0.0051 | Dom: 344.3332 | DomAcc: 0.5545\n",
      "[Epoch 27] Total Loss: 173.8225 | Cls: 0.0016 | Dom: 346.8429 | DomAcc: 0.5358\n",
      "[Epoch 28] Total Loss: 173.6588 | Cls: 0.0025 | Dom: 346.0501 | DomAcc: 0.5343\n",
      "[Epoch 29] Total Loss: 173.9371 | Cls: 0.0017 | Dom: 347.0200 | DomAcc: 0.5198\n",
      "[Epoch 30] Total Loss: 174.2248 | Cls: 0.0035 | Dom: 346.7077 | DomAcc: 0.5182\n",
      "[Epoch 31] Total Loss: 174.0182 | Cls: 0.0007 | Dom: 347.7031 | DomAcc: 0.4978\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 32] Total Loss: 174.4568 | Cls: 0.0007 | Dom: 348.5603 | DomAcc: 0.4757\n",
      "[Epoch 33] Total Loss: 174.9165 | Cls: 0.0084 | Dom: 345.6556 | DomAcc: 0.5233\n",
      "[Epoch 34] Total Loss: 174.6864 | Cls: 0.0029 | Dom: 347.9100 | DomAcc: 0.4839\n",
      "[Epoch 35] Total Loss: 175.0960 | Cls: 0.0072 | Dom: 346.6143 | DomAcc: 0.5134\n",
      "[Epoch 36] Total Loss: 174.1200 | Cls: 0.0031 | Dom: 346.7082 | DomAcc: 0.5057\n",
      "[Epoch 37] Total Loss: 174.7319 | Cls: 0.0065 | Dom: 346.2367 | DomAcc: 0.5144\n",
      "[Epoch 38] Total Loss: 174.2948 | Cls: 0.0077 | Dom: 344.7448 | DomAcc: 0.5242\n",
      "[Epoch 39] Total Loss: 173.9282 | Cls: 0.0052 | Dom: 345.2785 | DomAcc: 0.5308\n",
      "[Epoch 40] Total Loss: 176.3113 | Cls: 0.0159 | Dom: 344.6571 | DomAcc: 0.5262\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.679201, test Acc: 0.6826\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T11:09:33.864356Z",
     "start_time": "2025-08-11T10:55:30.315479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_cls_loss, total_dom_loss = 0.0, 0.0, 0.0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "        num_batches = 0\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            num_batches += 1\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            total_dom_loss += loss_dom.item()\n",
    "\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        avg_cls_loss = total_cls_loss / num_batches\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {total_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.03 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "        if best_model_state is not None:\n",
    "            # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=188)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "eae45690c147fc83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 262.9256 | Cls: 0.5452 | Dom: 253.2420 | DomAcc: 0.7581\n",
      "[Epoch 2] Total Loss: 183.4174 | Cls: 0.1369 | Dom: 298.4002 | DomAcc: 0.6896\n",
      "[Epoch 3] Total Loss: 168.9419 | Cls: 0.0898 | Dom: 293.0012 | DomAcc: 0.6950\n",
      "[Epoch 4] Total Loss: 175.1027 | Cls: 0.0783 | Dom: 311.0342 | DomAcc: 0.6667\n",
      "[Epoch 5] Total Loss: 174.7883 | Cls: 0.0521 | Dom: 323.5196 | DomAcc: 0.6151\n",
      "[Epoch 6] Total Loss: 179.4945 | Cls: 0.0465 | Dom: 335.7604 | DomAcc: 0.5862\n",
      "[Epoch 7] Total Loss: 169.3155 | Cls: 0.0487 | Dom: 314.2756 | DomAcc: 0.6539\n",
      "[Epoch 8] Total Loss: 163.3561 | Cls: 0.0370 | Dom: 308.2311 | DomAcc: 0.6749\n",
      "[Epoch 9] Total Loss: 173.5950 | Cls: 0.0430 | Dom: 325.7055 | DomAcc: 0.6066\n",
      "[Epoch 10] Total Loss: 169.5951 | Cls: 0.0224 | Dom: 328.0061 | DomAcc: 0.6178\n",
      "[Epoch 11] Total Loss: 174.5310 | Cls: 0.0343 | Dom: 331.8966 | DomAcc: 0.5922\n",
      "[Epoch 12] Total Loss: 174.0767 | Cls: 0.0216 | Dom: 337.3530 | DomAcc: 0.5893\n",
      "[Epoch 13] Total Loss: 169.0750 | Cls: 0.0122 | Dom: 332.0552 | DomAcc: 0.6106\n",
      "[Epoch 14] Total Loss: 174.1357 | Cls: 0.0201 | Dom: 338.2427 | DomAcc: 0.5656\n",
      "[Epoch 15] Total Loss: 174.8362 | Cls: 0.0195 | Dom: 339.9078 | DomAcc: 0.5660\n",
      "[Epoch 16] Total Loss: 173.0815 | Cls: 0.0111 | Dom: 340.6077 | DomAcc: 0.5632\n",
      "[Epoch 17] Total Loss: 170.6753 | Cls: 0.0095 | Dom: 336.5766 | DomAcc: 0.5885\n",
      "[Epoch 18] Total Loss: 172.2456 | Cls: 0.0107 | Dom: 339.1321 | DomAcc: 0.5635\n",
      "[Epoch 19] Total Loss: 174.4030 | Cls: 0.0155 | Dom: 341.0477 | DomAcc: 0.5572\n",
      "[Epoch 20] Total Loss: 172.0058 | Cls: 0.0031 | Dom: 342.4438 | DomAcc: 0.5626\n",
      "[Epoch 21] Total Loss: 172.3752 | Cls: 0.0100 | Dom: 339.7651 | DomAcc: 0.5799\n",
      "[Epoch 22] Total Loss: 172.9220 | Cls: 0.0037 | Dom: 343.9965 | DomAcc: 0.5358\n",
      "[Epoch 23] Total Loss: 173.4074 | Cls: 0.0015 | Dom: 346.0431 | DomAcc: 0.5104\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 24] Total Loss: 172.7256 | Cls: 0.0028 | Dom: 344.0617 | DomAcc: 0.5435\n",
      "[Epoch 25] Total Loss: 172.9357 | Cls: 0.0005 | Dom: 345.6023 | DomAcc: 0.5227\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 26] Total Loss: 174.6764 | Cls: 0.0018 | Dom: 348.4607 | DomAcc: 0.4837\n",
      "[INFO] patience 2 / 3\n",
      "[Epoch 27] Total Loss: 172.8117 | Cls: 0.0004 | Dom: 345.4376 | DomAcc: 0.5307\n",
      "[Epoch 28] Total Loss: 173.6840 | Cls: 0.0004 | Dom: 347.1745 | DomAcc: 0.4976\n",
      "[INFO] patience 1 / 3\n",
      "[Epoch 29] Total Loss: 172.7610 | Cls: 0.0003 | Dom: 345.3671 | DomAcc: 0.5148\n",
      "[INFO] patience 2 / 3\n",
      "[Epoch 30] Total Loss: 173.7958 | Cls: 0.0003 | Dom: 347.4434 | DomAcc: 0.4818\n",
      "[INFO] patience 3 / 3\n",
      "[Epoch 31] Total Loss: 173.9773 | Cls: 0.0004 | Dom: 347.7694 | DomAcc: 0.4704\n",
      "[INFO] patience 4 / 3\n",
      "[INFO] Early stopping: domain aligned and classifier converged.\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.394681, test Acc: 0.4310\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:14:00.836280Z",
     "start_time": "2025-08-11T23:10:01.074239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_cls_loss, total_dom_loss = 0.0, 0.0, 0.0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "        num_batches = 0\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            num_batches += 1\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            total_dom_loss += loss_dom.item()\n",
    "\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        avg_cls_loss = total_cls_loss / num_batches\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {total_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.015 and avg_cls_loss < 0.05 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "        if best_model_state is not None:\n",
    "            # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=191)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/test/HC_T191_RP.txt'\n",
    "    target_test_path = '../datasets/target/train/HC_T191_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=0.5,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "af67191a76772b7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 107.4720 | Cls: 1.2015 | Dom: 61.1553 | DomAcc: 0.7852\n",
      "[Epoch 2] Total Loss: 42.0315 | Cls: 0.3317 | Dom: 41.6113 | DomAcc: 0.9031\n",
      "[Epoch 3] Total Loss: 35.3511 | Cls: 0.1812 | Dom: 47.5135 | DomAcc: 0.8437\n",
      "[Epoch 4] Total Loss: 35.0832 | Cls: 0.1315 | Dom: 53.3284 | DomAcc: 0.7684\n",
      "[Epoch 5] Total Loss: 35.8496 | Cls: 0.1410 | Dom: 53.6561 | DomAcc: 0.8269\n",
      "[Epoch 6] Total Loss: 32.5553 | Cls: 0.1231 | Dom: 49.3595 | DomAcc: 0.8491\n",
      "[Epoch 7] Total Loss: 33.1143 | Cls: 0.0969 | Dom: 53.8270 | DomAcc: 0.8368\n",
      "[Epoch 8] Total Loss: 38.0941 | Cls: 0.1343 | Dom: 58.9997 | DomAcc: 0.8024\n",
      "[Epoch 9] Total Loss: 38.6681 | Cls: 0.0865 | Dom: 66.2631 | DomAcc: 0.7522\n",
      "[Epoch 10] Total Loss: 35.6480 | Cls: 0.0938 | Dom: 59.2918 | DomAcc: 0.7979\n",
      "[Epoch 11] Total Loss: 38.1515 | Cls: 0.1236 | Dom: 60.4816 | DomAcc: 0.7817\n",
      "[Epoch 12] Total Loss: 37.1170 | Cls: 0.0730 | Dom: 64.8887 | DomAcc: 0.7557\n",
      "[Epoch 13] Total Loss: 37.6678 | Cls: 0.0934 | Dom: 63.3761 | DomAcc: 0.7822\n",
      "[Epoch 14] Total Loss: 39.5822 | Cls: 0.0846 | Dom: 68.3411 | DomAcc: 0.7360\n",
      "[Epoch 15] Total Loss: 39.3257 | Cls: 0.0504 | Dom: 72.2044 | DomAcc: 0.7217\n",
      "[Epoch 16] Total Loss: 39.1553 | Cls: 0.0577 | Dom: 70.9271 | DomAcc: 0.7266\n",
      "[Epoch 17] Total Loss: 40.5227 | Cls: 0.0626 | Dom: 73.0345 | DomAcc: 0.7144\n",
      "[Epoch 18] Total Loss: 37.0208 | Cls: 0.0354 | Dom: 69.5114 | DomAcc: 0.7448\n",
      "[Epoch 19] Total Loss: 39.7879 | Cls: 0.0833 | Dom: 68.9098 | DomAcc: 0.7542\n",
      "[Epoch 20] Total Loss: 38.1460 | Cls: 0.0465 | Dom: 70.3399 | DomAcc: 0.7527\n",
      "[Epoch 21] Total Loss: 35.7352 | Cls: 0.0475 | Dom: 65.3926 | DomAcc: 0.7704\n",
      "[Epoch 22] Total Loss: 35.5293 | Cls: 0.0372 | Dom: 66.2969 | DomAcc: 0.7778\n",
      "[Epoch 23] Total Loss: 38.3839 | Cls: 0.0345 | Dom: 72.3454 | DomAcc: 0.7330\n",
      "[Epoch 24] Total Loss: 35.8476 | Cls: 0.0480 | Dom: 65.5541 | DomAcc: 0.7837\n",
      "[Epoch 25] Total Loss: 37.7359 | Cls: 0.0224 | Dom: 72.6100 | DomAcc: 0.7163\n",
      "[Epoch 26] Total Loss: 37.5506 | Cls: 0.0227 | Dom: 72.1906 | DomAcc: 0.7232\n",
      "[Epoch 27] Total Loss: 36.4334 | Cls: 0.0248 | Dom: 69.6968 | DomAcc: 0.7360\n",
      "[Epoch 28] Total Loss: 37.1165 | Cls: 0.0173 | Dom: 72.0166 | DomAcc: 0.7419\n",
      "[Epoch 29] Total Loss: 40.6819 | Cls: 0.0239 | Dom: 78.3029 | DomAcc: 0.6691\n",
      "[Epoch 30] Total Loss: 39.9923 | Cls: 0.0132 | Dom: 78.2888 | DomAcc: 0.6731\n",
      "[Epoch 31] Total Loss: 38.8775 | Cls: 0.0142 | Dom: 75.9416 | DomAcc: 0.7011\n",
      "[Epoch 32] Total Loss: 38.9706 | Cls: 0.0106 | Dom: 76.5829 | DomAcc: 0.6883\n",
      "[Epoch 33] Total Loss: 40.6979 | Cls: 0.0147 | Dom: 79.5155 | DomAcc: 0.6760\n",
      "[Epoch 34] Total Loss: 42.7805 | Cls: 0.0276 | Dom: 82.0345 | DomAcc: 0.6500\n",
      "[Epoch 35] Total Loss: 40.9183 | Cls: 0.0130 | Dom: 80.1684 | DomAcc: 0.6686\n",
      "[Epoch 36] Total Loss: 39.1487 | Cls: 0.0320 | Dom: 74.2072 | DomAcc: 0.7021\n",
      "[Epoch 37] Total Loss: 40.5550 | Cls: 0.0196 | Dom: 78.6041 | DomAcc: 0.6760\n",
      "[Epoch 38] Total Loss: 41.5825 | Cls: 0.0410 | Dom: 77.9136 | DomAcc: 0.6691\n",
      "[Epoch 39] Total Loss: 42.5882 | Cls: 0.0307 | Dom: 81.2439 | DomAcc: 0.6441\n",
      "[Epoch 40] Total Loss: 43.2135 | Cls: 0.0366 | Dom: 81.7446 | DomAcc: 0.6259\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.703248, test Acc: 0.5260\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T23:43:26.066145Z",
     "start_time": "2025-08-12T23:21:27.999756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=20, lambda_=0.1,scheduler = None):\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, total_loss_sum = 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src = model(src_x)\n",
    "            _, dom_out_tgt = model(tgt_x)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long).to(device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0), dtype=torch.long).to(device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "            lambda_ = dann_lambda(epoch, num_epochs)\n",
    "\n",
    "            loss = loss_cls + lambda_ * loss_dom\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cls_loss_sum += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "        avg_cls_loss = cls_loss_sum / total_cls_samples\n",
    "        avg_dom_loss = dom_loss_sum / total_dom_samples\n",
    "        avg_total_loss = total_loss_sum / total_dom_samples\n",
    "\n",
    "        # 域分类准确率（整轮）\n",
    "        dom_acc = dom_correct / dom_total\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total Loss: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"DomAcc: {dom_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if gap < 0.03 and avg_cls_loss < 0.5 and epoch > 10:\n",
    "            patience +=1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            # torch.save(best_model_state, os.path.join(out_path, 'test_best_model.pth'))\n",
    "            model.load_state_dict(best_model_state)\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=194)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T194_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model=train_dann(model, source_loader, target_loader,\n",
    "               optimizer, criterion_cls, criterion_domain,\n",
    "               device, num_epochs=40, lambda_=1,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "cbfe2fb5e5f1a299",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total Loss: 1.3830 | Cls: 0.4813 | Dom: 1.1275 | DomAcc: 0.6980\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.379489, test Acc: 0.3473\n",
      "[Epoch 2] Total Loss: 1.1783 | Cls: 0.1138 | Dom: 1.3307 | DomAcc: 0.5889\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.943707, test Acc: 0.3565\n",
      "[Epoch 3] Total Loss: 1.0821 | Cls: 0.0630 | Dom: 1.2737 | DomAcc: 0.6454\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.992497, test Acc: 0.4413\n",
      "[Epoch 4] Total Loss: 1.1115 | Cls: 0.0695 | Dom: 1.3025 | DomAcc: 0.6242\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.874673, test Acc: 0.4382\n",
      "[Epoch 5] Total Loss: 1.0684 | Cls: 0.0725 | Dom: 1.2448 | DomAcc: 0.6642\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.328156, test Acc: 0.4331\n",
      "[Epoch 6] Total Loss: 1.0800 | Cls: 0.0498 | Dom: 1.2878 | DomAcc: 0.6309\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.347125, test Acc: 0.3115\n",
      "[Epoch 7] Total Loss: 1.1123 | Cls: 0.0389 | Dom: 1.3417 | DomAcc: 0.5885\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.166218, test Acc: 0.3647\n",
      "[Epoch 8] Total Loss: 1.0798 | Cls: 0.0345 | Dom: 1.3066 | DomAcc: 0.6282\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.026965, test Acc: 0.3126\n",
      "[Epoch 9] Total Loss: 1.1150 | Cls: 0.0391 | Dom: 1.3448 | DomAcc: 0.5955\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.233624, test Acc: 0.3453\n",
      "[Epoch 10] Total Loss: 1.1029 | Cls: 0.0427 | Dom: 1.3257 | DomAcc: 0.5872\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.128353, test Acc: 0.3606\n",
      "[Epoch 11] Total Loss: 0.6876 | Cls: 0.0261 | Dom: 1.3229 | DomAcc: 0.6101\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.788423, test Acc: 0.3197\n",
      "[Epoch 12] Total Loss: 0.6843 | Cls: 0.0276 | Dom: 1.3134 | DomAcc: 0.6219\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.815337, test Acc: 0.3851\n",
      "[Epoch 13] Total Loss: 0.6835 | Cls: 0.0172 | Dom: 1.3326 | DomAcc: 0.6069\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.795501, test Acc: 0.2993\n",
      "[Epoch 14] Total Loss: 0.6818 | Cls: 0.0170 | Dom: 1.3296 | DomAcc: 0.6000\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.662990, test Acc: 0.3044\n",
      "[Epoch 15] Total Loss: 0.6958 | Cls: 0.0131 | Dom: 1.3652 | DomAcc: 0.5641\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.780650, test Acc: 0.2584\n",
      "[Epoch 16] Total Loss: 0.6923 | Cls: 0.0131 | Dom: 1.3585 | DomAcc: 0.5791\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.521843, test Acc: 0.2584\n",
      "[Epoch 17] Total Loss: 0.6810 | Cls: 0.0117 | Dom: 1.3387 | DomAcc: 0.5982\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.235641, test Acc: 0.2135\n",
      "[Epoch 18] Total Loss: 0.6944 | Cls: 0.0168 | Dom: 1.3551 | DomAcc: 0.5719\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.623472, test Acc: 0.2176\n",
      "[Epoch 19] Total Loss: 0.6879 | Cls: 0.0058 | Dom: 1.3642 | DomAcc: 0.5577\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.317632, test Acc: 0.2421\n",
      "[Epoch 20] Total Loss: 0.6944 | Cls: 0.0105 | Dom: 1.3678 | DomAcc: 0.5563\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.752021, test Acc: 0.2421\n",
      "[Epoch 21] Total Loss: 0.4170 | Cls: 0.0065 | Dom: 1.3686 | DomAcc: 0.5593\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.482497, test Acc: 0.2339\n",
      "[Epoch 22] Total Loss: 0.4196 | Cls: 0.0102 | Dom: 1.3647 | DomAcc: 0.5709\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.816940, test Acc: 0.2319\n",
      "[Epoch 23] Total Loss: 0.4128 | Cls: 0.0038 | Dom: 1.3633 | DomAcc: 0.5751\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 8.078077, test Acc: 0.2308\n",
      "[Epoch 24] Total Loss: 0.4157 | Cls: 0.0026 | Dom: 1.3768 | DomAcc: 0.5436\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 8.691498, test Acc: 0.2247\n",
      "[Epoch 25] Total Loss: 0.4153 | Cls: 0.0040 | Dom: 1.3708 | DomAcc: 0.5563\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 8.670503, test Acc: 0.2278\n",
      "[Epoch 26] Total Loss: 0.4151 | Cls: 0.0011 | Dom: 1.3802 | DomAcc: 0.5332\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 8.814206, test Acc: 0.2288\n",
      "[Epoch 27] Total Loss: 0.4154 | Cls: 0.0005 | Dom: 1.3829 | DomAcc: 0.5307\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.199013, test Acc: 0.2227\n",
      "[Epoch 28] Total Loss: 0.4130 | Cls: 0.0006 | Dom: 1.3749 | DomAcc: 0.5500\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.168770, test Acc: 0.2237\n",
      "[Epoch 29] Total Loss: 0.4164 | Cls: 0.0010 | Dom: 1.3846 | DomAcc: 0.5086\n",
      "[INFO] patience 1 / 3\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 30] Total Loss: 0.4195 | Cls: 0.0004 | Dom: 1.3971 | DomAcc: 0.4806\n",
      "[INFO] patience 2 / 3\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 31] Total Loss: 0.4204 | Cls: 0.0004 | Dom: 1.4000 | DomAcc: 0.4631\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 32] Total Loss: 0.4213 | Cls: 0.0005 | Dom: 1.4025 | DomAcc: 0.4685\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 33] Total Loss: 0.4227 | Cls: 0.0005 | Dom: 1.4073 | DomAcc: 0.4554\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 34] Total Loss: 0.4241 | Cls: 0.0006 | Dom: 1.4115 | DomAcc: 0.4423\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 35] Total Loss: 0.4242 | Cls: 0.0020 | Dom: 1.4073 | DomAcc: 0.4529\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 36] Total Loss: 0.4226 | Cls: 0.0005 | Dom: 1.4071 | DomAcc: 0.4345\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 9.051707, test Acc: 0.2288\n",
      "[Epoch 37] Total Loss: 0.4305 | Cls: 0.0139 | Dom: 1.3889 | DomAcc: 0.5035\n",
      "[INFO] patience 1 / 3\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n",
      "[Epoch 38] Total Loss: 0.4152 | Cls: 0.0088 | Dom: 1.3546 | DomAcc: 0.5744\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n",
      "[Epoch 39] Total Loss: 0.4121 | Cls: 0.0053 | Dom: 1.3560 | DomAcc: 0.5804\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n",
      "[Epoch 40] Total Loss: 0.4155 | Cls: 0.0049 | Dom: 1.3689 | DomAcc: 0.5571\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.594167, test Acc: 0.2196\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
