{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:40:35.413563Z",
     "start_time": "2025-08-19T11:40:32.131337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "import math"
   ],
   "id": "ca932f18d5cc82da",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T21:49:51.777032Z",
     "start_time": "2025-08-19T21:49:51.761424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    \"\"\"\n",
    "            Construct the DataLoaders for the source domain and target domain.\n",
    "\n",
    "                Parameters:\n",
    "                    source_path (str): Path to the txt file of the source domain data.\n",
    "                                       Each line usually contains the sample file path and its label.\n",
    "                    target_path (str): Path to the txt file of the target domain data.\n",
    "                                       The target domain usually has no labels.\n",
    "                    batch_size (int): Number of samples in each batch.\n",
    "\n",
    "                Returns:\n",
    "                    tuple:\n",
    "                        - source_loader: DataLoader of the source domain,\n",
    "                          which returns batches in the format (x, y).\n",
    "                        - target_loader: DataLoader of the target domain,\n",
    "                          which returns x (without labels).\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def score_metric(curr, w_gap=0.5, w_mmd=0.1):\n",
    "    \"\"\"\n",
    "    curr: {\"gap\": float, \"mmd\": float}\n",
    "    - gap 越小越好 → + (0.5 - gap)\n",
    "    - cls/mmd 越小越好 → 减去它们\n",
    "    \"\"\"\n",
    "    return 1 - curr[\"gap\"] * w_gap - curr[\"mmd\"] * w_mmd\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 0.6\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / max(1, num_epochs - 1)\n",
    "    return (2. / (1. + np.exp(-5 * p)) - 1.) * 0.5\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=5e-2):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_mmd_max=1e-1,           # MMD 最大权重\n",
    "                        use_mk=True,                   # 多核 MMD\n",
    "                        scheduler=None,\n",
    "                        score_weights=(1.0, 0.1), # (w_gap, w_mmd)\n",
    "                        warmup_best_start=10            # 多少个 epoch 后才开始考虑“最佳”\n",
    "                        ):\n",
    "\n",
    "\n",
    "    best_score = -float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "\n",
    "    mmd_hist = deque(maxlen=3)  # 用最近4个epoch判断是否进入平台期\n",
    "    gap_hist = deque(maxlen=3)\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased_with_gamma(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            # lambda_dann_now = dann_lambda(epoch, num_epochs)\n",
    "            lambda_dann_now = 0.3\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x, lambda_=lambda_dann_now)\n",
    "            _, dom_out_tgt, feat_tgt = model(tgt_x, lambda_=lambda_dann_now)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            bs_src, bs_tgt = src_x.size(0), tgt_x.size(0)\n",
    "            loss_dom_src = criterion_domain(dom_out_src, dom_label_src)\n",
    "            loss_dom_tgt = criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 样本数加权的“单个域损失均值”\n",
    "            loss_dom = (loss_dom_src * bs_src + loss_dom_tgt * bs_tgt) / (bs_src + bs_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "\n",
    "            loss = loss_cls + loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total loss: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls loss: {avg_cls_loss:.4f} | Dom avg loss: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD avg loss: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        gap_hist.append(gap)\n",
    "\n",
    "        # 选优\n",
    "        curr = {\"gap\": gap, \"mmd\": avg_mmd_loss}\n",
    "        w_gap, w_mmd = score_weights\n",
    "        curr_score = score_metric(curr, w_gap=w_gap, w_mmd=w_mmd)\n",
    "        improved = False\n",
    "        if epoch >= warmup_best_start and (curr_score > best_score + 1e-6):\n",
    "            best_score = curr_score\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # 早停判据\n",
    "        MIN_EPOCH = 10\n",
    "        PATIENCE = 4\n",
    "        GAP = 0.05\n",
    "        EPS_REL = 0.05  # LMMD 相对变化 <5% 视为平台期\n",
    "\n",
    "        # 1) 对齐“软门槛”：最近窗口平均 gap 很小\n",
    "        gap_ok = (len(gap_hist) == gap_hist.maxlen) and (sum(gap_hist) / len(gap_hist) < GAP)\n",
    "\n",
    "        # 2) LMMD 相对平台：最近窗口的首尾相差占比很小\n",
    "        if len(mmd_hist) == mmd_hist.maxlen:\n",
    "            lmmd_first, lmmd_last = mmd_hist[0], mmd_hist[-1]\n",
    "            denom = max(1e-8, max(lmmd_first, lmmd_last))\n",
    "            lmmd_plateau_rel = (abs(lmmd_last - lmmd_first) / denom) < EPS_REL\n",
    "        else:\n",
    "            lmmd_plateau_rel = False\n",
    "\n",
    "        # 3) 主判据：score 没提升就累计耐心；同时要求“对齐达标 + LMMD 进入平台”\n",
    "        if epoch >= MIN_EPOCH and gap_ok and lmmd_plateau_rel:\n",
    "            patience = 0 if improved else (patience + 1)\n",
    "            print(f\"[INFO] patience {patience}/{PATIENCE} | gap_ok={gap_ok} | lmmd_plateau_rel={lmmd_plateau_rel}\")\n",
    "            if patience >= PATIENCE:\n",
    "                print(\"[INFO] Early stopping by score patience with stable alignment/MMD.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "        target_test_path = '../datasets/target/test/HC_T191_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n"
   ],
   "id": "859f7da26af031f4",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T12:05:55.995911Z",
     "start_time": "2025-08-19T11:45:52.708710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    set_seed(seed=85)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=dann_lambda).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device,\n",
    "                                num_epochs=num_epochs,\n",
    "                                lambda_mmd_max=5e-2,\n",
    "                                use_mk=True,\n",
    "                                scheduler=scheduler,\n",
    "                                score_weights=(1.0, 0.5, 0.1),  # 可按任务调整权重\n",
    "                                warmup_best_start=3)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)"
   ],
   "id": "e25de06e735199e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total loss: 0.5470 | Cls loss: 0.4569 | Dom avg loss: 0.0896 | MMD avg loss: 0.0984 | DomAcc: 0.9624 | λ_dann: 0.0000 | λ_mmd: 0.0003\n",
      "[Epoch 2] Total loss: 0.0642 | Cls loss: 0.0617 | Dom avg loss: 0.0024 | MMD avg loss: 0.0980 | DomAcc: 0.9999 | λ_dann: 0.0638 | λ_mmd: 0.0004\n",
      "[Epoch 3] Total loss: 0.0341 | Cls loss: 0.0336 | Dom avg loss: 0.0005 | MMD avg loss: 0.0808 | DomAcc: 1.0000 | λ_dann: 0.1255 | λ_mmd: 0.0006\n",
      "[Epoch 4] Total loss: 0.0310 | Cls loss: 0.0302 | Dom avg loss: 0.0006 | MMD avg loss: 0.0740 | DomAcc: 1.0000 | λ_dann: 0.1834 | λ_mmd: 0.0007\n",
      "[Epoch 5] Total loss: 0.2598 | Cls loss: 0.0492 | Dom avg loss: 0.2108 | MMD avg loss: 0.0688 | DomAcc: 0.9163 | λ_dann: 0.2361 | λ_mmd: 0.0009\n",
      "[Epoch 6] Total loss: 0.4870 | Cls loss: 0.0685 | Dom avg loss: 0.4183 | MMD avg loss: 0.0667 | DomAcc: 0.7995 | λ_dann: 0.2828 | λ_mmd: 0.0012\n",
      "[Epoch 7] Total loss: 0.6216 | Cls loss: 0.0544 | Dom avg loss: 0.5671 | MMD avg loss: 0.0734 | DomAcc: 0.7016 | λ_dann: 0.3232 | λ_mmd: 0.0015\n",
      "[Epoch 8] Total loss: 0.6029 | Cls loss: 0.0385 | Dom avg loss: 0.5645 | MMD avg loss: 0.0697 | DomAcc: 0.7032 | λ_dann: 0.3575 | λ_mmd: 0.0019\n",
      "[Epoch 9] Total loss: 0.7079 | Cls loss: 0.0384 | Dom avg loss: 0.6692 | MMD avg loss: 0.0736 | DomAcc: 0.5986 | λ_dann: 0.3861 | λ_mmd: 0.0025\n",
      "[Epoch 10] Total loss: 0.6750 | Cls loss: 0.0274 | Dom avg loss: 0.6474 | MMD avg loss: 0.0691 | DomAcc: 0.6118 | λ_dann: 0.4095 | λ_mmd: 0.0032\n",
      "[Epoch 11] Total loss: 0.7323 | Cls loss: 0.0246 | Dom avg loss: 0.7074 | MMD avg loss: 0.0704 | DomAcc: 0.5049 | λ_dann: 0.4285 | λ_mmd: 0.0040\n",
      "[Epoch 12] Total loss: 0.6820 | Cls loss: 0.0229 | Dom avg loss: 0.6587 | MMD avg loss: 0.0657 | DomAcc: 0.6142 | λ_dann: 0.4438 | λ_mmd: 0.0051\n",
      "[Epoch 13] Total loss: 0.7018 | Cls loss: 0.0213 | Dom avg loss: 0.6802 | MMD avg loss: 0.0658 | DomAcc: 0.5707 | λ_dann: 0.4559 | λ_mmd: 0.0064\n",
      "[Epoch 14] Total loss: 0.7087 | Cls loss: 0.0254 | Dom avg loss: 0.6828 | MMD avg loss: 0.0656 | DomAcc: 0.5471 | λ_dann: 0.4656 | λ_mmd: 0.0079\n",
      "[INFO] patience 1 / 3 | MMD_small=False plateau=True\n",
      "[Epoch 15] Total loss: 0.6843 | Cls loss: 0.0114 | Dom avg loss: 0.6724 | MMD avg loss: 0.0659 | DomAcc: 0.6006 | λ_dann: 0.4731 | λ_mmd: 0.0098\n",
      "[Epoch 16] Total loss: 0.6808 | Cls loss: 0.0102 | Dom avg loss: 0.6698 | MMD avg loss: 0.0606 | DomAcc: 0.5924 | λ_dann: 0.4791 | λ_mmd: 0.0120\n",
      "[Epoch 17] Total loss: 0.6994 | Cls loss: 0.0193 | Dom avg loss: 0.6791 | MMD avg loss: 0.0624 | DomAcc: 0.5717 | λ_dann: 0.4837 | λ_mmd: 0.0145\n",
      "[Epoch 18] Total loss: 0.6741 | Cls loss: 0.0078 | Dom avg loss: 0.6652 | MMD avg loss: 0.0654 | DomAcc: 0.6096 | λ_dann: 0.4874 | λ_mmd: 0.0173\n",
      "[Epoch 19] Total loss: 0.7090 | Cls loss: 0.0109 | Dom avg loss: 0.6968 | MMD avg loss: 0.0615 | DomAcc: 0.5138 | λ_dann: 0.4902 | λ_mmd: 0.0203\n",
      "[INFO] patience 1 / 3 | MMD_small=False plateau=True\n",
      "[Epoch 20] Total loss: 0.7286 | Cls loss: 0.0166 | Dom avg loss: 0.7105 | MMD avg loss: 0.0638 | DomAcc: 0.5070 | λ_dann: 0.4924 | λ_mmd: 0.0234\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[Epoch 21] Total loss: 0.6936 | Cls loss: 0.0118 | Dom avg loss: 0.6802 | MMD avg loss: 0.0589 | DomAcc: 0.5748 | λ_dann: 0.4941 | λ_mmd: 0.0266\n",
      "[Epoch 22] Total loss: 0.6884 | Cls loss: 0.0071 | Dom avg loss: 0.6796 | MMD avg loss: 0.0568 | DomAcc: 0.5763 | λ_dann: 0.4954 | λ_mmd: 0.0297\n",
      "[Epoch 23] Total loss: 0.6919 | Cls loss: 0.0126 | Dom avg loss: 0.6775 | MMD avg loss: 0.0537 | DomAcc: 0.6076 | λ_dann: 0.4965 | λ_mmd: 0.0327\n",
      "[Epoch 24] Total loss: 0.6801 | Cls loss: 0.0087 | Dom avg loss: 0.6695 | MMD avg loss: 0.0519 | DomAcc: 0.6093 | λ_dann: 0.4973 | λ_mmd: 0.0355\n",
      "[Epoch 25] Total loss: 0.7061 | Cls loss: 0.0076 | Dom avg loss: 0.6961 | MMD avg loss: 0.0628 | DomAcc: 0.5153 | λ_dann: 0.4979 | λ_mmd: 0.0380\n",
      "[INFO] patience 1 / 3 | MMD_small=False plateau=True\n",
      "[Epoch 26] Total loss: 0.7002 | Cls loss: 0.0088 | Dom avg loss: 0.6893 | MMD avg loss: 0.0510 | DomAcc: 0.5368 | λ_dann: 0.4984 | λ_mmd: 0.0402\n",
      "[INFO] patience 2 / 3 | MMD_small=False plateau=True\n",
      "[Epoch 27] Total loss: 0.6903 | Cls loss: 0.0092 | Dom avg loss: 0.6790 | MMD avg loss: 0.0484 | DomAcc: 0.5749 | λ_dann: 0.4987 | λ_mmd: 0.0421\n",
      "[Epoch 28] Total loss: 0.6782 | Cls loss: 0.0058 | Dom avg loss: 0.6711 | MMD avg loss: 0.0293 | DomAcc: 0.5957 | λ_dann: 0.4990 | λ_mmd: 0.0436\n",
      "[Epoch 29] Total loss: 0.6837 | Cls loss: 0.0029 | Dom avg loss: 0.6794 | MMD avg loss: 0.0299 | DomAcc: 0.5606 | λ_dann: 0.4992 | λ_mmd: 0.0449\n",
      "[Epoch 30] Total loss: 0.6832 | Cls loss: 0.0014 | Dom avg loss: 0.6809 | MMD avg loss: 0.0208 | DomAcc: 0.5773 | λ_dann: 0.4994 | λ_mmd: 0.0460\n",
      "[Epoch 31] Total loss: 0.6920 | Cls loss: 0.0014 | Dom avg loss: 0.6897 | MMD avg loss: 0.0188 | DomAcc: 0.5444 | λ_dann: 0.4995 | λ_mmd: 0.0468\n",
      "[INFO] patience 1 / 3 | MMD_small=True plateau=True\n",
      "[Epoch 32] Total loss: 0.6895 | Cls loss: 0.0007 | Dom avg loss: 0.6882 | MMD avg loss: 0.0144 | DomAcc: 0.5574 | λ_dann: 0.4996 | λ_mmd: 0.0475\n",
      "[Epoch 33] Total loss: 0.6947 | Cls loss: 0.0007 | Dom avg loss: 0.6935 | MMD avg loss: 0.0096 | DomAcc: 0.5082 | λ_dann: 0.4997 | λ_mmd: 0.0481\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=True\n",
      "[Epoch 34] Total loss: 0.6922 | Cls loss: 0.0002 | Dom avg loss: 0.6913 | MMD avg loss: 0.0126 | DomAcc: 0.5222 | λ_dann: 0.4998 | λ_mmd: 0.0485\n",
      "[INFO] patience 1 / 3 | MMD_small=True plateau=True\n",
      "[Epoch 35] Total loss: 0.6935 | Cls loss: 0.0002 | Dom avg loss: 0.6927 | MMD avg loss: 0.0101 | DomAcc: 0.5200 | λ_dann: 0.4998 | λ_mmd: 0.0488\n",
      "[INFO] patience 2 / 3 | MMD_small=True plateau=True\n",
      "[Epoch 36] Total loss: 0.6932 | Cls loss: 0.0003 | Dom avg loss: 0.6923 | MMD avg loss: 0.0110 | DomAcc: 0.5151 | λ_dann: 0.4999 | λ_mmd: 0.0491\n",
      "[INFO] patience 3 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.310450, test Acc: 0.5796\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T12:35:16.637682Z",
     "start_time": "2025-08-19T12:10:22.061697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    set_seed(seed=88)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=dann_lambda).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device,\n",
    "                                num_epochs=num_epochs,\n",
    "                                lambda_mmd_max=5e-2,\n",
    "                                use_mk=True,\n",
    "                                scheduler=scheduler,\n",
    "                                score_weights=(1.0, 0.5, 0.1),  # 可按任务调整权重\n",
    "                                warmup_best_start=3)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)"
   ],
   "id": "b3fe8de877886fd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total loss: 0.5134 | Cls loss: 0.4477 | Dom avg loss: 0.0655 | MMD avg loss: 0.1125 | DomAcc: 0.9737 | λ_dann: 0.0000 | λ_mmd: 0.0003\n",
      "[Epoch 2] Total loss: 0.0558 | Cls loss: 0.0533 | Dom avg loss: 0.0024 | MMD avg loss: 0.0887 | DomAcc: 0.9999 | λ_dann: 0.0320 | λ_mmd: 0.0004\n",
      "[Epoch 3] Total loss: 0.0456 | Cls loss: 0.0432 | Dom avg loss: 0.0024 | MMD avg loss: 0.0775 | DomAcc: 0.9999 | λ_dann: 0.0638 | λ_mmd: 0.0006\n",
      "[Epoch 4] Total loss: 0.0212 | Cls loss: 0.0209 | Dom avg loss: 0.0003 | MMD avg loss: 0.0855 | DomAcc: 1.0000 | λ_dann: 0.0950 | λ_mmd: 0.0007\n",
      "[Epoch 5] Total loss: 0.0476 | Cls loss: 0.0360 | Dom avg loss: 0.0115 | MMD avg loss: 0.0825 | DomAcc: 0.9981 | λ_dann: 0.1255 | λ_mmd: 0.0009\n",
      "[Epoch 6] Total loss: 0.2835 | Cls loss: 0.0283 | Dom avg loss: 0.2551 | MMD avg loss: 0.0794 | DomAcc: 0.9048 | λ_dann: 0.1550 | λ_mmd: 0.0012\n",
      "[Epoch 7] Total loss: 0.3578 | Cls loss: 0.0326 | Dom avg loss: 0.3251 | MMD avg loss: 0.0797 | DomAcc: 0.8693 | λ_dann: 0.1834 | λ_mmd: 0.0015\n",
      "[Epoch 8] Total loss: 0.4165 | Cls loss: 0.0448 | Dom avg loss: 0.3715 | MMD avg loss: 0.0822 | DomAcc: 0.8301 | λ_dann: 0.2104 | λ_mmd: 0.0019\n",
      "[Epoch 9] Total loss: 0.4680 | Cls loss: 0.0446 | Dom avg loss: 0.4232 | MMD avg loss: 0.0723 | DomAcc: 0.7936 | λ_dann: 0.2361 | λ_mmd: 0.0025\n",
      "[Epoch 10] Total loss: 0.5944 | Cls loss: 0.0360 | Dom avg loss: 0.5581 | MMD avg loss: 0.0776 | DomAcc: 0.7075 | λ_dann: 0.2602 | λ_mmd: 0.0032\n",
      "[Epoch 11] Total loss: 0.5961 | Cls loss: 0.0416 | Dom avg loss: 0.5541 | MMD avg loss: 0.0880 | DomAcc: 0.7205 | λ_dann: 0.2828 | λ_mmd: 0.0040\n",
      "[Epoch 12] Total loss: 0.5827 | Cls loss: 0.0274 | Dom avg loss: 0.5549 | MMD avg loss: 0.0802 | DomAcc: 0.7149 | λ_dann: 0.3038 | λ_mmd: 0.0051\n",
      "[Epoch 13] Total loss: 0.6700 | Cls loss: 0.0254 | Dom avg loss: 0.6440 | MMD avg loss: 0.0835 | DomAcc: 0.6158 | λ_dann: 0.3232 | λ_mmd: 0.0064\n",
      "[Epoch 14] Total loss: 0.6682 | Cls loss: 0.0227 | Dom avg loss: 0.6448 | MMD avg loss: 0.0757 | DomAcc: 0.6366 | λ_dann: 0.3411 | λ_mmd: 0.0079\n",
      "[Epoch 15] Total loss: 0.6743 | Cls loss: 0.0293 | Dom avg loss: 0.6444 | MMD avg loss: 0.0646 | DomAcc: 0.6302 | λ_dann: 0.3575 | λ_mmd: 0.0098\n",
      "[Epoch 16] Total loss: 0.6744 | Cls loss: 0.0154 | Dom avg loss: 0.6581 | MMD avg loss: 0.0693 | DomAcc: 0.6004 | λ_dann: 0.3725 | λ_mmd: 0.0120\n",
      "[Epoch 17] Total loss: 0.6689 | Cls loss: 0.0314 | Dom avg loss: 0.6366 | MMD avg loss: 0.0650 | DomAcc: 0.6377 | λ_dann: 0.3861 | λ_mmd: 0.0145\n",
      "[Epoch 18] Total loss: 0.6874 | Cls loss: 0.0225 | Dom avg loss: 0.6638 | MMD avg loss: 0.0581 | DomAcc: 0.5855 | λ_dann: 0.3984 | λ_mmd: 0.0173\n",
      "[Epoch 19] Total loss: 0.6919 | Cls loss: 0.0067 | Dom avg loss: 0.6841 | MMD avg loss: 0.0562 | DomAcc: 0.5576 | λ_dann: 0.4095 | λ_mmd: 0.0203\n",
      "[Epoch 20] Total loss: 0.6887 | Cls loss: 0.0052 | Dom avg loss: 0.6823 | MMD avg loss: 0.0483 | DomAcc: 0.5589 | λ_dann: 0.4195 | λ_mmd: 0.0234\n",
      "[Epoch 21] Total loss: 0.6890 | Cls loss: 0.0068 | Dom avg loss: 0.6810 | MMD avg loss: 0.0469 | DomAcc: 0.5575 | λ_dann: 0.4285 | λ_mmd: 0.0266\n",
      "[Epoch 22] Total loss: 0.6901 | Cls loss: 0.0044 | Dom avg loss: 0.6843 | MMD avg loss: 0.0449 | DomAcc: 0.5520 | λ_dann: 0.4366 | λ_mmd: 0.0297\n",
      "[Epoch 23] Total loss: 0.6996 | Cls loss: 0.0121 | Dom avg loss: 0.6861 | MMD avg loss: 0.0426 | DomAcc: 0.5540 | λ_dann: 0.4438 | λ_mmd: 0.0327\n",
      "[Epoch 24] Total loss: 0.7054 | Cls loss: 0.0081 | Dom avg loss: 0.6958 | MMD avg loss: 0.0417 | DomAcc: 0.5080 | λ_dann: 0.4502 | λ_mmd: 0.0355\n",
      "[Epoch 25] Total loss: 0.7110 | Cls loss: 0.0182 | Dom avg loss: 0.6911 | MMD avg loss: 0.0431 | DomAcc: 0.5274 | λ_dann: 0.4559 | λ_mmd: 0.0380\n",
      "[Epoch 26] Total loss: 0.6901 | Cls loss: 0.0038 | Dom avg loss: 0.6848 | MMD avg loss: 0.0360 | DomAcc: 0.5522 | λ_dann: 0.4610 | λ_mmd: 0.0402\n",
      "[Epoch 27] Total loss: 0.6924 | Cls loss: 0.0032 | Dom avg loss: 0.6879 | MMD avg loss: 0.0315 | DomAcc: 0.5358 | λ_dann: 0.4656 | λ_mmd: 0.0421\n",
      "[Epoch 28] Total loss: 0.6904 | Cls loss: 0.0051 | Dom avg loss: 0.6840 | MMD avg loss: 0.0291 | DomAcc: 0.5624 | λ_dann: 0.4696 | λ_mmd: 0.0436\n",
      "[Epoch 29] Total loss: 0.6994 | Cls loss: 0.0033 | Dom avg loss: 0.6951 | MMD avg loss: 0.0221 | DomAcc: 0.4832 | λ_dann: 0.4731 | λ_mmd: 0.0449\n",
      "[Epoch 30] Total loss: 0.6913 | Cls loss: 0.0020 | Dom avg loss: 0.6884 | MMD avg loss: 0.0201 | DomAcc: 0.5558 | λ_dann: 0.4763 | λ_mmd: 0.0460\n",
      "[Epoch 31] Total loss: 0.6944 | Cls loss: 0.0023 | Dom avg loss: 0.6913 | MMD avg loss: 0.0167 | DomAcc: 0.5291 | λ_dann: 0.4791 | λ_mmd: 0.0468\n",
      "[Epoch 32] Total loss: 0.6922 | Cls loss: 0.0015 | Dom avg loss: 0.6900 | MMD avg loss: 0.0134 | DomAcc: 0.5240 | λ_dann: 0.4816 | λ_mmd: 0.0475\n",
      "[Epoch 33] Total loss: 0.6962 | Cls loss: 0.0020 | Dom avg loss: 0.6935 | MMD avg loss: 0.0141 | DomAcc: 0.5118 | λ_dann: 0.4837 | λ_mmd: 0.0481\n",
      "[Epoch 34] Total loss: 0.6916 | Cls loss: 0.0004 | Dom avg loss: 0.6907 | MMD avg loss: 0.0098 | DomAcc: 0.5247 | λ_dann: 0.4857 | λ_mmd: 0.0485\n",
      "[Epoch 35] Total loss: 0.6984 | Cls loss: 0.0022 | Dom avg loss: 0.6958 | MMD avg loss: 0.0069 | DomAcc: 0.4840 | λ_dann: 0.4874 | λ_mmd: 0.0488\n",
      "[Epoch 36] Total loss: 0.6982 | Cls loss: 0.0004 | Dom avg loss: 0.6973 | MMD avg loss: 0.0095 | DomAcc: 0.4772 | λ_dann: 0.4889 | λ_mmd: 0.0491\n",
      "[Epoch 37] Total loss: 0.7020 | Cls loss: 0.0028 | Dom avg loss: 0.6986 | MMD avg loss: 0.0114 | DomAcc: 0.4713 | λ_dann: 0.4902 | λ_mmd: 0.0493\n",
      "[Epoch 38] Total loss: 0.6949 | Cls loss: 0.0003 | Dom avg loss: 0.6940 | MMD avg loss: 0.0123 | DomAcc: 0.5089 | λ_dann: 0.4914 | λ_mmd: 0.0494\n",
      "[Epoch 39] Total loss: 0.6988 | Cls loss: 0.0003 | Dom avg loss: 0.6981 | MMD avg loss: 0.0094 | DomAcc: 0.4368 | λ_dann: 0.4924 | λ_mmd: 0.0496\n",
      "[Epoch 40] Total loss: 0.6974 | Cls loss: 0.0013 | Dom avg loss: 0.6956 | MMD avg loss: 0.0094 | DomAcc: 0.4677 | λ_dann: 0.4933 | λ_mmd: 0.0497\n",
      "[INFO] patience 1/4 | gap_ok=True | lmmd_plateau_rel=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.867159, test Acc: 0.4320\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T22:45:43.519150Z",
     "start_time": "2025-08-19T22:41:04.173598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    # set_seed(seed=12)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T191_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T191_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    num_epochs=20\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device,\n",
    "                                num_epochs=num_epochs,\n",
    "                                lambda_mmd_max=5e-2,\n",
    "                                use_mk=True,\n",
    "                                scheduler=scheduler,\n",
    "                                score_weights=(0.8, 0.1),  # 可按任务调整权重\n",
    "                                warmup_best_start=3)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)"
   ],
   "id": "63621533e4bbe42e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total loss: 0.8738 | Cls loss: 0.4987 | Dom avg loss: 0.3751 | MMD avg loss: 0.0977 | DomAcc: 0.8453 | λ_dann: 0.3000 | λ_mmd: 0.0003\n",
      "- test Loss: 3.004224, test Acc: 0.3267\n",
      "[Epoch 2] Total loss: 0.5745 | Cls loss: 0.1101 | Dom avg loss: 0.4644 | MMD avg loss: 0.0666 | DomAcc: 0.7919 | λ_dann: 0.3000 | λ_mmd: 0.0006\n",
      "- test Loss: 3.020441, test Acc: 0.3644\n",
      "[Epoch 3] Total loss: 0.5223 | Cls loss: 0.0648 | Dom avg loss: 0.4574 | MMD avg loss: 0.0621 | DomAcc: 0.8013 | λ_dann: 0.3000 | λ_mmd: 0.0009\n",
      "- test Loss: 3.107357, test Acc: 0.3752\n",
      "[Epoch 4] Total loss: 0.5432 | Cls loss: 0.0572 | Dom avg loss: 0.4859 | MMD avg loss: 0.0649 | DomAcc: 0.7670 | λ_dann: 0.3000 | λ_mmd: 0.0016\n",
      "- test Loss: 4.314679, test Acc: 0.3317\n",
      "[Epoch 5] Total loss: 0.5699 | Cls loss: 0.0600 | Dom avg loss: 0.5097 | MMD avg loss: 0.0629 | DomAcc: 0.7513 | λ_dann: 0.3000 | λ_mmd: 0.0026\n",
      "- test Loss: 4.547645, test Acc: 0.2446\n",
      "[Epoch 6] Total loss: 0.6243 | Cls loss: 0.0446 | Dom avg loss: 0.5794 | MMD avg loss: 0.0618 | DomAcc: 0.6884 | λ_dann: 0.3000 | λ_mmd: 0.0043\n",
      "- test Loss: 4.069784, test Acc: 0.2792\n",
      "[Epoch 7] Total loss: 0.6308 | Cls loss: 0.0255 | Dom avg loss: 0.6047 | MMD avg loss: 0.0847 | DomAcc: 0.6645 | λ_dann: 0.3000 | λ_mmd: 0.0068\n",
      "- test Loss: 5.054811, test Acc: 0.2693\n",
      "[Epoch 8] Total loss: 0.6495 | Cls loss: 0.0376 | Dom avg loss: 0.6112 | MMD avg loss: 0.0633 | DomAcc: 0.6656 | λ_dann: 0.3000 | λ_mmd: 0.0106\n",
      "- test Loss: 4.867932, test Acc: 0.3040\n",
      "[Epoch 9] Total loss: 0.6567 | Cls loss: 0.0274 | Dom avg loss: 0.6282 | MMD avg loss: 0.0659 | DomAcc: 0.6460 | λ_dann: 0.3000 | λ_mmd: 0.0156\n",
      "- test Loss: 5.430273, test Acc: 0.2495\n",
      "[Epoch 10] Total loss: 0.6791 | Cls loss: 0.0204 | Dom avg loss: 0.6574 | MMD avg loss: 0.0594 | DomAcc: 0.6180 | λ_dann: 0.3000 | λ_mmd: 0.0217\n",
      "- test Loss: 5.634687, test Acc: 0.2901\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[51]\u001B[39m\u001B[32m, line 38\u001B[39m\n\u001B[32m     35\u001B[39m criterion_domain = nn.CrossEntropyLoss()\n\u001B[32m     37\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m[INFO] Starting standard DANN training (no pseudo labels)...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m model = train_dann_with_mmd(model, source_loader, target_loader,\n\u001B[32m     39\u001B[39m                             optimizer, criterion_cls, criterion_domain,\n\u001B[32m     40\u001B[39m                             device,\n\u001B[32m     41\u001B[39m                             num_epochs=num_epochs,\n\u001B[32m     42\u001B[39m                             lambda_mmd_max=\u001B[32m5e-2\u001B[39m,\n\u001B[32m     43\u001B[39m                             use_mk=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     44\u001B[39m                             scheduler=scheduler,\n\u001B[32m     45\u001B[39m                             score_weights=(\u001B[32m0.8\u001B[39m, \u001B[32m0.1\u001B[39m),  \u001B[38;5;66;03m# 可按任务调整权重\u001B[39;00m\n\u001B[32m     46\u001B[39m                             warmup_best_start=\u001B[32m3\u001B[39m)\n\u001B[32m     48\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m[INFO] Evaluating on target test set...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     49\u001B[39m test_dataset = PKLDataset(target_test_path)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[49]\u001B[39m\u001B[32m, line 90\u001B[39m, in \u001B[36mtrain_dann_with_mmd\u001B[39m\u001B[34m(model, source_loader, target_loader, optimizer, criterion_cls, criterion_domain, device, num_epochs, lambda_mmd_max, use_mk, scheduler, score_weights, warmup_best_start)\u001B[39m\n\u001B[32m     88\u001B[39m \u001B[38;5;66;03m# lambda_dann_now = dann_lambda(epoch, num_epochs)\u001B[39;00m\n\u001B[32m     89\u001B[39m lambda_dann_now = \u001B[32m0.3\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m cls_out_src, dom_out_src, feat_src = model(src_x, lambda_=lambda_dann_now)\n\u001B[32m     91\u001B[39m _, dom_out_tgt, feat_tgt = model(tgt_x, lambda_=lambda_dann_now)\n\u001B[32m     93\u001B[39m \u001B[38;5;66;03m# 1) 分类损失（仅源域）\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Masterarbeit\\models\\Flexible_DANN_MMD.py:53\u001B[39m, in \u001B[36mFlexible_DANN.forward\u001B[39m\u001B[34m(self, x, lambda_)\u001B[39m\n\u001B[32m     50\u001B[39m class_outputs = \u001B[38;5;28mself\u001B[39m.classifier(features)\n\u001B[32m     51\u001B[39m reversed_features = grad_reverse(features,lambda_)\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m domain_outputs = \u001B[38;5;28mself\u001B[39m.domain_classifier(reversed_features)\n\u001B[32m     54\u001B[39m reduced_features = \u001B[38;5;28mself\u001B[39m.feature_reducer(features)\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m class_outputs, domain_outputs, reduced_features\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Masterarbeit\\models\\Flexible_DANN_MMD.py:30\u001B[39m, in \u001B[36mDomainClassifier.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.net(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    249\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m         \u001B[38;5;28minput\u001B[39m = module(\u001B[38;5;28minput\u001B[39m)\n\u001B[32m    251\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\Masterarbeit\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.linear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m.weight, \u001B[38;5;28mself\u001B[39m.bias)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T18:29:55.803751Z",
     "start_time": "2025-08-19T18:16:45.649892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    set_seed(seed=194)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T194_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    num_epochs=30\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=dann_lambda).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device,\n",
    "                                num_epochs=num_epochs,\n",
    "                                lambda_mmd_max=1e-1,\n",
    "                                use_mk=True,\n",
    "                                scheduler=scheduler,\n",
    "                                score_weights=(0.8, 0.1),  # 可按任务调整权重\n",
    "                                warmup_best_start=3)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)"
   ],
   "id": "cc303c1db447115f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total loss: 0.5402 | Cls loss: 0.4294 | Dom avg loss: 0.1103 | MMD avg loss: 0.1836 | DomAcc: 0.9561 | λ_dann: 0.0000 | λ_mmd: 0.0007\n",
      "[Epoch 2] Total loss: 0.0575 | Cls loss: 0.0511 | Dom avg loss: 0.0064 | MMD avg loss: 0.0980 | DomAcc: 0.9990 | λ_dann: 0.0430 | λ_mmd: 0.0009\n",
      "[Epoch 3] Total loss: 0.0319 | Cls loss: 0.0272 | Dom avg loss: 0.0047 | MMD avg loss: 0.1078 | DomAcc: 0.9991 | λ_dann: 0.0854 | λ_mmd: 0.0013\n",
      "[Epoch 4] Total loss: 0.0764 | Cls loss: 0.0233 | Dom avg loss: 0.0529 | MMD avg loss: 0.1008 | DomAcc: 0.9826 | λ_dann: 0.1265 | λ_mmd: 0.0019\n",
      "[Epoch 5] Total loss: 0.2519 | Cls loss: 0.0468 | Dom avg loss: 0.2048 | MMD avg loss: 0.1061 | DomAcc: 0.9199 | λ_dann: 0.1659 | λ_mmd: 0.0026\n",
      "[Epoch 6] Total loss: 0.3253 | Cls loss: 0.0348 | Dom avg loss: 0.2902 | MMD avg loss: 0.1085 | DomAcc: 0.8846 | λ_dann: 0.2031 | λ_mmd: 0.0036\n",
      "[Epoch 7] Total loss: 0.4346 | Cls loss: 0.0471 | Dom avg loss: 0.3869 | MMD avg loss: 0.1033 | DomAcc: 0.8243 | λ_dann: 0.2378 | λ_mmd: 0.0051\n",
      "[Epoch 8] Total loss: 0.5508 | Cls loss: 0.0408 | Dom avg loss: 0.5093 | MMD avg loss: 0.0987 | DomAcc: 0.7528 | λ_dann: 0.2697 | λ_mmd: 0.0070\n",
      "[Epoch 9] Total loss: 0.5973 | Cls loss: 0.0502 | Dom avg loss: 0.5461 | MMD avg loss: 0.0907 | DomAcc: 0.7312 | λ_dann: 0.2989 | λ_mmd: 0.0096\n",
      "[Epoch 10] Total loss: 0.6415 | Cls loss: 0.0430 | Dom avg loss: 0.5972 | MMD avg loss: 0.0936 | DomAcc: 0.6834 | λ_dann: 0.3252 | λ_mmd: 0.0130\n",
      "[Epoch 11] Total loss: 0.6563 | Cls loss: 0.0311 | Dom avg loss: 0.6236 | MMD avg loss: 0.0925 | DomAcc: 0.6631 | λ_dann: 0.3487 | λ_mmd: 0.0175\n",
      "[Epoch 12] Total loss: 0.6728 | Cls loss: 0.0352 | Dom avg loss: 0.6354 | MMD avg loss: 0.0959 | DomAcc: 0.6252 | λ_dann: 0.3695 | λ_mmd: 0.0230\n",
      "[Epoch 13] Total loss: 0.6728 | Cls loss: 0.0227 | Dom avg loss: 0.6476 | MMD avg loss: 0.0878 | DomAcc: 0.6395 | λ_dann: 0.3878 | λ_mmd: 0.0297\n",
      "[Epoch 14] Total loss: 0.6849 | Cls loss: 0.0225 | Dom avg loss: 0.6588 | MMD avg loss: 0.0959 | DomAcc: 0.6208 | λ_dann: 0.4039 | λ_mmd: 0.0373\n",
      "[Epoch 15] Total loss: 0.6843 | Cls loss: 0.0151 | Dom avg loss: 0.6651 | MMD avg loss: 0.0913 | DomAcc: 0.6000 | λ_dann: 0.4179 | λ_mmd: 0.0457\n",
      "[Epoch 16] Total loss: 0.6872 | Cls loss: 0.0224 | Dom avg loss: 0.6587 | MMD avg loss: 0.1106 | DomAcc: 0.6170 | λ_dann: 0.4300 | λ_mmd: 0.0543\n",
      "[Epoch 17] Total loss: 0.6847 | Cls loss: 0.0111 | Dom avg loss: 0.6655 | MMD avg loss: 0.1290 | DomAcc: 0.5970 | λ_dann: 0.4404 | λ_mmd: 0.0627\n",
      "[Epoch 18] Total loss: 0.6937 | Cls loss: 0.0081 | Dom avg loss: 0.6768 | MMD avg loss: 0.1246 | DomAcc: 0.5791 | λ_dann: 0.4494 | λ_mmd: 0.0703\n",
      "[Epoch 19] Total loss: 0.6924 | Cls loss: 0.0096 | Dom avg loss: 0.6732 | MMD avg loss: 0.1250 | DomAcc: 0.5883 | λ_dann: 0.4570 | λ_mmd: 0.0770\n",
      "[Epoch 20] Total loss: 0.6919 | Cls loss: 0.0045 | Dom avg loss: 0.6771 | MMD avg loss: 0.1251 | DomAcc: 0.5796 | λ_dann: 0.4636 | λ_mmd: 0.0825\n",
      "[Epoch 21] Total loss: 0.6959 | Cls loss: 0.0044 | Dom avg loss: 0.6818 | MMD avg loss: 0.1113 | DomAcc: 0.5732 | λ_dann: 0.4692 | λ_mmd: 0.0870\n",
      "[Epoch 22] Total loss: 0.7000 | Cls loss: 0.0080 | Dom avg loss: 0.6828 | MMD avg loss: 0.1018 | DomAcc: 0.5629 | λ_dann: 0.4739 | λ_mmd: 0.0904\n",
      "[Epoch 23] Total loss: 0.6968 | Cls loss: 0.0069 | Dom avg loss: 0.6858 | MMD avg loss: 0.0452 | DomAcc: 0.5575 | λ_dann: 0.4780 | λ_mmd: 0.0930\n",
      "[Epoch 24] Total loss: 0.6967 | Cls loss: 0.0034 | Dom avg loss: 0.6897 | MMD avg loss: 0.0385 | DomAcc: 0.5440 | λ_dann: 0.4814 | λ_mmd: 0.0949\n",
      "[Epoch 25] Total loss: 0.6984 | Cls loss: 0.0065 | Dom avg loss: 0.6886 | MMD avg loss: 0.0330 | DomAcc: 0.5460 | λ_dann: 0.4843 | λ_mmd: 0.0964\n",
      "[Epoch 26] Total loss: 0.6959 | Cls loss: 0.0017 | Dom avg loss: 0.6902 | MMD avg loss: 0.0407 | DomAcc: 0.5301 | λ_dann: 0.4867 | λ_mmd: 0.0974\n",
      "[Epoch 27] Total loss: 0.7014 | Cls loss: 0.0044 | Dom avg loss: 0.6925 | MMD avg loss: 0.0460 | DomAcc: 0.5126 | λ_dann: 0.4888 | λ_mmd: 0.0981\n",
      "[Epoch 28] Total loss: 0.7003 | Cls loss: 0.0007 | Dom avg loss: 0.6949 | MMD avg loss: 0.0475 | DomAcc: 0.5026 | λ_dann: 0.4906 | λ_mmd: 0.0987\n",
      "[Epoch 29] Total loss: 0.7023 | Cls loss: 0.0034 | Dom avg loss: 0.6947 | MMD avg loss: 0.0417 | DomAcc: 0.5015 | λ_dann: 0.4921 | λ_mmd: 0.0991\n",
      "[INFO] patience 0/4 | gap_ok=True | lmmd_plateau_rel=True\n",
      "[Epoch 30] Total loss: 0.6968 | Cls loss: 0.0012 | Dom avg loss: 0.6916 | MMD avg loss: 0.0399 | DomAcc: 0.5236 | λ_dann: 0.4933 | λ_mmd: 0.0993\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 7.074754, test Acc: 0.3095\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:56:57.880514Z",
     "start_time": "2025-08-19T17:43:31.493266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    set_seed(seed=97)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T197_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T197_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    num_epochs=30\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=dann_lambda).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device,\n",
    "                                num_epochs=num_epochs,\n",
    "                                lambda_mmd_max=1e-1,\n",
    "                                use_mk=True,\n",
    "                                scheduler=scheduler,\n",
    "                                score_weights=(0.8, 0.1),  # 可按任务调整权重\n",
    "                                warmup_best_start=3)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)"
   ],
   "id": "5040f236447abfcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total loss: 0.8804 | Cls loss: 0.5821 | Dom avg loss: 0.2984 | MMD avg loss: 0.1554 | DomAcc: 0.8547 | λ_dann: 0.0000 | λ_mmd: 0.0007\n",
      "[Epoch 2] Total loss: 0.2447 | Cls loss: 0.0871 | Dom avg loss: 0.1575 | MMD avg loss: 0.1539 | DomAcc: 0.9426 | λ_dann: 0.0430 | λ_mmd: 0.0009\n",
      "[Epoch 3] Total loss: 0.8268 | Cls loss: 0.0606 | Dom avg loss: 0.7661 | MMD avg loss: 0.0764 | DomAcc: 0.5921 | λ_dann: 0.0854 | λ_mmd: 0.0013\n",
      "[Epoch 4] Total loss: 0.6964 | Cls loss: 0.0658 | Dom avg loss: 0.6304 | MMD avg loss: 0.0990 | DomAcc: 0.6257 | λ_dann: 0.1265 | λ_mmd: 0.0019\n",
      "[Epoch 5] Total loss: 0.6215 | Cls loss: 0.0373 | Dom avg loss: 0.5840 | MMD avg loss: 0.0982 | DomAcc: 0.6419 | λ_dann: 0.1659 | λ_mmd: 0.0026\n",
      "[Epoch 6] Total loss: 0.5366 | Cls loss: 0.0450 | Dom avg loss: 0.4912 | MMD avg loss: 0.0980 | DomAcc: 0.7698 | λ_dann: 0.2031 | λ_mmd: 0.0036\n",
      "[Epoch 7] Total loss: 0.5547 | Cls loss: 0.0438 | Dom avg loss: 0.5105 | MMD avg loss: 0.0973 | DomAcc: 0.7558 | λ_dann: 0.2378 | λ_mmd: 0.0051\n",
      "[Epoch 8] Total loss: 0.5660 | Cls loss: 0.0375 | Dom avg loss: 0.5278 | MMD avg loss: 0.1048 | DomAcc: 0.7281 | λ_dann: 0.2697 | λ_mmd: 0.0070\n",
      "[Epoch 9] Total loss: 0.6117 | Cls loss: 0.0448 | Dom avg loss: 0.5659 | MMD avg loss: 0.1064 | DomAcc: 0.6926 | λ_dann: 0.2989 | λ_mmd: 0.0096\n",
      "[Epoch 10] Total loss: 0.6022 | Cls loss: 0.0433 | Dom avg loss: 0.5573 | MMD avg loss: 0.1214 | DomAcc: 0.7035 | λ_dann: 0.3252 | λ_mmd: 0.0130\n",
      "[Epoch 11] Total loss: 0.5994 | Cls loss: 0.0316 | Dom avg loss: 0.5657 | MMD avg loss: 0.1214 | DomAcc: 0.6976 | λ_dann: 0.3487 | λ_mmd: 0.0175\n",
      "[Epoch 12] Total loss: 0.6428 | Cls loss: 0.0169 | Dom avg loss: 0.6232 | MMD avg loss: 0.1160 | DomAcc: 0.6386 | λ_dann: 0.3695 | λ_mmd: 0.0230\n",
      "[Epoch 13] Total loss: 0.6341 | Cls loss: 0.0391 | Dom avg loss: 0.5910 | MMD avg loss: 0.1333 | DomAcc: 0.6668 | λ_dann: 0.3878 | λ_mmd: 0.0297\n",
      "[Epoch 14] Total loss: 0.6416 | Cls loss: 0.0301 | Dom avg loss: 0.6072 | MMD avg loss: 0.1180 | DomAcc: 0.6461 | λ_dann: 0.4039 | λ_mmd: 0.0373\n",
      "[Epoch 15] Total loss: 0.6704 | Cls loss: 0.0299 | Dom avg loss: 0.6347 | MMD avg loss: 0.1254 | DomAcc: 0.6356 | λ_dann: 0.4179 | λ_mmd: 0.0457\n",
      "[Epoch 16] Total loss: 0.6746 | Cls loss: 0.0230 | Dom avg loss: 0.6442 | MMD avg loss: 0.1328 | DomAcc: 0.6169 | λ_dann: 0.4300 | λ_mmd: 0.0543\n",
      "[Epoch 17] Total loss: 0.6745 | Cls loss: 0.0114 | Dom avg loss: 0.6555 | MMD avg loss: 0.1207 | DomAcc: 0.6091 | λ_dann: 0.4404 | λ_mmd: 0.0627\n",
      "[Epoch 18] Total loss: 0.6895 | Cls loss: 0.0206 | Dom avg loss: 0.6599 | MMD avg loss: 0.1268 | DomAcc: 0.6016 | λ_dann: 0.4494 | λ_mmd: 0.0703\n",
      "[Epoch 19] Total loss: 0.6762 | Cls loss: 0.0192 | Dom avg loss: 0.6476 | MMD avg loss: 0.1213 | DomAcc: 0.6242 | λ_dann: 0.4570 | λ_mmd: 0.0770\n",
      "[Epoch 20] Total loss: 0.6755 | Cls loss: 0.0119 | Dom avg loss: 0.6533 | MMD avg loss: 0.1246 | DomAcc: 0.6041 | λ_dann: 0.4636 | λ_mmd: 0.0825\n",
      "[Epoch 21] Total loss: 0.6893 | Cls loss: 0.0116 | Dom avg loss: 0.6672 | MMD avg loss: 0.1207 | DomAcc: 0.5941 | λ_dann: 0.4692 | λ_mmd: 0.0870\n",
      "[Epoch 22] Total loss: 0.6996 | Cls loss: 0.0126 | Dom avg loss: 0.6761 | MMD avg loss: 0.1212 | DomAcc: 0.5685 | λ_dann: 0.4739 | λ_mmd: 0.0904\n",
      "[Epoch 23] Total loss: 0.6980 | Cls loss: 0.0156 | Dom avg loss: 0.6714 | MMD avg loss: 0.1187 | DomAcc: 0.5918 | λ_dann: 0.4780 | λ_mmd: 0.0930\n",
      "[Epoch 24] Total loss: 0.6874 | Cls loss: 0.0080 | Dom avg loss: 0.6693 | MMD avg loss: 0.1064 | DomAcc: 0.6100 | λ_dann: 0.4814 | λ_mmd: 0.0949\n",
      "[Epoch 25] Total loss: 0.6923 | Cls loss: 0.0046 | Dom avg loss: 0.6767 | MMD avg loss: 0.1132 | DomAcc: 0.5828 | λ_dann: 0.4843 | λ_mmd: 0.0964\n",
      "[Epoch 26] Total loss: 0.6904 | Cls loss: 0.0063 | Dom avg loss: 0.6735 | MMD avg loss: 0.1084 | DomAcc: 0.5920 | λ_dann: 0.4867 | λ_mmd: 0.0974\n",
      "[Epoch 27] Total loss: 0.6972 | Cls loss: 0.0029 | Dom avg loss: 0.6844 | MMD avg loss: 0.1012 | DomAcc: 0.5635 | λ_dann: 0.4888 | λ_mmd: 0.0981\n",
      "[Epoch 28] Total loss: 0.6975 | Cls loss: 0.0052 | Dom avg loss: 0.6841 | MMD avg loss: 0.0826 | DomAcc: 0.5650 | λ_dann: 0.4906 | λ_mmd: 0.0987\n",
      "[Epoch 29] Total loss: 0.6971 | Cls loss: 0.0018 | Dom avg loss: 0.6855 | MMD avg loss: 0.0983 | DomAcc: 0.5492 | λ_dann: 0.4921 | λ_mmd: 0.0991\n",
      "[Epoch 30] Total loss: 0.6934 | Cls loss: 0.0010 | Dom avg loss: 0.6857 | MMD avg loss: 0.0682 | DomAcc: 0.5488 | λ_dann: 0.4933 | λ_mmd: 0.0993\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.834792, test Acc: 0.5634\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
