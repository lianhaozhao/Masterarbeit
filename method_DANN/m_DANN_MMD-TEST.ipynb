{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T19:19:52.923117Z",
     "start_time": "2025-08-13T19:11:33.100116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on source test set...\")\n",
    "        source_test_path = '../datasets/source/test/DC_T197_RP.txt'\n",
    "        s_dataset = PKLDataset(source_test_path)\n",
    "        s_loader = DataLoader(s_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, s_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "3db16bb5ef97c52a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.2440 | Cls: 0.6058 | Dom: 1.2747 | MMD: 0.0368 | DomAcc: 0.6091 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.117786, test Acc: 0.9665\n",
      "[Epoch 2] Total: 0.7993 | Cls: 0.1340 | Dom: 1.3301 | MMD: 0.0678 | DomAcc: 0.5857 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.097509, test Acc: 0.9685\n",
      "[Epoch 3] Total: 0.7864 | Cls: 0.1028 | Dom: 1.3667 | MMD: 0.0560 | DomAcc: 0.5511 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.082509, test Acc: 0.9783\n",
      "[Epoch 4] Total: 0.7638 | Cls: 0.0781 | Dom: 1.3709 | MMD: 0.0516 | DomAcc: 0.5596 | λ_dann: 0.5000 | λ_mmd: 0.0032\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.043204, test Acc: 0.9823\n",
      "[Epoch 5] Total: 0.7463 | Cls: 0.0771 | Dom: 1.3376 | MMD: 0.0532 | DomAcc: 0.5982 | λ_dann: 0.5000 | λ_mmd: 0.0052\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.062141, test Acc: 0.9783\n",
      "[Epoch 6] Total: 0.7289 | Cls: 0.0692 | Dom: 1.3182 | MMD: 0.0543 | DomAcc: 0.5916 | λ_dann: 0.5000 | λ_mmd: 0.0086\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.067696, test Acc: 0.9744\n",
      "[Epoch 7] Total: 0.7508 | Cls: 0.0586 | Dom: 1.3826 | MMD: 0.0639 | DomAcc: 0.5250 | λ_dann: 0.5000 | λ_mmd: 0.0137\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.062322, test Acc: 0.9744\n",
      "[Epoch 8] Total: 0.7259 | Cls: 0.0563 | Dom: 1.3372 | MMD: 0.0463 | DomAcc: 0.5992 | λ_dann: 0.5000 | λ_mmd: 0.0212\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.135420, test Acc: 0.9606\n",
      "[Epoch 9] Total: 0.7193 | Cls: 0.0500 | Dom: 1.3355 | MMD: 0.0482 | DomAcc: 0.5951 | λ_dann: 0.5000 | λ_mmd: 0.0312\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.076382, test Acc: 0.9764\n",
      "[Epoch 10] Total: 0.7143 | Cls: 0.0542 | Dom: 1.3160 | MMD: 0.0489 | DomAcc: 0.6217 | λ_dann: 0.5000 | λ_mmd: 0.0435\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.062569, test Acc: 0.9803\n",
      "[Epoch 11] Total: 0.7175 | Cls: 0.0508 | Dom: 1.3286 | MMD: 0.0415 | DomAcc: 0.6008 | λ_dann: 0.5000 | λ_mmd: 0.0565\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.107303, test Acc: 0.9705\n",
      "[Epoch 12] Total: 0.7140 | Cls: 0.0415 | Dom: 1.3387 | MMD: 0.0449 | DomAcc: 0.5864 | λ_dann: 0.5000 | λ_mmd: 0.0688\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.052310, test Acc: 0.9803\n",
      "[Epoch 13] Total: 0.7265 | Cls: 0.0426 | Dom: 1.3602 | MMD: 0.0465 | DomAcc: 0.5601 | λ_dann: 0.5000 | λ_mmd: 0.0788\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.028503, test Acc: 0.9921\n",
      "[Epoch 14] Total: 0.7247 | Cls: 0.0316 | Dom: 1.3779 | MMD: 0.0487 | DomAcc: 0.5323 | λ_dann: 0.5000 | λ_mmd: 0.0863\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.050342, test Acc: 0.9882\n",
      "[Epoch 15] Total: 0.7127 | Cls: 0.0217 | Dom: 1.3738 | MMD: 0.0447 | DomAcc: 0.5461 | λ_dann: 0.5000 | λ_mmd: 0.0914\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.017066, test Acc: 0.9941\n",
      "[Epoch 16] Total: 0.7134 | Cls: 0.0178 | Dom: 1.3825 | MMD: 0.0451 | DomAcc: 0.5292 | λ_dann: 0.5000 | λ_mmd: 0.0948\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.059669, test Acc: 0.9803\n",
      "[Epoch 17] Total: 0.7120 | Cls: 0.0132 | Dom: 1.3892 | MMD: 0.0430 | DomAcc: 0.5047 | λ_dann: 0.5000 | λ_mmd: 0.0968\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.058307, test Acc: 0.9823\n",
      "[Epoch 18] Total: 0.7037 | Cls: 0.0101 | Dom: 1.3780 | MMD: 0.0464 | DomAcc: 0.5364 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.005262, test Acc: 0.9961\n",
      "[Epoch 19] Total: 0.7043 | Cls: 0.0094 | Dom: 1.3807 | MMD: 0.0465 | DomAcc: 0.5399 | λ_dann: 0.5000 | λ_mmd: 0.0989\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.023235, test Acc: 0.9980\n",
      "[Epoch 20] Total: 0.7019 | Cls: 0.0116 | Dom: 1.3715 | MMD: 0.0456 | DomAcc: 0.5636 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.029232, test Acc: 0.9862\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.383763, test Acc: 0.7258\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T20:21:48.852612Z",
     "start_time": "2025-08-13T20:04:17.606729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on source test set...\")\n",
    "        source_test_path = '../datasets/source/test/DC_T197_RP.txt'\n",
    "        s_dataset = PKLDataset(source_test_path)\n",
    "        s_loader = DataLoader(s_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, s_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=18)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=30, lambda_dann=0.5,use_mk=True,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "131df9585fa71cd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.1895 | Cls: 0.5764 | Dom: 1.2251 | MMD: 0.0556 | DomAcc: 0.6446 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.149571, test Acc: 0.9547\n",
      "[Epoch 2] Total: 0.7854 | Cls: 0.1362 | Dom: 1.2980 | MMD: 0.0733 | DomAcc: 0.6359 | λ_dann: 0.5000 | λ_mmd: 0.0009\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.112532, test Acc: 0.9724\n",
      "[Epoch 3] Total: 0.7367 | Cls: 0.0965 | Dom: 1.2804 | MMD: 0.0701 | DomAcc: 0.6248 | λ_dann: 0.5000 | λ_mmd: 0.0013\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.118564, test Acc: 0.9547\n",
      "[Epoch 4] Total: 0.7170 | Cls: 0.0644 | Dom: 1.3049 | MMD: 0.0660 | DomAcc: 0.6256 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.071798, test Acc: 0.9764\n",
      "[Epoch 5] Total: 0.7360 | Cls: 0.0455 | Dom: 1.3806 | MMD: 0.0805 | DomAcc: 0.5358 | λ_dann: 0.5000 | λ_mmd: 0.0026\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.046606, test Acc: 0.9882\n",
      "[Epoch 6] Total: 0.7089 | Cls: 0.0394 | Dom: 1.3384 | MMD: 0.0655 | DomAcc: 0.6161 | λ_dann: 0.5000 | λ_mmd: 0.0036\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.063685, test Acc: 0.9783\n",
      "[Epoch 7] Total: 0.7368 | Cls: 0.0388 | Dom: 1.3952 | MMD: 0.0764 | DomAcc: 0.5346 | λ_dann: 0.5000 | λ_mmd: 0.0051\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.050784, test Acc: 0.9902\n",
      "[Epoch 8] Total: 0.7028 | Cls: 0.0213 | Dom: 1.3624 | MMD: 0.0720 | DomAcc: 0.5755 | λ_dann: 0.5000 | λ_mmd: 0.0070\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.039059, test Acc: 0.9843\n",
      "[Epoch 9] Total: 0.7473 | Cls: 0.0300 | Dom: 1.4329 | MMD: 0.0835 | DomAcc: 0.4592 | λ_dann: 0.5000 | λ_mmd: 0.0096\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.026639, test Acc: 0.9921\n",
      "[Epoch 10] Total: 0.7168 | Cls: 0.0266 | Dom: 1.3782 | MMD: 0.0753 | DomAcc: 0.5344 | λ_dann: 0.5000 | λ_mmd: 0.0130\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.027998, test Acc: 0.9921\n",
      "[Epoch 11] Total: 0.7104 | Cls: 0.0152 | Dom: 1.3884 | MMD: 0.0701 | DomAcc: 0.4890 | λ_dann: 0.5000 | λ_mmd: 0.0175\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.056695, test Acc: 0.9882\n",
      "[Epoch 12] Total: 0.7025 | Cls: 0.0131 | Dom: 1.3758 | MMD: 0.0653 | DomAcc: 0.5506 | λ_dann: 0.5000 | λ_mmd: 0.0230\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.023029, test Acc: 0.9941\n",
      "[Epoch 13] Total: 0.7075 | Cls: 0.0256 | Dom: 1.3599 | MMD: 0.0661 | DomAcc: 0.5770 | λ_dann: 0.5000 | λ_mmd: 0.0297\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.042402, test Acc: 0.9862\n",
      "[Epoch 14] Total: 0.7319 | Cls: 0.0233 | Dom: 1.4114 | MMD: 0.0777 | DomAcc: 0.4872 | λ_dann: 0.5000 | λ_mmd: 0.0373\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.022283, test Acc: 0.9941\n",
      "[Epoch 15] Total: 0.7044 | Cls: 0.0136 | Dom: 1.3762 | MMD: 0.0586 | DomAcc: 0.5334 | λ_dann: 0.5000 | λ_mmd: 0.0457\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.017207, test Acc: 0.9961\n",
      "[Epoch 16] Total: 0.6964 | Cls: 0.0064 | Dom: 1.3740 | MMD: 0.0559 | DomAcc: 0.5651 | λ_dann: 0.5000 | λ_mmd: 0.0543\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.021344, test Acc: 0.9941\n",
      "[Epoch 17] Total: 0.7062 | Cls: 0.0097 | Dom: 1.3856 | MMD: 0.0593 | DomAcc: 0.5029 | λ_dann: 0.5000 | λ_mmd: 0.0627\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.012560, test Acc: 0.9961\n",
      "[Epoch 18] Total: 0.6906 | Cls: 0.0093 | Dom: 1.3558 | MMD: 0.0480 | DomAcc: 0.6024 | λ_dann: 0.5000 | λ_mmd: 0.0703\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.016318, test Acc: 0.9961\n",
      "[Epoch 19] Total: 0.7293 | Cls: 0.0163 | Dom: 1.4163 | MMD: 0.0629 | DomAcc: 0.5060 | λ_dann: 0.5000 | λ_mmd: 0.0770\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.009570, test Acc: 0.9980\n",
      "[Epoch 20] Total: 0.7235 | Cls: 0.0124 | Dom: 1.4091 | MMD: 0.0798 | DomAcc: 0.4737 | λ_dann: 0.5000 | λ_mmd: 0.0825\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.017609, test Acc: 0.9941\n",
      "[Epoch 21] Total: 0.7057 | Cls: 0.0061 | Dom: 1.3892 | MMD: 0.0575 | DomAcc: 0.4899 | λ_dann: 0.5000 | λ_mmd: 0.0870\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.015575, test Acc: 0.9941\n",
      "[Epoch 22] Total: 0.6975 | Cls: 0.0045 | Dom: 1.3781 | MMD: 0.0441 | DomAcc: 0.5460 | λ_dann: 0.5000 | λ_mmd: 0.0904\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.011399, test Acc: 0.9961\n",
      "[Epoch 23] Total: 0.6961 | Cls: 0.0024 | Dom: 1.3785 | MMD: 0.0480 | DomAcc: 0.5483 | λ_dann: 0.5000 | λ_mmd: 0.0930\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.014984, test Acc: 0.9961\n",
      "[Epoch 24] Total: 0.6980 | Cls: 0.0029 | Dom: 1.3823 | MMD: 0.0414 | DomAcc: 0.5240 | λ_dann: 0.5000 | λ_mmd: 0.0949\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.015882, test Acc: 0.9961\n",
      "[Epoch 25] Total: 0.6986 | Cls: 0.0032 | Dom: 1.3828 | MMD: 0.0407 | DomAcc: 0.5264 | λ_dann: 0.5000 | λ_mmd: 0.0964\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.015662, test Acc: 0.9961\n",
      "[Epoch 26] Total: 0.6957 | Cls: 0.0017 | Dom: 1.3795 | MMD: 0.0433 | DomAcc: 0.5507 | λ_dann: 0.5000 | λ_mmd: 0.0974\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.018983, test Acc: 0.9961\n",
      "[Epoch 27] Total: 0.6957 | Cls: 0.0008 | Dom: 1.3818 | MMD: 0.0415 | DomAcc: 0.5352 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.018626, test Acc: 0.9961\n",
      "[Epoch 28] Total: 0.6973 | Cls: 0.0014 | Dom: 1.3839 | MMD: 0.0398 | DomAcc: 0.5182 | λ_dann: 0.5000 | λ_mmd: 0.0987\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.012579, test Acc: 0.9961\n",
      "[Epoch 29] Total: 0.6970 | Cls: 0.0006 | Dom: 1.3843 | MMD: 0.0426 | DomAcc: 0.5194 | λ_dann: 0.5000 | λ_mmd: 0.0991\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.021671, test Acc: 0.9961\n",
      "[Epoch 30] Total: 0.6987 | Cls: 0.0006 | Dom: 1.3883 | MMD: 0.0408 | DomAcc: 0.4843 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] patience 1 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on source test set...\n",
      "- test Loss: 0.020804, test Acc: 0.9961\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.746494, test Acc: 0.5462\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T20:53:36.508787Z",
     "start_time": "2025-08-13T20:44:24.153950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def set_bn_eval(m):\n",
    "    if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "        m.eval()\n",
    "\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "\n",
    "        model.apply(set_bn_eval)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=1).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "b551f558daf1f1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.2440 | Cls: 0.6058 | Dom: 1.2747 | MMD: 0.0368 | DomAcc: 0.6091 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[Epoch 2] Total: 0.7820 | Cls: 0.1334 | Dom: 1.2975 | MMD: 0.0681 | DomAcc: 0.6119 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[Epoch 3] Total: 0.7743 | Cls: 0.1089 | Dom: 1.3309 | MMD: 0.0512 | DomAcc: 0.5886 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[Epoch 4] Total: 0.7548 | Cls: 0.0903 | Dom: 1.3287 | MMD: 0.0429 | DomAcc: 0.5896 | λ_dann: 0.5000 | λ_mmd: 0.0032\n",
      "[Epoch 5] Total: 0.7447 | Cls: 0.0931 | Dom: 1.3025 | MMD: 0.0456 | DomAcc: 0.6101 | λ_dann: 0.5000 | λ_mmd: 0.0052\n",
      "[Epoch 6] Total: 0.6982 | Cls: 0.0678 | Dom: 1.2601 | MMD: 0.0449 | DomAcc: 0.6440 | λ_dann: 0.5000 | λ_mmd: 0.0086\n",
      "[Epoch 7] Total: 0.7289 | Cls: 0.0625 | Dom: 1.3315 | MMD: 0.0455 | DomAcc: 0.5826 | λ_dann: 0.5000 | λ_mmd: 0.0137\n",
      "[Epoch 8] Total: 0.7222 | Cls: 0.0657 | Dom: 1.3108 | MMD: 0.0485 | DomAcc: 0.6229 | λ_dann: 0.5000 | λ_mmd: 0.0212\n",
      "[Epoch 9] Total: 0.7327 | Cls: 0.0685 | Dom: 1.3252 | MMD: 0.0471 | DomAcc: 0.5946 | λ_dann: 0.5000 | λ_mmd: 0.0312\n",
      "[Epoch 10] Total: 0.7185 | Cls: 0.0537 | Dom: 1.3257 | MMD: 0.0446 | DomAcc: 0.5972 | λ_dann: 0.5000 | λ_mmd: 0.0435\n",
      "[Epoch 11] Total: 0.7321 | Cls: 0.0589 | Dom: 1.3414 | MMD: 0.0437 | DomAcc: 0.5907 | λ_dann: 0.5000 | λ_mmd: 0.0565\n",
      "[Epoch 12] Total: 0.7132 | Cls: 0.0425 | Dom: 1.3357 | MMD: 0.0401 | DomAcc: 0.5994 | λ_dann: 0.5000 | λ_mmd: 0.0688\n",
      "[Epoch 13] Total: 0.7157 | Cls: 0.0352 | Dom: 1.3537 | MMD: 0.0463 | DomAcc: 0.5674 | λ_dann: 0.5000 | λ_mmd: 0.0788\n",
      "[Epoch 14] Total: 0.7123 | Cls: 0.0337 | Dom: 1.3493 | MMD: 0.0452 | DomAcc: 0.5840 | λ_dann: 0.5000 | λ_mmd: 0.0863\n",
      "[Epoch 15] Total: 0.7100 | Cls: 0.0253 | Dom: 1.3607 | MMD: 0.0473 | DomAcc: 0.5629 | λ_dann: 0.5000 | λ_mmd: 0.0914\n",
      "[Epoch 16] Total: 0.7237 | Cls: 0.0264 | Dom: 1.3856 | MMD: 0.0475 | DomAcc: 0.5172 | λ_dann: 0.5000 | λ_mmd: 0.0948\n",
      "[Epoch 17] Total: 0.7100 | Cls: 0.0174 | Dom: 1.3763 | MMD: 0.0452 | DomAcc: 0.5408 | λ_dann: 0.5000 | λ_mmd: 0.0968\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[Epoch 18] Total: 0.7070 | Cls: 0.0142 | Dom: 1.3771 | MMD: 0.0429 | DomAcc: 0.5407 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[Epoch 19] Total: 0.7060 | Cls: 0.0132 | Dom: 1.3772 | MMD: 0.0425 | DomAcc: 0.5390 | λ_dann: 0.5000 | λ_mmd: 0.0989\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[Epoch 20] Total: 0.7050 | Cls: 0.0100 | Dom: 1.3812 | MMD: 0.0436 | DomAcc: 0.5388 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.193144, test Acc: 0.5403\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T21:43:42.651866Z",
     "start_time": "2025-08-13T21:34:09.463262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=True,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 4e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-2  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        if epoch == (num_epochs-1):\n",
    "            if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, use_mk=True,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "c09f29cc06441afb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.0782 | Cls: 0.5802 | Dom: 0.9961 | MMD: 0.1656 | DomAcc: 0.7460 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[Epoch 2] Total: 0.6515 | Cls: 0.1473 | Dom: 1.0077 | MMD: 0.1466 | DomAcc: 0.7404 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[Epoch 3] Total: 0.6933 | Cls: 0.1279 | Dom: 1.1303 | MMD: 0.1383 | DomAcc: 0.6893 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[Epoch 4] Total: 0.6753 | Cls: 0.0984 | Dom: 1.1529 | MMD: 0.1339 | DomAcc: 0.6866 | λ_dann: 0.5000 | λ_mmd: 0.0032\n",
      "[Epoch 5] Total: 0.6815 | Cls: 0.0905 | Dom: 1.1805 | MMD: 0.1312 | DomAcc: 0.6775 | λ_dann: 0.5000 | λ_mmd: 0.0052\n",
      "[Epoch 6] Total: 0.6827 | Cls: 0.0724 | Dom: 1.2187 | MMD: 0.1180 | DomAcc: 0.6472 | λ_dann: 0.5000 | λ_mmd: 0.0086\n",
      "[Epoch 7] Total: 0.6870 | Cls: 0.0542 | Dom: 1.2622 | MMD: 0.1237 | DomAcc: 0.6511 | λ_dann: 0.5000 | λ_mmd: 0.0137\n",
      "[Epoch 8] Total: 0.7050 | Cls: 0.0549 | Dom: 1.2948 | MMD: 0.1267 | DomAcc: 0.6182 | λ_dann: 0.5000 | λ_mmd: 0.0212\n",
      "[Epoch 9] Total: 0.7036 | Cls: 0.0491 | Dom: 1.3008 | MMD: 0.1332 | DomAcc: 0.6230 | λ_dann: 0.5000 | λ_mmd: 0.0312\n",
      "[Epoch 10] Total: 0.6837 | Cls: 0.0339 | Dom: 1.2899 | MMD: 0.1124 | DomAcc: 0.6329 | λ_dann: 0.5000 | λ_mmd: 0.0435\n",
      "[Epoch 11] Total: 0.6933 | Cls: 0.0383 | Dom: 1.2977 | MMD: 0.1079 | DomAcc: 0.6217 | λ_dann: 0.5000 | λ_mmd: 0.0565\n",
      "[Epoch 12] Total: 0.6935 | Cls: 0.0256 | Dom: 1.3211 | MMD: 0.1071 | DomAcc: 0.6031 | λ_dann: 0.5000 | λ_mmd: 0.0688\n",
      "[Epoch 13] Total: 0.6857 | Cls: 0.0253 | Dom: 1.3055 | MMD: 0.0962 | DomAcc: 0.6230 | λ_dann: 0.5000 | λ_mmd: 0.0788\n",
      "[Epoch 14] Total: 0.6882 | Cls: 0.0160 | Dom: 1.3271 | MMD: 0.0998 | DomAcc: 0.5960 | λ_dann: 0.5000 | λ_mmd: 0.0863\n",
      "[Epoch 15] Total: 0.6925 | Cls: 0.0237 | Dom: 1.3209 | MMD: 0.0907 | DomAcc: 0.6050 | λ_dann: 0.5000 | λ_mmd: 0.0914\n",
      "[Epoch 16] Total: 0.6954 | Cls: 0.0182 | Dom: 1.3358 | MMD: 0.0983 | DomAcc: 0.5960 | λ_dann: 0.5000 | λ_mmd: 0.0948\n",
      "[Epoch 17] Total: 0.6979 | Cls: 0.0203 | Dom: 1.3378 | MMD: 0.0892 | DomAcc: 0.5986 | λ_dann: 0.5000 | λ_mmd: 0.0968\n",
      "[Epoch 18] Total: 0.6984 | Cls: 0.0120 | Dom: 1.3568 | MMD: 0.0822 | DomAcc: 0.5723 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[Epoch 19] Total: 0.6943 | Cls: 0.0085 | Dom: 1.3573 | MMD: 0.0722 | DomAcc: 0.5775 | λ_dann: 0.5000 | λ_mmd: 0.0989\n",
      "[Epoch 20] Total: 0.7052 | Cls: 0.0077 | Dom: 1.3823 | MMD: 0.0641 | DomAcc: 0.5329 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 0.770198, test Acc: 0.8692\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T23:20:04.305132Z",
     "start_time": "2025-08-13T23:03:39.188317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=15e-2,       # MMD 的最大权重\n",
    "                        use_mk=True,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 4e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-2  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small and cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        if epoch == (num_epochs-1):\n",
    "            if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T188_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T188_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=30, lambda_dann=0.5, use_mk=True,scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "cdff8ec7a7de5e06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 0.9870 | Cls: 0.5132 | Dom: 0.9463 | MMD: 0.1543 | DomAcc: 0.7726 | λ_dann: 0.5000 | λ_mmd: 0.0010\n",
      "[Epoch 2] Total: 0.6831 | Cls: 0.1454 | Dom: 1.0747 | MMD: 0.1511 | DomAcc: 0.7375 | λ_dann: 0.5000 | λ_mmd: 0.0014\n",
      "[Epoch 3] Total: 0.7013 | Cls: 0.0918 | Dom: 1.2183 | MMD: 0.1638 | DomAcc: 0.6536 | λ_dann: 0.5000 | λ_mmd: 0.0020\n",
      "[Epoch 4] Total: 0.6790 | Cls: 0.0974 | Dom: 1.1627 | MMD: 0.1552 | DomAcc: 0.6857 | λ_dann: 0.5000 | λ_mmd: 0.0028\n",
      "[Epoch 5] Total: 0.7018 | Cls: 0.0732 | Dom: 1.2558 | MMD: 0.1663 | DomAcc: 0.6365 | λ_dann: 0.5000 | λ_mmd: 0.0039\n",
      "[Epoch 6] Total: 0.7201 | Cls: 0.0595 | Dom: 1.3192 | MMD: 0.1746 | DomAcc: 0.6017 | λ_dann: 0.5000 | λ_mmd: 0.0055\n",
      "[Epoch 7] Total: 0.7186 | Cls: 0.0466 | Dom: 1.3412 | MMD: 0.1763 | DomAcc: 0.5612 | λ_dann: 0.5000 | λ_mmd: 0.0076\n",
      "[Epoch 8] Total: 0.7218 | Cls: 0.0579 | Dom: 1.3240 | MMD: 0.1765 | DomAcc: 0.5884 | λ_dann: 0.5000 | λ_mmd: 0.0105\n",
      "[Epoch 9] Total: 0.6957 | Cls: 0.0349 | Dom: 1.3165 | MMD: 0.1748 | DomAcc: 0.6039 | λ_dann: 0.5000 | λ_mmd: 0.0144\n",
      "[Epoch 10] Total: 0.7118 | Cls: 0.0314 | Dom: 1.3531 | MMD: 0.1962 | DomAcc: 0.5671 | λ_dann: 0.5000 | λ_mmd: 0.0196\n",
      "[Epoch 11] Total: 0.7175 | Cls: 0.0336 | Dom: 1.3574 | MMD: 0.1971 | DomAcc: 0.5626 | λ_dann: 0.5000 | λ_mmd: 0.0262\n",
      "[Epoch 12] Total: 0.6916 | Cls: 0.0297 | Dom: 1.3125 | MMD: 0.1619 | DomAcc: 0.6145 | λ_dann: 0.5000 | λ_mmd: 0.0345\n",
      "[Epoch 13] Total: 0.6921 | Cls: 0.0183 | Dom: 1.3347 | MMD: 0.1430 | DomAcc: 0.5920 | λ_dann: 0.5000 | λ_mmd: 0.0445\n",
      "[Epoch 14] Total: 0.7016 | Cls: 0.0244 | Dom: 1.3375 | MMD: 0.1500 | DomAcc: 0.5994 | λ_dann: 0.5000 | λ_mmd: 0.0560\n",
      "[Epoch 15] Total: 0.7085 | Cls: 0.0188 | Dom: 1.3574 | MMD: 0.1601 | DomAcc: 0.5579 | λ_dann: 0.5000 | λ_mmd: 0.0686\n",
      "[Epoch 16] Total: 0.6907 | Cls: 0.0141 | Dom: 1.3333 | MMD: 0.1222 | DomAcc: 0.6089 | λ_dann: 0.5000 | λ_mmd: 0.0814\n",
      "[Epoch 17] Total: 0.6893 | Cls: 0.0200 | Dom: 1.3195 | MMD: 0.1017 | DomAcc: 0.6092 | λ_dann: 0.5000 | λ_mmd: 0.0940\n",
      "[Epoch 18] Total: 0.7166 | Cls: 0.0178 | Dom: 1.3696 | MMD: 0.1328 | DomAcc: 0.5586 | λ_dann: 0.5000 | λ_mmd: 0.1055\n",
      "[Epoch 19] Total: 0.7017 | Cls: 0.0104 | Dom: 1.3581 | MMD: 0.1060 | DomAcc: 0.5566 | λ_dann: 0.5000 | λ_mmd: 0.1155\n",
      "[Epoch 20] Total: 0.6872 | Cls: 0.0127 | Dom: 1.3250 | MMD: 0.0967 | DomAcc: 0.6106 | λ_dann: 0.5000 | λ_mmd: 0.1238\n",
      "[Epoch 21] Total: 0.6856 | Cls: 0.0071 | Dom: 1.3340 | MMD: 0.0884 | DomAcc: 0.6039 | λ_dann: 0.5000 | λ_mmd: 0.1304\n",
      "[Epoch 22] Total: 0.6840 | Cls: 0.0128 | Dom: 1.3183 | MMD: 0.0889 | DomAcc: 0.6166 | λ_dann: 0.5000 | λ_mmd: 0.1356\n",
      "[Epoch 23] Total: 0.6855 | Cls: 0.0081 | Dom: 1.3296 | MMD: 0.0903 | DomAcc: 0.6027 | λ_dann: 0.5000 | λ_mmd: 0.1395\n",
      "[Epoch 24] Total: 0.6887 | Cls: 0.0094 | Dom: 1.3362 | MMD: 0.0782 | DomAcc: 0.5872 | λ_dann: 0.5000 | λ_mmd: 0.1424\n",
      "[Epoch 25] Total: 0.6874 | Cls: 0.0053 | Dom: 1.3430 | MMD: 0.0735 | DomAcc: 0.5865 | λ_dann: 0.5000 | λ_mmd: 0.1445\n",
      "[Epoch 26] Total: 0.6908 | Cls: 0.0107 | Dom: 1.3406 | MMD: 0.0670 | DomAcc: 0.5765 | λ_dann: 0.5000 | λ_mmd: 0.1461\n",
      "[Epoch 27] Total: 0.6902 | Cls: 0.0051 | Dom: 1.3541 | MMD: 0.0548 | DomAcc: 0.5675 | λ_dann: 0.5000 | λ_mmd: 0.1472\n",
      "[Epoch 28] Total: 0.6994 | Cls: 0.0032 | Dom: 1.3777 | MMD: 0.0497 | DomAcc: 0.5369 | λ_dann: 0.5000 | λ_mmd: 0.1480\n",
      "[Epoch 29] Total: 0.7041 | Cls: 0.0061 | Dom: 1.3824 | MMD: 0.0462 | DomAcc: 0.5229 | λ_dann: 0.5000 | λ_mmd: 0.1486\n",
      "[Epoch 30] Total: 0.7001 | Cls: 0.0025 | Dom: 1.3806 | MMD: 0.0490 | DomAcc: 0.5247 | λ_dann: 0.5000 | λ_mmd: 0.1490\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.317238, test Acc: 0.3793\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
