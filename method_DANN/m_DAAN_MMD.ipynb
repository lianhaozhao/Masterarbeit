{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-13T11:07:31.246115Z",
     "start_time": "2025-08-13T10:49:36.454093Z"
    }
   },
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 4e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased_with_gamma(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=40, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.0701 | Cls: 0.5714 | Dom: 0.9962 | MMD: 0.0242 | DomAcc: 0.7502 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.448983, test Acc: 0.5796\n",
      "[Epoch 2] Total: 0.6403 | Cls: 0.1197 | Dom: 1.0410 | MMD: 0.0476 | DomAcc: 0.7304 | λ_dann: 0.5000 | λ_mmd: 0.0009\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.799134, test Acc: 0.4869\n",
      "[Epoch 3] Total: 0.6151 | Cls: 0.0998 | Dom: 1.0305 | MMD: 0.0452 | DomAcc: 0.7326 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.634858, test Acc: 0.4829\n",
      "[Epoch 4] Total: 0.6203 | Cls: 0.0796 | Dom: 1.0811 | MMD: 0.0519 | DomAcc: 0.7152 | λ_dann: 0.5000 | λ_mmd: 0.0014\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.649415, test Acc: 0.4546\n",
      "[Epoch 5] Total: 0.6736 | Cls: 0.0877 | Dom: 1.1715 | MMD: 0.0469 | DomAcc: 0.6883 | λ_dann: 0.5000 | λ_mmd: 0.0018\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.761514, test Acc: 0.4637\n",
      "[Epoch 6] Total: 0.6890 | Cls: 0.0710 | Dom: 1.2357 | MMD: 0.0430 | DomAcc: 0.6506 | λ_dann: 0.5000 | λ_mmd: 0.0024\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.779604, test Acc: 0.5262\n",
      "[Epoch 7] Total: 0.7031 | Cls: 0.0671 | Dom: 1.2717 | MMD: 0.0480 | DomAcc: 0.6454 | λ_dann: 0.5000 | λ_mmd: 0.0030\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.596122, test Acc: 0.4677\n",
      "[Epoch 8] Total: 0.6832 | Cls: 0.0633 | Dom: 1.2398 | MMD: 0.0402 | DomAcc: 0.6545 | λ_dann: 0.5000 | λ_mmd: 0.0039\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.000392, test Acc: 0.4970\n",
      "[Epoch 9] Total: 0.6895 | Cls: 0.0474 | Dom: 1.2835 | MMD: 0.0503 | DomAcc: 0.6167 | λ_dann: 0.5000 | λ_mmd: 0.0050\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.651336, test Acc: 0.5181\n",
      "[Epoch 10] Total: 0.6676 | Cls: 0.0351 | Dom: 1.2644 | MMD: 0.0423 | DomAcc: 0.6483 | λ_dann: 0.5000 | λ_mmd: 0.0063\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.237129, test Acc: 0.5030\n",
      "[Epoch 11] Total: 0.6877 | Cls: 0.0361 | Dom: 1.3023 | MMD: 0.0460 | DomAcc: 0.6195 | λ_dann: 0.5000 | λ_mmd: 0.0080\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.041260, test Acc: 0.4889\n",
      "[Epoch 12] Total: 0.6828 | Cls: 0.0357 | Dom: 1.2934 | MMD: 0.0373 | DomAcc: 0.6308 | λ_dann: 0.5000 | λ_mmd: 0.0102\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.753387, test Acc: 0.5363\n",
      "[Epoch 13] Total: 0.6991 | Cls: 0.0340 | Dom: 1.3289 | MMD: 0.0471 | DomAcc: 0.6046 | λ_dann: 0.5000 | λ_mmd: 0.0128\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.592513, test Acc: 0.5665\n",
      "[Epoch 14] Total: 0.6921 | Cls: 0.0232 | Dom: 1.3366 | MMD: 0.0371 | DomAcc: 0.5965 | λ_dann: 0.5000 | λ_mmd: 0.0159\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.407670, test Acc: 0.6159\n",
      "[Epoch 15] Total: 0.6857 | Cls: 0.0231 | Dom: 1.3235 | MMD: 0.0421 | DomAcc: 0.5996 | λ_dann: 0.5000 | λ_mmd: 0.0196\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.348152, test Acc: 0.6069\n",
      "[Epoch 16] Total: 0.6935 | Cls: 0.0184 | Dom: 1.3486 | MMD: 0.0342 | DomAcc: 0.5830 | λ_dann: 0.5000 | λ_mmd: 0.0240\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.626153, test Acc: 0.5917\n",
      "[Epoch 17] Total: 0.6960 | Cls: 0.0186 | Dom: 1.3525 | MMD: 0.0384 | DomAcc: 0.5797 | λ_dann: 0.5000 | λ_mmd: 0.0290\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.634472, test Acc: 0.6069\n",
      "[Epoch 18] Total: 0.6920 | Cls: 0.0102 | Dom: 1.3611 | MMD: 0.0364 | DomAcc: 0.5788 | λ_dann: 0.5000 | λ_mmd: 0.0345\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.653757, test Acc: 0.6099\n",
      "[Epoch 19] Total: 0.6884 | Cls: 0.0120 | Dom: 1.3500 | MMD: 0.0340 | DomAcc: 0.5830 | λ_dann: 0.5000 | λ_mmd: 0.0405\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.581524, test Acc: 0.6190\n",
      "[Epoch 20] Total: 0.6841 | Cls: 0.0095 | Dom: 1.3466 | MMD: 0.0296 | DomAcc: 0.5863 | λ_dann: 0.5000 | λ_mmd: 0.0468\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.417734, test Acc: 0.6210\n",
      "[Epoch 21] Total: 0.6908 | Cls: 0.0058 | Dom: 1.3669 | MMD: 0.0298 | DomAcc: 0.5608 | λ_dann: 0.5000 | λ_mmd: 0.0532\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.991776, test Acc: 0.6149\n",
      "[Epoch 22] Total: 0.6947 | Cls: 0.0062 | Dom: 1.3731 | MMD: 0.0331 | DomAcc: 0.5584 | λ_dann: 0.5000 | λ_mmd: 0.0595\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.646142, test Acc: 0.6260\n",
      "[Epoch 23] Total: 0.6931 | Cls: 0.0045 | Dom: 1.3731 | MMD: 0.0311 | DomAcc: 0.5482 | λ_dann: 0.5000 | λ_mmd: 0.0655\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.935679, test Acc: 0.6169\n",
      "[Epoch 24] Total: 0.6965 | Cls: 0.0027 | Dom: 1.3831 | MMD: 0.0328 | DomAcc: 0.5293 | λ_dann: 0.5000 | λ_mmd: 0.0710\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.075439, test Acc: 0.6109\n",
      "[Epoch 25] Total: 0.6991 | Cls: 0.0021 | Dom: 1.3894 | MMD: 0.0300 | DomAcc: 0.5155 | λ_dann: 0.5000 | λ_mmd: 0.0760\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.033109, test Acc: 0.6139\n",
      "[Epoch 26] Total: 0.7015 | Cls: 0.0016 | Dom: 1.3942 | MMD: 0.0345 | DomAcc: 0.4944 | λ_dann: 0.5000 | λ_mmd: 0.0804\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.010173, test Acc: 0.6290\n",
      "[Epoch 27] Total: 0.6991 | Cls: 0.0020 | Dom: 1.3892 | MMD: 0.0331 | DomAcc: 0.4999 | λ_dann: 0.5000 | λ_mmd: 0.0841\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.081550, test Acc: 0.6139\n",
      "[Epoch 28] Total: 0.6998 | Cls: 0.0028 | Dom: 1.3880 | MMD: 0.0343 | DomAcc: 0.4949 | λ_dann: 0.5000 | λ_mmd: 0.0872\n",
      "[INFO] patience 1 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.992501, test Acc: 0.6169\n",
      "[Epoch 29] Total: 0.6988 | Cls: 0.0010 | Dom: 1.3895 | MMD: 0.0329 | DomAcc: 0.4952 | λ_dann: 0.5000 | λ_mmd: 0.0898\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.144400, test Acc: 0.6018\n",
      "[Epoch 30] Total: 0.6996 | Cls: 0.0013 | Dom: 1.3904 | MMD: 0.0344 | DomAcc: 0.4804 | λ_dann: 0.5000 | λ_mmd: 0.0920\n",
      "[INFO] patience 1 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.207027, test Acc: 0.6139\n",
      "[Epoch 31] Total: 0.6998 | Cls: 0.0004 | Dom: 1.3922 | MMD: 0.0353 | DomAcc: 0.4568 | λ_dann: 0.5000 | λ_mmd: 0.0937\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.256500, test Acc: 0.6200\n",
      "[Epoch 32] Total: 0.7019 | Cls: 0.0024 | Dom: 1.3916 | MMD: 0.0389 | DomAcc: 0.4648 | λ_dann: 0.5000 | λ_mmd: 0.0950\n",
      "[INFO] patience 1 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.932623, test Acc: 0.6230\n",
      "[Epoch 33] Total: 0.6991 | Cls: 0.0006 | Dom: 1.3899 | MMD: 0.0370 | DomAcc: 0.4887 | λ_dann: 0.5000 | λ_mmd: 0.0961\n",
      "[INFO] patience 2 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.226695, test Acc: 0.6200\n",
      "[Epoch 34] Total: 0.7022 | Cls: 0.0044 | Dom: 1.3886 | MMD: 0.0364 | DomAcc: 0.4968 | λ_dann: 0.5000 | λ_mmd: 0.0970\n",
      "[INFO] patience 3 / 3 | MMD_small=True plateau=True\n",
      "[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.256500, test Acc: 0.6200\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T11:34:54.033363Z",
     "start_time": "2025-08-13T11:13:09.500379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 4e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 2e-2  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    use_mk = True\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased_with_gamma(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=40, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "e43bc0b93c033404",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.0593 | Cls: 0.5681 | Dom: 0.9808 | MMD: 0.1618 | DomAcc: 0.7507 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.610506, test Acc: 0.5030\n",
      "[Epoch 2] Total: 0.6453 | Cls: 0.1194 | Dom: 1.0514 | MMD: 0.1499 | DomAcc: 0.7281 | λ_dann: 0.5000 | λ_mmd: 0.0009\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.531214, test Acc: 0.4990\n",
      "[Epoch 3] Total: 0.6382 | Cls: 0.1172 | Dom: 1.0414 | MMD: 0.1375 | DomAcc: 0.7316 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.333076, test Acc: 0.5514\n",
      "[Epoch 4] Total: 0.6530 | Cls: 0.0899 | Dom: 1.1255 | MMD: 0.1320 | DomAcc: 0.6997 | λ_dann: 0.5000 | λ_mmd: 0.0014\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.803632, test Acc: 0.5373\n",
      "[Epoch 5] Total: 0.6985 | Cls: 0.0898 | Dom: 1.2168 | MMD: 0.1303 | DomAcc: 0.6627 | λ_dann: 0.5000 | λ_mmd: 0.0018\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.046692, test Acc: 0.5171\n",
      "[Epoch 6] Total: 0.6895 | Cls: 0.0641 | Dom: 1.2501 | MMD: 0.1326 | DomAcc: 0.6423 | λ_dann: 0.5000 | λ_mmd: 0.0024\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.641619, test Acc: 0.5312\n",
      "[Epoch 7] Total: 0.6820 | Cls: 0.0568 | Dom: 1.2494 | MMD: 0.1308 | DomAcc: 0.6517 | λ_dann: 0.5000 | λ_mmd: 0.0030\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.161669, test Acc: 0.3286\n",
      "[Epoch 8] Total: 0.6923 | Cls: 0.0483 | Dom: 1.2869 | MMD: 0.1265 | DomAcc: 0.6271 | λ_dann: 0.5000 | λ_mmd: 0.0039\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.646321, test Acc: 0.4879\n",
      "[Epoch 9] Total: 0.6958 | Cls: 0.0473 | Dom: 1.2956 | MMD: 0.1264 | DomAcc: 0.6248 | λ_dann: 0.5000 | λ_mmd: 0.0050\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.763052, test Acc: 0.3599\n",
      "[Epoch 10] Total: 0.6637 | Cls: 0.0346 | Dom: 1.2567 | MMD: 0.1200 | DomAcc: 0.6432 | λ_dann: 0.5000 | λ_mmd: 0.0063\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.316863, test Acc: 0.4929\n",
      "[Epoch 11] Total: 0.6807 | Cls: 0.0318 | Dom: 1.2958 | MMD: 0.1148 | DomAcc: 0.6300 | λ_dann: 0.5000 | λ_mmd: 0.0080\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.063363, test Acc: 0.5514\n",
      "[Epoch 12] Total: 0.6791 | Cls: 0.0350 | Dom: 1.2858 | MMD: 0.1160 | DomAcc: 0.6158 | λ_dann: 0.5000 | λ_mmd: 0.0102\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.661003, test Acc: 0.5403\n",
      "[Epoch 13] Total: 0.6864 | Cls: 0.0382 | Dom: 1.2933 | MMD: 0.1150 | DomAcc: 0.6241 | λ_dann: 0.5000 | λ_mmd: 0.0128\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.245374, test Acc: 0.5726\n",
      "[Epoch 14] Total: 0.6967 | Cls: 0.0254 | Dom: 1.3389 | MMD: 0.1190 | DomAcc: 0.5994 | λ_dann: 0.5000 | λ_mmd: 0.0159\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.843843, test Acc: 0.5796\n",
      "[Epoch 15] Total: 0.6702 | Cls: 0.0187 | Dom: 1.2983 | MMD: 0.1132 | DomAcc: 0.6371 | λ_dann: 0.5000 | λ_mmd: 0.0196\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.827950, test Acc: 0.6411\n",
      "[Epoch 16] Total: 0.6713 | Cls: 0.0159 | Dom: 1.3058 | MMD: 0.1026 | DomAcc: 0.6241 | λ_dann: 0.5000 | λ_mmd: 0.0240\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.315509, test Acc: 0.5907\n",
      "[Epoch 17] Total: 0.6830 | Cls: 0.0082 | Dom: 1.3439 | MMD: 0.1001 | DomAcc: 0.5923 | λ_dann: 0.5000 | λ_mmd: 0.0290\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.997272, test Acc: 0.6280\n",
      "[Epoch 18] Total: 0.6920 | Cls: 0.0137 | Dom: 1.3494 | MMD: 0.1032 | DomAcc: 0.5828 | λ_dann: 0.5000 | λ_mmd: 0.0345\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.794005, test Acc: 0.6018\n",
      "[Epoch 19] Total: 0.6767 | Cls: 0.0093 | Dom: 1.3278 | MMD: 0.0880 | DomAcc: 0.6003 | λ_dann: 0.5000 | λ_mmd: 0.0405\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.990075, test Acc: 0.5897\n",
      "[Epoch 20] Total: 0.6857 | Cls: 0.0082 | Dom: 1.3473 | MMD: 0.0827 | DomAcc: 0.5796 | λ_dann: 0.5000 | λ_mmd: 0.0468\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.425765, test Acc: 0.6260\n",
      "[Epoch 21] Total: 0.6911 | Cls: 0.0095 | Dom: 1.3544 | MMD: 0.0822 | DomAcc: 0.5838 | λ_dann: 0.5000 | λ_mmd: 0.0532\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.233516, test Acc: 0.7288\n",
      "[Epoch 22] Total: 0.7021 | Cls: 0.0047 | Dom: 1.3850 | MMD: 0.0822 | DomAcc: 0.5264 | λ_dann: 0.5000 | λ_mmd: 0.0595\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.249978, test Acc: 0.7308\n",
      "[Epoch 23] Total: 0.7013 | Cls: 0.0036 | Dom: 1.3851 | MMD: 0.0790 | DomAcc: 0.5248 | λ_dann: 0.5000 | λ_mmd: 0.0655\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.219298, test Acc: 0.6946\n",
      "[Epoch 24] Total: 0.7014 | Cls: 0.0031 | Dom: 1.3856 | MMD: 0.0780 | DomAcc: 0.5162 | λ_dann: 0.5000 | λ_mmd: 0.0710\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.297498, test Acc: 0.7218\n",
      "[Epoch 25] Total: 0.7011 | Cls: 0.0017 | Dom: 1.3886 | MMD: 0.0675 | DomAcc: 0.4914 | λ_dann: 0.5000 | λ_mmd: 0.0760\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.284741, test Acc: 0.7077\n",
      "[Epoch 26] Total: 0.7004 | Cls: 0.0018 | Dom: 1.3876 | MMD: 0.0607 | DomAcc: 0.5025 | λ_dann: 0.5000 | λ_mmd: 0.0804\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.282317, test Acc: 0.7046\n",
      "[Epoch 27] Total: 0.7010 | Cls: 0.0030 | Dom: 1.3878 | MMD: 0.0513 | DomAcc: 0.4971 | λ_dann: 0.5000 | λ_mmd: 0.0841\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.452630, test Acc: 0.6804\n",
      "[Epoch 28] Total: 0.6993 | Cls: 0.0020 | Dom: 1.3862 | MMD: 0.0479 | DomAcc: 0.5075 | λ_dann: 0.5000 | λ_mmd: 0.0872\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.345781, test Acc: 0.7077\n",
      "[Epoch 29] Total: 0.6987 | Cls: 0.0018 | Dom: 1.3861 | MMD: 0.0428 | DomAcc: 0.5015 | λ_dann: 0.5000 | λ_mmd: 0.0898\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.336568, test Acc: 0.7147\n",
      "[Epoch 30] Total: 0.6986 | Cls: 0.0010 | Dom: 1.3880 | MMD: 0.0383 | DomAcc: 0.4984 | λ_dann: 0.5000 | λ_mmd: 0.0920\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.412745, test Acc: 0.7067\n",
      "[Epoch 31] Total: 0.6981 | Cls: 0.0004 | Dom: 1.3888 | MMD: 0.0343 | DomAcc: 0.4796 | λ_dann: 0.5000 | λ_mmd: 0.0937\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.360594, test Acc: 0.7268\n",
      "[Epoch 32] Total: 0.7005 | Cls: 0.0033 | Dom: 1.3881 | MMD: 0.0333 | DomAcc: 0.4920 | λ_dann: 0.5000 | λ_mmd: 0.0950\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.199436, test Acc: 0.7006\n",
      "[Epoch 33] Total: 0.6990 | Cls: 0.0017 | Dom: 1.3859 | MMD: 0.0452 | DomAcc: 0.5082 | λ_dann: 0.5000 | λ_mmd: 0.0961\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.363521, test Acc: 0.6946\n",
      "[Epoch 34] Total: 0.7027 | Cls: 0.0050 | Dom: 1.3886 | MMD: 0.0346 | DomAcc: 0.4894 | λ_dann: 0.5000 | λ_mmd: 0.0970\n",
      "[INFO] patience 1 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.212366, test Acc: 0.7369\n",
      "[Epoch 35] Total: 0.6995 | Cls: 0.0019 | Dom: 1.3894 | MMD: 0.0300 | DomAcc: 0.4901 | λ_dann: 0.5000 | λ_mmd: 0.0976\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.161227, test Acc: 0.7278\n",
      "[Epoch 36] Total: 0.6982 | Cls: 0.0004 | Dom: 1.3901 | MMD: 0.0279 | DomAcc: 0.4793 | λ_dann: 0.5000 | λ_mmd: 0.0982\n",
      "[INFO] patience 0 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.283724, test Acc: 0.7157\n",
      "[Epoch 37] Total: 0.7003 | Cls: 0.0042 | Dom: 1.3864 | MMD: 0.0297 | DomAcc: 0.4949 | λ_dann: 0.5000 | λ_mmd: 0.0986\n",
      "[INFO] patience 1 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.148108, test Acc: 0.7228\n",
      "[Epoch 38] Total: 0.7079 | Cls: 0.0159 | Dom: 1.3732 | MMD: 0.0543 | DomAcc: 0.5548 | λ_dann: 0.5000 | λ_mmd: 0.0989\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.087005, test Acc: 0.6875\n",
      "[Epoch 39] Total: 0.6964 | Cls: 0.0032 | Dom: 1.3770 | MMD: 0.0470 | DomAcc: 0.5430 | λ_dann: 0.5000 | λ_mmd: 0.0991\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.288982, test Acc: 0.6815\n",
      "[Epoch 40] Total: 0.6989 | Cls: 0.0009 | Dom: 1.3885 | MMD: 0.0372 | DomAcc: 0.4985 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] patience 1 / 3 | MMD_small=True plateau=False\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.320372, test Acc: 0.7409\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.320372, test Acc: 0.7409\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T22:55:13.951057Z",
     "start_time": "2025-08-19T22:46:02.369028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 4e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 2e-2  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    use_mk = True\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased_with_gamma(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler,use_mk=True)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "beb1eba89d834a99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.2814 | Cls: 0.6822 | Dom: 1.1965 | MMD: 0.1878 | DomAcc: 0.6556 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.855115, test Acc: 0.4909\n",
      "[Epoch 2] Total: 0.8026 | Cls: 0.1799 | Dom: 1.2445 | MMD: 0.1614 | DomAcc: 0.6444 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.510852, test Acc: 0.4012\n",
      "[Epoch 3] Total: 0.7637 | Cls: 0.0931 | Dom: 1.3404 | MMD: 0.1861 | DomAcc: 0.5812 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.055924, test Acc: 0.3891\n",
      "[Epoch 4] Total: 0.7172 | Cls: 0.0897 | Dom: 1.2540 | MMD: 0.1398 | DomAcc: 0.6593 | λ_dann: 0.5000 | λ_mmd: 0.0032\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.925288, test Acc: 0.3931\n",
      "[Epoch 5] Total: 0.7317 | Cls: 0.0785 | Dom: 1.3047 | MMD: 0.1369 | DomAcc: 0.6194 | λ_dann: 0.5000 | λ_mmd: 0.0052\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.448269, test Acc: 0.3780\n",
      "[Epoch 6] Total: 0.7223 | Cls: 0.0716 | Dom: 1.2990 | MMD: 0.1375 | DomAcc: 0.6188 | λ_dann: 0.5000 | λ_mmd: 0.0086\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.895137, test Acc: 0.4083\n",
      "[Epoch 7] Total: 0.7386 | Cls: 0.0700 | Dom: 1.3333 | MMD: 0.1330 | DomAcc: 0.5965 | λ_dann: 0.5000 | λ_mmd: 0.0137\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.385023, test Acc: 0.2671\n",
      "[Epoch 8] Total: 0.7463 | Cls: 0.0598 | Dom: 1.3662 | MMD: 0.1602 | DomAcc: 0.5748 | λ_dann: 0.5000 | λ_mmd: 0.0212\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.910586, test Acc: 0.3407\n",
      "[Epoch 9] Total: 0.7145 | Cls: 0.0528 | Dom: 1.3155 | MMD: 0.1245 | DomAcc: 0.6183 | λ_dann: 0.5000 | λ_mmd: 0.0312\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.853691, test Acc: 0.3024\n",
      "[Epoch 10] Total: 0.7320 | Cls: 0.0640 | Dom: 1.3251 | MMD: 0.1256 | DomAcc: 0.5929 | λ_dann: 0.5000 | λ_mmd: 0.0435\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.502204, test Acc: 0.3105\n",
      "[Epoch 11] Total: 0.7320 | Cls: 0.0511 | Dom: 1.3496 | MMD: 0.1128 | DomAcc: 0.5834 | λ_dann: 0.5000 | λ_mmd: 0.0565\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.864743, test Acc: 0.2742\n",
      "[Epoch 12] Total: 0.7263 | Cls: 0.0423 | Dom: 1.3523 | MMD: 0.1139 | DomAcc: 0.5823 | λ_dann: 0.5000 | λ_mmd: 0.0688\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.850411, test Acc: 0.4506\n",
      "[Epoch 13] Total: 0.7222 | Cls: 0.0380 | Dom: 1.3504 | MMD: 0.1126 | DomAcc: 0.5801 | λ_dann: 0.5000 | λ_mmd: 0.0788\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.589353, test Acc: 0.3095\n",
      "[Epoch 14] Total: 0.7245 | Cls: 0.0313 | Dom: 1.3679 | MMD: 0.1069 | DomAcc: 0.5677 | λ_dann: 0.5000 | λ_mmd: 0.0863\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.414790, test Acc: 0.3337\n",
      "[Epoch 15] Total: 0.7169 | Cls: 0.0252 | Dom: 1.3664 | MMD: 0.1001 | DomAcc: 0.5744 | λ_dann: 0.5000 | λ_mmd: 0.0914\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.659000, test Acc: 0.2641\n",
      "[Epoch 16] Total: 0.7256 | Cls: 0.0300 | Dom: 1.3726 | MMD: 0.1010 | DomAcc: 0.5598 | λ_dann: 0.5000 | λ_mmd: 0.0948\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.429111, test Acc: 0.2903\n",
      "[Epoch 17] Total: 0.7200 | Cls: 0.0238 | Dom: 1.3714 | MMD: 0.1081 | DomAcc: 0.5615 | λ_dann: 0.5000 | λ_mmd: 0.0968\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.063447, test Acc: 0.2661\n",
      "[Epoch 18] Total: 0.7286 | Cls: 0.0280 | Dom: 1.3801 | MMD: 0.1065 | DomAcc: 0.5403 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.149092, test Acc: 0.2823\n",
      "[Epoch 19] Total: 0.7186 | Cls: 0.0208 | Dom: 1.3760 | MMD: 0.0987 | DomAcc: 0.5593 | λ_dann: 0.5000 | λ_mmd: 0.0989\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.722118, test Acc: 0.3054\n",
      "[Epoch 20] Total: 0.7173 | Cls: 0.0179 | Dom: 1.3787 | MMD: 0.1027 | DomAcc: 0.5298 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] patience 0 / 3 | MMD_small=False plateau=True\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.864337, test Acc: 0.2913\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 5.864337, test Acc: 0.2913\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T00:32:56.760518Z",
     "start_time": "2025-08-13T00:22:37.558890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased_with_gamma(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T194_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "17c4b77ec2387301",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 0.9718 | Cls: 0.5379 | Dom: 0.8728 | MMD: 0.0294 | DomAcc: 0.7842 | λ_dann: 0.5000 | λ_mmd: -0.0987\n",
      "[Epoch 2] Total: 0.6114 | Cls: 0.1429 | Dom: 0.9465 | MMD: 0.0496 | DomAcc: 0.7675 | λ_dann: 0.5000 | λ_mmd: -0.0977\n",
      "[Epoch 3] Total: 0.5388 | Cls: 0.1018 | Dom: 0.8834 | MMD: 0.0492 | DomAcc: 0.7953 | λ_dann: 0.5000 | λ_mmd: -0.0962\n",
      "[Epoch 4] Total: 0.6447 | Cls: 0.0947 | Dom: 1.1085 | MMD: 0.0465 | DomAcc: 0.7207 | λ_dann: 0.5000 | λ_mmd: -0.0937\n",
      "[Epoch 5] Total: 0.6532 | Cls: 0.0878 | Dom: 1.1368 | MMD: 0.0344 | DomAcc: 0.7068 | λ_dann: 0.5000 | λ_mmd: -0.0895\n",
      "[Epoch 6] Total: 0.6525 | Cls: 0.0660 | Dom: 1.1799 | MMD: 0.0428 | DomAcc: 0.6833 | λ_dann: 0.5000 | λ_mmd: -0.0829\n",
      "[Epoch 7] Total: 0.6525 | Cls: 0.0512 | Dom: 1.2078 | MMD: 0.0359 | DomAcc: 0.6628 | λ_dann: 0.5000 | λ_mmd: -0.0726\n",
      "[Epoch 8] Total: 0.6613 | Cls: 0.0565 | Dom: 1.2142 | MMD: 0.0413 | DomAcc: 0.6598 | λ_dann: 0.5000 | λ_mmd: -0.0577\n",
      "[Epoch 9] Total: 0.6679 | Cls: 0.0471 | Dom: 1.2436 | MMD: 0.0294 | DomAcc: 0.6454 | λ_dann: 0.5000 | λ_mmd: -0.0375\n",
      "[Epoch 10] Total: 0.6600 | Cls: 0.0433 | Dom: 1.2344 | MMD: 0.0340 | DomAcc: 0.6571 | λ_dann: 0.5000 | λ_mmd: -0.0131\n",
      "[Epoch 11] Total: 0.6637 | Cls: 0.0342 | Dom: 1.2577 | MMD: 0.0446 | DomAcc: 0.6337 | λ_dann: 0.5000 | λ_mmd: 0.0131\n",
      "[Epoch 12] Total: 0.6604 | Cls: 0.0308 | Dom: 1.2562 | MMD: 0.0399 | DomAcc: 0.6368 | λ_dann: 0.5000 | λ_mmd: 0.0375\n",
      "[Epoch 13] Total: 0.6889 | Cls: 0.0282 | Dom: 1.3174 | MMD: 0.0351 | DomAcc: 0.6002 | λ_dann: 0.5000 | λ_mmd: 0.0577\n",
      "[Epoch 14] Total: 0.6760 | Cls: 0.0255 | Dom: 1.2959 | MMD: 0.0348 | DomAcc: 0.6301 | λ_dann: 0.5000 | λ_mmd: 0.0726\n",
      "[Epoch 15] Total: 0.6936 | Cls: 0.0371 | Dom: 1.3064 | MMD: 0.0407 | DomAcc: 0.6222 | λ_dann: 0.5000 | λ_mmd: 0.0829\n",
      "[Epoch 16] Total: 0.6688 | Cls: 0.0262 | Dom: 1.2778 | MMD: 0.0419 | DomAcc: 0.6231 | λ_dann: 0.5000 | λ_mmd: 0.0895\n",
      "[Epoch 17] Total: 0.6781 | Cls: 0.0150 | Dom: 1.3187 | MMD: 0.0397 | DomAcc: 0.6014 | λ_dann: 0.5000 | λ_mmd: 0.0937\n",
      "[Epoch 18] Total: 0.6980 | Cls: 0.0133 | Dom: 1.3604 | MMD: 0.0465 | DomAcc: 0.5521 | λ_dann: 0.5000 | λ_mmd: 0.0962\n",
      "[Epoch 19] Total: 0.6964 | Cls: 0.0124 | Dom: 1.3599 | MMD: 0.0413 | DomAcc: 0.5575 | λ_dann: 0.5000 | λ_mmd: 0.0977\n",
      "[Epoch 20] Total: 0.6909 | Cls: 0.0113 | Dom: 1.3515 | MMD: 0.0386 | DomAcc: 0.5795 | λ_dann: 0.5000 | λ_mmd: 0.0987\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.776891, test Acc: 0.2431\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T00:46:17.895634Z",
     "start_time": "2025-08-13T00:35:56.985213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased_with_gamma(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T194_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "1991c2e0c94458b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 0.9868 | Cls: 0.5346 | Dom: 0.9038 | MMD: 0.0307 | DomAcc: 0.7751 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.323531, test Acc: 0.4178\n",
      "[Epoch 2] Total: 0.5708 | Cls: 0.1274 | Dom: 0.8867 | MMD: 0.0483 | DomAcc: 0.7917 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.747535, test Acc: 0.3504\n",
      "[Epoch 3] Total: 0.6392 | Cls: 0.1112 | Dom: 1.0557 | MMD: 0.0435 | DomAcc: 0.7389 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.904590, test Acc: 0.4127\n",
      "[Epoch 4] Total: 0.6634 | Cls: 0.0813 | Dom: 1.1639 | MMD: 0.0482 | DomAcc: 0.7019 | λ_dann: 0.5000 | λ_mmd: 0.0032\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.319887, test Acc: 0.4311\n",
      "[Epoch 5] Total: 0.6620 | Cls: 0.0830 | Dom: 1.1573 | MMD: 0.0529 | DomAcc: 0.6999 | λ_dann: 0.5000 | λ_mmd: 0.0052\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.879085, test Acc: 0.4188\n",
      "[Epoch 6] Total: 0.6541 | Cls: 0.0775 | Dom: 1.1520 | MMD: 0.0560 | DomAcc: 0.6967 | λ_dann: 0.5000 | λ_mmd: 0.0086\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.874129, test Acc: 0.3973\n",
      "[Epoch 7] Total: 0.6572 | Cls: 0.0490 | Dom: 1.2148 | MMD: 0.0548 | DomAcc: 0.6674 | λ_dann: 0.5000 | λ_mmd: 0.0137\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.617330, test Acc: 0.5046\n",
      "[Epoch 8] Total: 0.6807 | Cls: 0.0666 | Dom: 1.2259 | MMD: 0.0511 | DomAcc: 0.6550 | λ_dann: 0.5000 | λ_mmd: 0.0212\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.001766, test Acc: 0.4699\n",
      "[Epoch 9] Total: 0.6642 | Cls: 0.0329 | Dom: 1.2602 | MMD: 0.0397 | DomAcc: 0.6507 | λ_dann: 0.5000 | λ_mmd: 0.0312\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.938647, test Acc: 0.4178\n",
      "[Epoch 10] Total: 0.6821 | Cls: 0.0417 | Dom: 1.2770 | MMD: 0.0451 | DomAcc: 0.6311 | λ_dann: 0.5000 | λ_mmd: 0.0435\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.292496, test Acc: 0.4913\n",
      "[Epoch 11] Total: 0.6675 | Cls: 0.0270 | Dom: 1.2773 | MMD: 0.0317 | DomAcc: 0.6374 | λ_dann: 0.5000 | λ_mmd: 0.0565\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.930283, test Acc: 0.4525\n",
      "[Epoch 12] Total: 0.6822 | Cls: 0.0327 | Dom: 1.2940 | MMD: 0.0354 | DomAcc: 0.6265 | λ_dann: 0.5000 | λ_mmd: 0.0688\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.506001, test Acc: 0.4607\n",
      "[Epoch 13] Total: 0.6796 | Cls: 0.0237 | Dom: 1.3062 | MMD: 0.0357 | DomAcc: 0.6110 | λ_dann: 0.5000 | λ_mmd: 0.0788\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.216299, test Acc: 0.4627\n",
      "[Epoch 14] Total: 0.6808 | Cls: 0.0351 | Dom: 1.2852 | MMD: 0.0358 | DomAcc: 0.6342 | λ_dann: 0.5000 | λ_mmd: 0.0863\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.451402, test Acc: 0.5158\n",
      "[Epoch 15] Total: 0.6885 | Cls: 0.0272 | Dom: 1.3156 | MMD: 0.0382 | DomAcc: 0.6042 | λ_dann: 0.5000 | λ_mmd: 0.0914\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.278318, test Acc: 0.5281\n",
      "[Epoch 16] Total: 0.6967 | Cls: 0.0195 | Dom: 1.3468 | MMD: 0.0405 | DomAcc: 0.5872 | λ_dann: 0.5000 | λ_mmd: 0.0948\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.883770, test Acc: 0.4801\n",
      "[Epoch 17] Total: 0.6939 | Cls: 0.0131 | Dom: 1.3546 | MMD: 0.0364 | DomAcc: 0.5760 | λ_dann: 0.5000 | λ_mmd: 0.0968\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.473674, test Acc: 0.4995\n",
      "[Epoch 18] Total: 0.6887 | Cls: 0.0102 | Dom: 1.3500 | MMD: 0.0360 | DomAcc: 0.5850 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.351480, test Acc: 0.5107\n",
      "[Epoch 19] Total: 0.7004 | Cls: 0.0170 | Dom: 1.3602 | MMD: 0.0328 | DomAcc: 0.5705 | λ_dann: 0.5000 | λ_mmd: 0.0989\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.276604, test Acc: 0.5087\n",
      "[Epoch 20] Total: 0.7001 | Cls: 0.0122 | Dom: 1.3690 | MMD: 0.0339 | DomAcc: 0.5650 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.292932, test Acc: 0.5077\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.292932, test Acc: 0.5077\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:46.481389Z",
     "start_time": "2025-08-13T00:48:13.317326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased_with_gamma(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T197_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T197_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T197_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "ea92ed7bc316240c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.0468 | Cls: 0.5257 | Dom: 1.0426 | MMD: 0.0196 | DomAcc: 0.7374 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.204481, test Acc: 0.4366\n",
      "[Epoch 2] Total: 0.6952 | Cls: 0.1240 | Dom: 1.1422 | MMD: 0.0400 | DomAcc: 0.6986 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.733186, test Acc: 0.3772\n",
      "[Epoch 3] Total: 0.6440 | Cls: 0.0992 | Dom: 1.0894 | MMD: 0.0394 | DomAcc: 0.7139 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.069795, test Acc: 0.5693\n",
      "[Epoch 4] Total: 0.6452 | Cls: 0.0648 | Dom: 1.1605 | MMD: 0.0365 | DomAcc: 0.6825 | λ_dann: 0.5000 | λ_mmd: 0.0032\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.749521, test Acc: 0.4416\n",
      "[Epoch 5] Total: 0.6671 | Cls: 0.0704 | Dom: 1.1930 | MMD: 0.0405 | DomAcc: 0.6580 | λ_dann: 0.5000 | λ_mmd: 0.0052\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.893730, test Acc: 0.3713\n",
      "[Epoch 6] Total: 0.6839 | Cls: 0.0550 | Dom: 1.2570 | MMD: 0.0411 | DomAcc: 0.6340 | λ_dann: 0.5000 | λ_mmd: 0.0086\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.815816, test Acc: 0.4248\n",
      "[Epoch 7] Total: 0.6699 | Cls: 0.0403 | Dom: 1.2581 | MMD: 0.0401 | DomAcc: 0.6359 | λ_dann: 0.5000 | λ_mmd: 0.0137\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.775113, test Acc: 0.4604\n",
      "[Epoch 8] Total: 0.6647 | Cls: 0.0364 | Dom: 1.2545 | MMD: 0.0367 | DomAcc: 0.6482 | λ_dann: 0.5000 | λ_mmd: 0.0212\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.267090, test Acc: 0.4881\n",
      "[Epoch 9] Total: 0.6838 | Cls: 0.0335 | Dom: 1.2981 | MMD: 0.0401 | DomAcc: 0.6202 | λ_dann: 0.5000 | λ_mmd: 0.0312\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.697407, test Acc: 0.3139\n",
      "[Epoch 10] Total: 0.6707 | Cls: 0.0349 | Dom: 1.2685 | MMD: 0.0355 | DomAcc: 0.6340 | λ_dann: 0.5000 | λ_mmd: 0.0435\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.976839, test Acc: 0.4307\n",
      "[Epoch 11] Total: 0.6849 | Cls: 0.0323 | Dom: 1.3011 | MMD: 0.0365 | DomAcc: 0.6202 | λ_dann: 0.5000 | λ_mmd: 0.0565\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.369074, test Acc: 0.4911\n",
      "[Epoch 12] Total: 0.6781 | Cls: 0.0330 | Dom: 1.2855 | MMD: 0.0341 | DomAcc: 0.6261 | λ_dann: 0.5000 | λ_mmd: 0.0688\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.054360, test Acc: 0.4693\n",
      "[Epoch 13] Total: 0.6829 | Cls: 0.0286 | Dom: 1.3034 | MMD: 0.0329 | DomAcc: 0.6201 | λ_dann: 0.5000 | λ_mmd: 0.0788\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.571793, test Acc: 0.4505\n",
      "[Epoch 14] Total: 0.6819 | Cls: 0.0229 | Dom: 1.3118 | MMD: 0.0356 | DomAcc: 0.6075 | λ_dann: 0.5000 | λ_mmd: 0.0863\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.281801, test Acc: 0.5257\n",
      "[Epoch 15] Total: 0.6912 | Cls: 0.0257 | Dom: 1.3241 | MMD: 0.0373 | DomAcc: 0.6020 | λ_dann: 0.5000 | λ_mmd: 0.0914\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.035490, test Acc: 0.5554\n",
      "[Epoch 16] Total: 0.6942 | Cls: 0.0178 | Dom: 1.3465 | MMD: 0.0344 | DomAcc: 0.5742 | λ_dann: 0.5000 | λ_mmd: 0.0948\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.206986, test Acc: 0.5455\n",
      "[Epoch 17] Total: 0.6979 | Cls: 0.0182 | Dom: 1.3524 | MMD: 0.0353 | DomAcc: 0.5698 | λ_dann: 0.5000 | λ_mmd: 0.0968\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.383203, test Acc: 0.4881\n",
      "[Epoch 18] Total: 0.6920 | Cls: 0.0131 | Dom: 1.3506 | MMD: 0.0362 | DomAcc: 0.5710 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.328998, test Acc: 0.5574\n",
      "[Epoch 19] Total: 0.7011 | Cls: 0.0187 | Dom: 1.3582 | MMD: 0.0332 | DomAcc: 0.5718 | λ_dann: 0.5000 | λ_mmd: 0.0989\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.831312, test Acc: 0.5782\n",
      "[Epoch 20] Total: 0.6946 | Cls: 0.0210 | Dom: 1.3402 | MMD: 0.0348 | DomAcc: 0.5842 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.528048, test Acc: 0.6020\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.528048, test Acc: 0.6020\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
