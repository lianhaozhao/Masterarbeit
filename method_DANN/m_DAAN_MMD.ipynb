{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-13T00:16:03.753720Z",
     "start_time": "2025-08-13T00:05:19.235873Z"
    }
   },
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 建议对 MMD 系数做 warm-up，避免初期过对齐\n",
    "    p = epoch / max(1, num_epochs - 1)\n",
    "    return max_lambda * (2/(1+torch.exp(torch.tensor(-10*(p-0.5)))) - 1)  # 类似 DANN 的 S 曲线\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率（批级统计）\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        # ——Early stopping（保持你原有的标准；也可把 MMD 下降作为辅助条件）——\n",
    "        if gap < 0.03 and avg_cls_loss < 0.5 and epoch > 10:\n",
    "            patience += 1\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"[INFO] patience {patience} / 3\")\n",
    "            if patience > 3:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned and classifier converged.\")\n",
    "                break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_gap = gap\n",
    "\n",
    "        # ——你原来的目标评估（保持不变）——\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T185_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T185_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.0682 | Cls: 0.5760 | Dom: 0.9881 | MMD: 0.0242 | DomAcc: 0.7347 | λ_dann: 0.5000 | λ_mmd: -0.0987\n",
      "[Epoch 2] Total: 0.6314 | Cls: 0.1286 | Dom: 1.0151 | MMD: 0.0481 | DomAcc: 0.7478 | λ_dann: 0.5000 | λ_mmd: -0.0977\n",
      "[Epoch 3] Total: 0.6514 | Cls: 0.1073 | Dom: 1.0972 | MMD: 0.0477 | DomAcc: 0.7054 | λ_dann: 0.5000 | λ_mmd: -0.0962\n",
      "[Epoch 4] Total: 0.7322 | Cls: 0.0956 | Dom: 1.2841 | MMD: 0.0577 | DomAcc: 0.6103 | λ_dann: 0.5000 | λ_mmd: -0.0937\n",
      "[Epoch 5] Total: 0.6607 | Cls: 0.0921 | Dom: 1.1459 | MMD: 0.0495 | DomAcc: 0.7077 | λ_dann: 0.5000 | λ_mmd: -0.0895\n",
      "[Epoch 6] Total: 0.7372 | Cls: 0.0754 | Dom: 1.3326 | MMD: 0.0553 | DomAcc: 0.5956 | λ_dann: 0.5000 | λ_mmd: -0.0829\n",
      "[Epoch 7] Total: 0.7166 | Cls: 0.0561 | Dom: 1.3303 | MMD: 0.0637 | DomAcc: 0.5978 | λ_dann: 0.5000 | λ_mmd: -0.0726\n",
      "[Epoch 8] Total: 0.7360 | Cls: 0.0602 | Dom: 1.3604 | MMD: 0.0753 | DomAcc: 0.5835 | λ_dann: 0.5000 | λ_mmd: -0.0577\n",
      "[Epoch 9] Total: 0.7415 | Cls: 0.0502 | Dom: 1.3896 | MMD: 0.0838 | DomAcc: 0.5497 | λ_dann: 0.5000 | λ_mmd: -0.0375\n",
      "[Epoch 10] Total: 0.7273 | Cls: 0.0371 | Dom: 1.3825 | MMD: 0.0832 | DomAcc: 0.5793 | λ_dann: 0.5000 | λ_mmd: -0.0131\n",
      "[Epoch 11] Total: 0.7199 | Cls: 0.0282 | Dom: 1.3814 | MMD: 0.0711 | DomAcc: 0.5383 | λ_dann: 0.5000 | λ_mmd: 0.0131\n",
      "[Epoch 12] Total: 0.7021 | Cls: 0.0258 | Dom: 1.3480 | MMD: 0.0592 | DomAcc: 0.5663 | λ_dann: 0.5000 | λ_mmd: 0.0375\n",
      "[Epoch 13] Total: 0.6891 | Cls: 0.0281 | Dom: 1.3166 | MMD: 0.0469 | DomAcc: 0.6530 | λ_dann: 0.5000 | λ_mmd: 0.0577\n",
      "[Epoch 14] Total: 0.7012 | Cls: 0.0268 | Dom: 1.3424 | MMD: 0.0427 | DomAcc: 0.5842 | λ_dann: 0.5000 | λ_mmd: 0.0726\n",
      "[Epoch 15] Total: 0.7034 | Cls: 0.0271 | Dom: 1.3458 | MMD: 0.0409 | DomAcc: 0.5842 | λ_dann: 0.5000 | λ_mmd: 0.0829\n",
      "[Epoch 16] Total: 0.6750 | Cls: 0.0121 | Dom: 1.3175 | MMD: 0.0456 | DomAcc: 0.6081 | λ_dann: 0.5000 | λ_mmd: 0.0895\n",
      "[Epoch 17] Total: 0.6900 | Cls: 0.0140 | Dom: 1.3437 | MMD: 0.0433 | DomAcc: 0.5752 | λ_dann: 0.5000 | λ_mmd: 0.0937\n",
      "[Epoch 18] Total: 0.6874 | Cls: 0.0150 | Dom: 1.3380 | MMD: 0.0347 | DomAcc: 0.5891 | λ_dann: 0.5000 | λ_mmd: 0.0962\n",
      "[Epoch 19] Total: 0.6830 | Cls: 0.0112 | Dom: 1.3376 | MMD: 0.0302 | DomAcc: 0.5891 | λ_dann: 0.5000 | λ_mmd: 0.0977\n",
      "[Epoch 20] Total: 0.6882 | Cls: 0.0097 | Dom: 1.3519 | MMD: 0.0265 | DomAcc: 0.5700 | λ_dann: 0.5000 | λ_mmd: 0.0987\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 1.477114, test Acc: 0.7581\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T00:32:56.760518Z",
     "start_time": "2025-08-13T00:22:37.558890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T194_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "17c4b77ec2387301",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 0.9718 | Cls: 0.5379 | Dom: 0.8728 | MMD: 0.0294 | DomAcc: 0.7842 | λ_dann: 0.5000 | λ_mmd: -0.0987\n",
      "[Epoch 2] Total: 0.6114 | Cls: 0.1429 | Dom: 0.9465 | MMD: 0.0496 | DomAcc: 0.7675 | λ_dann: 0.5000 | λ_mmd: -0.0977\n",
      "[Epoch 3] Total: 0.5388 | Cls: 0.1018 | Dom: 0.8834 | MMD: 0.0492 | DomAcc: 0.7953 | λ_dann: 0.5000 | λ_mmd: -0.0962\n",
      "[Epoch 4] Total: 0.6447 | Cls: 0.0947 | Dom: 1.1085 | MMD: 0.0465 | DomAcc: 0.7207 | λ_dann: 0.5000 | λ_mmd: -0.0937\n",
      "[Epoch 5] Total: 0.6532 | Cls: 0.0878 | Dom: 1.1368 | MMD: 0.0344 | DomAcc: 0.7068 | λ_dann: 0.5000 | λ_mmd: -0.0895\n",
      "[Epoch 6] Total: 0.6525 | Cls: 0.0660 | Dom: 1.1799 | MMD: 0.0428 | DomAcc: 0.6833 | λ_dann: 0.5000 | λ_mmd: -0.0829\n",
      "[Epoch 7] Total: 0.6525 | Cls: 0.0512 | Dom: 1.2078 | MMD: 0.0359 | DomAcc: 0.6628 | λ_dann: 0.5000 | λ_mmd: -0.0726\n",
      "[Epoch 8] Total: 0.6613 | Cls: 0.0565 | Dom: 1.2142 | MMD: 0.0413 | DomAcc: 0.6598 | λ_dann: 0.5000 | λ_mmd: -0.0577\n",
      "[Epoch 9] Total: 0.6679 | Cls: 0.0471 | Dom: 1.2436 | MMD: 0.0294 | DomAcc: 0.6454 | λ_dann: 0.5000 | λ_mmd: -0.0375\n",
      "[Epoch 10] Total: 0.6600 | Cls: 0.0433 | Dom: 1.2344 | MMD: 0.0340 | DomAcc: 0.6571 | λ_dann: 0.5000 | λ_mmd: -0.0131\n",
      "[Epoch 11] Total: 0.6637 | Cls: 0.0342 | Dom: 1.2577 | MMD: 0.0446 | DomAcc: 0.6337 | λ_dann: 0.5000 | λ_mmd: 0.0131\n",
      "[Epoch 12] Total: 0.6604 | Cls: 0.0308 | Dom: 1.2562 | MMD: 0.0399 | DomAcc: 0.6368 | λ_dann: 0.5000 | λ_mmd: 0.0375\n",
      "[Epoch 13] Total: 0.6889 | Cls: 0.0282 | Dom: 1.3174 | MMD: 0.0351 | DomAcc: 0.6002 | λ_dann: 0.5000 | λ_mmd: 0.0577\n",
      "[Epoch 14] Total: 0.6760 | Cls: 0.0255 | Dom: 1.2959 | MMD: 0.0348 | DomAcc: 0.6301 | λ_dann: 0.5000 | λ_mmd: 0.0726\n",
      "[Epoch 15] Total: 0.6936 | Cls: 0.0371 | Dom: 1.3064 | MMD: 0.0407 | DomAcc: 0.6222 | λ_dann: 0.5000 | λ_mmd: 0.0829\n",
      "[Epoch 16] Total: 0.6688 | Cls: 0.0262 | Dom: 1.2778 | MMD: 0.0419 | DomAcc: 0.6231 | λ_dann: 0.5000 | λ_mmd: 0.0895\n",
      "[Epoch 17] Total: 0.6781 | Cls: 0.0150 | Dom: 1.3187 | MMD: 0.0397 | DomAcc: 0.6014 | λ_dann: 0.5000 | λ_mmd: 0.0937\n",
      "[Epoch 18] Total: 0.6980 | Cls: 0.0133 | Dom: 1.3604 | MMD: 0.0465 | DomAcc: 0.5521 | λ_dann: 0.5000 | λ_mmd: 0.0962\n",
      "[Epoch 19] Total: 0.6964 | Cls: 0.0124 | Dom: 1.3599 | MMD: 0.0413 | DomAcc: 0.5575 | λ_dann: 0.5000 | λ_mmd: 0.0977\n",
      "[Epoch 20] Total: 0.6909 | Cls: 0.0113 | Dom: 1.3515 | MMD: 0.0386 | DomAcc: 0.5795 | λ_dann: 0.5000 | λ_mmd: 0.0987\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 6.776891, test Acc: 0.2431\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T00:46:17.895634Z",
     "start_time": "2025-08-13T00:35:56.985213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T194_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T194_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "1991c2e0c94458b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 0.9868 | Cls: 0.5346 | Dom: 0.9038 | MMD: 0.0307 | DomAcc: 0.7751 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.323531, test Acc: 0.4178\n",
      "[Epoch 2] Total: 0.5708 | Cls: 0.1274 | Dom: 0.8867 | MMD: 0.0483 | DomAcc: 0.7917 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.747535, test Acc: 0.3504\n",
      "[Epoch 3] Total: 0.6392 | Cls: 0.1112 | Dom: 1.0557 | MMD: 0.0435 | DomAcc: 0.7389 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.904590, test Acc: 0.4127\n",
      "[Epoch 4] Total: 0.6634 | Cls: 0.0813 | Dom: 1.1639 | MMD: 0.0482 | DomAcc: 0.7019 | λ_dann: 0.5000 | λ_mmd: 0.0032\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.319887, test Acc: 0.4311\n",
      "[Epoch 5] Total: 0.6620 | Cls: 0.0830 | Dom: 1.1573 | MMD: 0.0529 | DomAcc: 0.6999 | λ_dann: 0.5000 | λ_mmd: 0.0052\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.879085, test Acc: 0.4188\n",
      "[Epoch 6] Total: 0.6541 | Cls: 0.0775 | Dom: 1.1520 | MMD: 0.0560 | DomAcc: 0.6967 | λ_dann: 0.5000 | λ_mmd: 0.0086\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.874129, test Acc: 0.3973\n",
      "[Epoch 7] Total: 0.6572 | Cls: 0.0490 | Dom: 1.2148 | MMD: 0.0548 | DomAcc: 0.6674 | λ_dann: 0.5000 | λ_mmd: 0.0137\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.617330, test Acc: 0.5046\n",
      "[Epoch 8] Total: 0.6807 | Cls: 0.0666 | Dom: 1.2259 | MMD: 0.0511 | DomAcc: 0.6550 | λ_dann: 0.5000 | λ_mmd: 0.0212\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.001766, test Acc: 0.4699\n",
      "[Epoch 9] Total: 0.6642 | Cls: 0.0329 | Dom: 1.2602 | MMD: 0.0397 | DomAcc: 0.6507 | λ_dann: 0.5000 | λ_mmd: 0.0312\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.938647, test Acc: 0.4178\n",
      "[Epoch 10] Total: 0.6821 | Cls: 0.0417 | Dom: 1.2770 | MMD: 0.0451 | DomAcc: 0.6311 | λ_dann: 0.5000 | λ_mmd: 0.0435\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.292496, test Acc: 0.4913\n",
      "[Epoch 11] Total: 0.6675 | Cls: 0.0270 | Dom: 1.2773 | MMD: 0.0317 | DomAcc: 0.6374 | λ_dann: 0.5000 | λ_mmd: 0.0565\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.930283, test Acc: 0.4525\n",
      "[Epoch 12] Total: 0.6822 | Cls: 0.0327 | Dom: 1.2940 | MMD: 0.0354 | DomAcc: 0.6265 | λ_dann: 0.5000 | λ_mmd: 0.0688\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.506001, test Acc: 0.4607\n",
      "[Epoch 13] Total: 0.6796 | Cls: 0.0237 | Dom: 1.3062 | MMD: 0.0357 | DomAcc: 0.6110 | λ_dann: 0.5000 | λ_mmd: 0.0788\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.216299, test Acc: 0.4627\n",
      "[Epoch 14] Total: 0.6808 | Cls: 0.0351 | Dom: 1.2852 | MMD: 0.0358 | DomAcc: 0.6342 | λ_dann: 0.5000 | λ_mmd: 0.0863\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.451402, test Acc: 0.5158\n",
      "[Epoch 15] Total: 0.6885 | Cls: 0.0272 | Dom: 1.3156 | MMD: 0.0382 | DomAcc: 0.6042 | λ_dann: 0.5000 | λ_mmd: 0.0914\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.278318, test Acc: 0.5281\n",
      "[Epoch 16] Total: 0.6967 | Cls: 0.0195 | Dom: 1.3468 | MMD: 0.0405 | DomAcc: 0.5872 | λ_dann: 0.5000 | λ_mmd: 0.0948\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.883770, test Acc: 0.4801\n",
      "[Epoch 17] Total: 0.6939 | Cls: 0.0131 | Dom: 1.3546 | MMD: 0.0364 | DomAcc: 0.5760 | λ_dann: 0.5000 | λ_mmd: 0.0968\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.473674, test Acc: 0.4995\n",
      "[Epoch 18] Total: 0.6887 | Cls: 0.0102 | Dom: 1.3500 | MMD: 0.0360 | DomAcc: 0.5850 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.351480, test Acc: 0.5107\n",
      "[Epoch 19] Total: 0.7004 | Cls: 0.0170 | Dom: 1.3602 | MMD: 0.0328 | DomAcc: 0.5705 | λ_dann: 0.5000 | λ_mmd: 0.0989\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.276604, test Acc: 0.5087\n",
      "[Epoch 20] Total: 0.7001 | Cls: 0.0122 | Dom: 1.3690 | MMD: 0.0339 | DomAcc: 0.5650 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.292932, test Acc: 0.5077\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 4.292932, test Acc: 0.5077\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:46.481389Z",
     "start_time": "2025-08-13T00:48:13.317326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "from models.Flexible_DANN_MMD import Flexible_DANN\n",
    "from PKLDataset import PKLDataset\n",
    "from utils.general_train_and_test import general_test_model\n",
    "from models.get_no_label_dataloader import get_target_loader\n",
    "from models.MMD import *\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders(source_path, target_path, batch_size):\n",
    "    source_dataset = PKLDataset(txt_path=source_path)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_loader = get_target_loader(target_path, batch_size=batch_size, shuffle=True)\n",
    "    return source_loader, target_loader\n",
    "\n",
    "def dann_lambda(epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    常用的 DANN λ 调度：从 0 平滑升到 1\n",
    "    你也可以把 -10 调轻/重来改变上升速度\n",
    "    \"\"\"\n",
    "    p = epoch / float(num_epochs)\n",
    "    return 2. / (1. + np.exp(-10 * p)) - 1.\n",
    "\n",
    "def mmd_lambda(epoch, num_epochs, max_lambda=1e-1):\n",
    "    # 0 → max_lambda，S 型上升\n",
    "    p = epoch / max(1, num_epochs - 1)         # p ∈ [0,1]\n",
    "    s = 1.0 / (1.0 + torch.exp(torch.tensor(-10.0*(p - 0.5))))  # ∈ (0,1)\n",
    "    return float(max_lambda * s)\n",
    "\n",
    "def train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                        optimizer, criterion_cls, criterion_domain,\n",
    "                        device, num_epochs=20,\n",
    "                        lambda_dann=0.1,           # 域分类器的权重\n",
    "                        lambda_mmd_max=1e-1,       # MMD 的最大权重\n",
    "                        use_mk=False,               # 是否用多核\n",
    "                        scheduler=None):\n",
    "    PATIENCE = 3\n",
    "    MIN_EPOCH = 10\n",
    "\n",
    "    best_gap = 0.5\n",
    "    best_cls = float('inf')\n",
    "    best_mmd = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 0\n",
    "\n",
    "    MMD_THRESH = 3e-2  # MMD²足够小的阈值，按任务可调（0.02~0.05常见）\n",
    "    MMD_PLATEAU_EPS = 5e-3  # 平台期判定的波动阈值\n",
    "    mmd_hist = deque(maxlen=5)  # 用最近5个epoch判断是否进入平台期\n",
    "\n",
    "    mmd_fn = (lambda x, y: mmd_mk_biased(x, y, gammas=(0.5,1,2,4,8))) if use_mk \\\n",
    "             else (lambda x, y: mmd_rbf_biased(x, y, gamma=None))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cls_loss_sum, dom_loss_sum, mmd_loss_sum, total_loss_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        total_cls_samples, total_dom_samples = 0, 0\n",
    "        dom_correct, dom_total = 0, 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for (src_x, src_y), tgt_x in zip(source_loader, target_loader):\n",
    "            src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "            tgt_x = tgt_x.to(device)\n",
    "\n",
    "            cls_out_src, dom_out_src, feat_src = model(src_x)\n",
    "            _,            dom_out_tgt, feat_tgt = model(tgt_x)\n",
    "\n",
    "            # 1) 分类损失（仅源域）\n",
    "            loss_cls = criterion_cls(cls_out_src, src_y)\n",
    "\n",
    "            # 2) 域分类损失（DANN）\n",
    "            dom_label_src = torch.zeros(src_x.size(0), dtype=torch.long, device=device)\n",
    "            dom_label_tgt = torch.ones(tgt_x.size(0),  dtype=torch.long, device=device)\n",
    "            loss_dom = criterion_domain(dom_out_src, dom_label_src) + \\\n",
    "                       criterion_domain(dom_out_tgt, dom_label_tgt)\n",
    "\n",
    "            # 3) RBF‑MMD（特征对齐）\n",
    "            # 建议先做 L2 归一化，提升稳定性\n",
    "            feat_src_n = F.normalize(feat_src, dim=1)\n",
    "            feat_tgt_n = F.normalize(feat_tgt, dim=1)\n",
    "            loss_mmd = mmd_fn(feat_src_n, feat_tgt_n)\n",
    "\n",
    "            # 4) 组合总损失\n",
    "            #    - DANN 的 lambda 可继续用你已有的动态 dann_lambda\n",
    "            #    - MMD 的权重做 warm‑up（避免一开始就把决策结构抹平）\n",
    "            lambda_dann_now = dann_lambda(epoch, num_epochs) if callable(lambda_dann) else lambda_dann\n",
    "            lambda_mmd_now  = float(mmd_lambda(epoch, num_epochs, max_lambda=lambda_mmd_max))\n",
    "\n",
    "            loss = loss_cls + lambda_dann_now * loss_dom + lambda_mmd_now * loss_mmd\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录指标\n",
    "            cls_loss_sum  += loss_cls.item() * src_x.size(0)\n",
    "            dom_loss_sum  += loss_dom.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            mmd_loss_sum  += loss_mmd.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "            total_loss_sum += loss.item() * (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            total_cls_samples += src_x.size(0)\n",
    "            total_dom_samples += (src_x.size(0) + tgt_x.size(0))\n",
    "\n",
    "            # 域分类准确率\n",
    "            dom_preds_src = torch.argmax(dom_out_src, dim=1)\n",
    "            dom_preds_tgt = torch.argmax(dom_out_tgt, dim=1)\n",
    "            dom_correct += (dom_preds_src == dom_label_src).sum().item()\n",
    "            dom_correct += (dom_preds_tgt == dom_label_tgt).sum().item()\n",
    "            dom_total   += dom_label_src.size(0) + dom_label_tgt.size(0)\n",
    "\n",
    "        # ——Epoch 级日志——\n",
    "        avg_cls_loss  = cls_loss_sum  / max(1, total_cls_samples)\n",
    "        avg_dom_loss  = dom_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_mmd_loss  = mmd_loss_sum  / max(1, total_dom_samples)\n",
    "        avg_total_loss= total_loss_sum/ max(1, total_dom_samples)\n",
    "        dom_acc = dom_correct / max(1, dom_total)\n",
    "        gap = abs(dom_acc - 0.5)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Total: {avg_total_loss:.4f} | \"\n",
    "              f\"Cls: {avg_cls_loss:.4f} | Dom: {avg_dom_loss:.4f} | \"\n",
    "              f\"MMD: {avg_mmd_loss:.4f} | DomAcc: {dom_acc:.4f} | \"\n",
    "              f\"λ_dann: {lambda_dann_now:.4f} | λ_mmd: {lambda_mmd_now:.4f}\")\n",
    "\n",
    "        mmd_hist.append(avg_mmd_loss)\n",
    "        mmd_plateau = (len(mmd_hist) == mmd_hist.maxlen) and (max(mmd_hist) - min(mmd_hist) < MMD_PLATEAU_EPS)\n",
    "\n",
    "        # 触发条件\n",
    "        cond_align = (gap < 0.05)\n",
    "        cond_cls = (avg_cls_loss < 0.5)\n",
    "        cond_mmd_small = (avg_mmd_loss < MMD_THRESH)\n",
    "        cond_mmd_plateau = mmd_plateau\n",
    "\n",
    "        # 是否有任何指标刷新“最好”\n",
    "        improved = False\n",
    "        if gap < best_gap - 1e-4:\n",
    "            best_gap = gap\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_cls_loss < best_cls - 1e-4:\n",
    "            best_cls = avg_cls_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "        if avg_mmd_loss < best_mmd - 1e-5:\n",
    "            best_mmd = avg_mmd_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improved = True\n",
    "\n",
    "        # ——Early stopping：对齐 + 分类收敛 + （MMD小 或 MMD平台期）——\n",
    "        if epoch > MIN_EPOCH and cond_align and cond_cls and (cond_mmd_small or cond_mmd_plateau):\n",
    "            if not improved:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "            print(f\"[INFO] patience {patience} / {PATIENCE} | MMD_small={cond_mmd_small} plateau={cond_mmd_plateau}\")\n",
    "            if patience >= PATIENCE:\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                print(\"[INFO] Early stopping: domain aligned, classifier converged, and MMD stabilized.\")\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            patience = 0\n",
    "        print(\"[INFO] Evaluating on target test set...\")\n",
    "        target_test_path = '../datasets/target/test/HC_T197_RP.txt'\n",
    "        test_dataset = PKLDataset(target_test_path)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        general_test_model(model, criterion_cls, test_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(seed=42)\n",
    "    with open(\"../configs/default.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)['baseline']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    num_layers = config['num_layers']\n",
    "    kernel_size = config['kernel_size']\n",
    "    start_channels = config['start_channels']\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    source_path = '../datasets/source/train/DC_T197_RP.txt'\n",
    "    target_path = '../datasets/target/train/HC_T197_RP.txt'\n",
    "    target_test_path = '../datasets/target/test/HC_T197_RP.txt'\n",
    "    out_path = 'model'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Flexible_DANN(num_layers=num_layers,\n",
    "                          start_channels=start_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          cnn_act='leakrelu',\n",
    "                          num_classes=10,\n",
    "                          lambda_=0.5).to(device)\n",
    "\n",
    "    source_loader, target_loader = get_dataloaders(source_path, target_path, batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"[INFO] Starting standard DANN training (no pseudo labels)...\")\n",
    "    model = train_dann_with_mmd(model, source_loader, target_loader,\n",
    "                                optimizer, criterion_cls, criterion_domain,\n",
    "                                device, num_epochs=20, lambda_dann=0.5, scheduler=scheduler)\n",
    "\n",
    "    print(\"[INFO] Evaluating on target test set...\")\n",
    "    test_dataset = PKLDataset(target_test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    general_test_model(model, criterion_cls, test_loader, device)\n"
   ],
   "id": "ea92ed7bc316240c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting standard DANN training (no pseudo labels)...\n",
      "[Epoch 1] Total: 1.0468 | Cls: 0.5257 | Dom: 1.0426 | MMD: 0.0196 | DomAcc: 0.7374 | λ_dann: 0.5000 | λ_mmd: 0.0007\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.204481, test Acc: 0.4366\n",
      "[Epoch 2] Total: 0.6952 | Cls: 0.1240 | Dom: 1.1422 | MMD: 0.0400 | DomAcc: 0.6986 | λ_dann: 0.5000 | λ_mmd: 0.0011\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.733186, test Acc: 0.3772\n",
      "[Epoch 3] Total: 0.6440 | Cls: 0.0992 | Dom: 1.0894 | MMD: 0.0394 | DomAcc: 0.7139 | λ_dann: 0.5000 | λ_mmd: 0.0019\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.069795, test Acc: 0.5693\n",
      "[Epoch 4] Total: 0.6452 | Cls: 0.0648 | Dom: 1.1605 | MMD: 0.0365 | DomAcc: 0.6825 | λ_dann: 0.5000 | λ_mmd: 0.0032\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.749521, test Acc: 0.4416\n",
      "[Epoch 5] Total: 0.6671 | Cls: 0.0704 | Dom: 1.1930 | MMD: 0.0405 | DomAcc: 0.6580 | λ_dann: 0.5000 | λ_mmd: 0.0052\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.893730, test Acc: 0.3713\n",
      "[Epoch 6] Total: 0.6839 | Cls: 0.0550 | Dom: 1.2570 | MMD: 0.0411 | DomAcc: 0.6340 | λ_dann: 0.5000 | λ_mmd: 0.0086\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.815816, test Acc: 0.4248\n",
      "[Epoch 7] Total: 0.6699 | Cls: 0.0403 | Dom: 1.2581 | MMD: 0.0401 | DomAcc: 0.6359 | λ_dann: 0.5000 | λ_mmd: 0.0137\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.775113, test Acc: 0.4604\n",
      "[Epoch 8] Total: 0.6647 | Cls: 0.0364 | Dom: 1.2545 | MMD: 0.0367 | DomAcc: 0.6482 | λ_dann: 0.5000 | λ_mmd: 0.0212\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.267090, test Acc: 0.4881\n",
      "[Epoch 9] Total: 0.6838 | Cls: 0.0335 | Dom: 1.2981 | MMD: 0.0401 | DomAcc: 0.6202 | λ_dann: 0.5000 | λ_mmd: 0.0312\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.697407, test Acc: 0.3139\n",
      "[Epoch 10] Total: 0.6707 | Cls: 0.0349 | Dom: 1.2685 | MMD: 0.0355 | DomAcc: 0.6340 | λ_dann: 0.5000 | λ_mmd: 0.0435\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.976839, test Acc: 0.4307\n",
      "[Epoch 11] Total: 0.6849 | Cls: 0.0323 | Dom: 1.3011 | MMD: 0.0365 | DomAcc: 0.6202 | λ_dann: 0.5000 | λ_mmd: 0.0565\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.369074, test Acc: 0.4911\n",
      "[Epoch 12] Total: 0.6781 | Cls: 0.0330 | Dom: 1.2855 | MMD: 0.0341 | DomAcc: 0.6261 | λ_dann: 0.5000 | λ_mmd: 0.0688\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.054360, test Acc: 0.4693\n",
      "[Epoch 13] Total: 0.6829 | Cls: 0.0286 | Dom: 1.3034 | MMD: 0.0329 | DomAcc: 0.6201 | λ_dann: 0.5000 | λ_mmd: 0.0788\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.571793, test Acc: 0.4505\n",
      "[Epoch 14] Total: 0.6819 | Cls: 0.0229 | Dom: 1.3118 | MMD: 0.0356 | DomAcc: 0.6075 | λ_dann: 0.5000 | λ_mmd: 0.0863\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.281801, test Acc: 0.5257\n",
      "[Epoch 15] Total: 0.6912 | Cls: 0.0257 | Dom: 1.3241 | MMD: 0.0373 | DomAcc: 0.6020 | λ_dann: 0.5000 | λ_mmd: 0.0914\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.035490, test Acc: 0.5554\n",
      "[Epoch 16] Total: 0.6942 | Cls: 0.0178 | Dom: 1.3465 | MMD: 0.0344 | DomAcc: 0.5742 | λ_dann: 0.5000 | λ_mmd: 0.0948\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.206986, test Acc: 0.5455\n",
      "[Epoch 17] Total: 0.6979 | Cls: 0.0182 | Dom: 1.3524 | MMD: 0.0353 | DomAcc: 0.5698 | λ_dann: 0.5000 | λ_mmd: 0.0968\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.383203, test Acc: 0.4881\n",
      "[Epoch 18] Total: 0.6920 | Cls: 0.0131 | Dom: 1.3506 | MMD: 0.0362 | DomAcc: 0.5710 | λ_dann: 0.5000 | λ_mmd: 0.0981\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 3.328998, test Acc: 0.5574\n",
      "[Epoch 19] Total: 0.7011 | Cls: 0.0187 | Dom: 1.3582 | MMD: 0.0332 | DomAcc: 0.5718 | λ_dann: 0.5000 | λ_mmd: 0.0989\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.831312, test Acc: 0.5782\n",
      "[Epoch 20] Total: 0.6946 | Cls: 0.0210 | Dom: 1.3402 | MMD: 0.0348 | DomAcc: 0.5842 | λ_dann: 0.5000 | λ_mmd: 0.0993\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.528048, test Acc: 0.6020\n",
      "[INFO] Evaluating on target test set...\n",
      "- test Loss: 2.528048, test Acc: 0.6020\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
